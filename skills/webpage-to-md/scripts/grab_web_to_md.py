#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æŠ“å–ç½‘é¡µæ­£æ–‡ä¸å›¾ç‰‡ï¼Œä¿å­˜ä¸º Markdown + æœ¬åœ° assets ç›®å½•ã€‚

ä¾èµ–è¯´æ˜ï¼š
- å¿…éœ€ä¾èµ–ï¼šrequestsï¼ˆHTTP è¯·æ±‚ï¼‰
- å¯é€‰ä¾èµ–ï¼šmarkdownï¼ˆç”¨äº PDF æ¸²æŸ“æ—¶çš„ Markdownâ†’HTML è½¬æ¢ï¼Œæ— åˆ™ä½¿ç”¨å†…ç½®ç®€æ˜“è½¬æ¢ï¼‰
- PDF ç”Ÿæˆï¼šä½¿ç”¨ç³»ç»Ÿå·²å®‰è£…çš„ Edge/Chrome æµè§ˆå™¨ headless æ¨¡å¼ï¼Œæ— éœ€é¢å¤–å®‰è£…å·¥å…·
- ä¸ä¾èµ–ï¼špandocã€playwrightã€seleniumã€bs4ã€lxml

è®¾è®¡ç›®æ ‡ï¼ˆæ¥è‡ªä¹‹å‰å››ä¸ªç«™ç‚¹çš„å®è·µï¼‰ï¼š
- ä¼˜å…ˆæå– <article>ï¼ˆå…¶æ¬¡ <main>/<body>ï¼‰ï¼Œå‡å°‘å¯¼èˆª/é¡µè„šå™ªéŸ³
- ä»…ç”¨æ ‡å‡†åº“ HTMLParserï¼ˆä¸ä¾èµ– bs4/lxmlï¼‰ï¼Œé€‚é…ç¦»çº¿/å—é™ç¯å¢ƒ
- å›¾ç‰‡ä¸‹è½½æ”¯æŒï¼šsrc/data-src/srcset/picture/sourceï¼›ç›¸å¯¹ URLï¼›content-type ç¼ºå¤±æ—¶å—…æ¢æ ¼å¼
- Ghost/Anthropic ç­‰ç«™ç‚¹ä¼šæŠŠè§†é¢‘æ’­æ”¾å™¨/å›¾æ ‡æ··è¿›æ­£æ–‡ï¼šè·³è¿‡å¸¸è§ UI æ ‡ç­¾/ç±»
- å¤„ç† <tag/> è‡ªé—­åˆå¯¼è‡´çš„ skip æ ˆä¸å‡ºæ ˆï¼šå®ç° handle_startendtag
- ç®€å•è¡¨æ ¼è½¬æ¢ä¸º Markdown tableï¼›å¹¶æä¾›æ ¡éªŒï¼ˆå¼•ç”¨æ•°=æ–‡ä»¶æ•°/æ–‡ä»¶å­˜åœ¨ï¼‰
"""

from __future__ import annotations

import argparse
import datetime
import hashlib
import html as htmllib
import json
import os
import re
import shutil
import subprocess
import sys
import tempfile
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, field
from html.parser import HTMLParser
from typing import Dict, Iterable, List, Optional, Sequence, Tuple, Callable, Union
from urllib.parse import urljoin, urlparse, unquote, quote

import requests


# ============================================================================
# é€€å‡ºç å®šä¹‰
# ============================================================================
EXIT_SUCCESS = 0
EXIT_ERROR = 1
EXIT_FILE_EXISTS = 2
EXIT_VALIDATION_FAILED = 3
EXIT_JS_CHALLENGE = 4  # æ£€æµ‹åˆ° JS åçˆ¬ä¿æŠ¤ï¼Œæ— æ³•è·å–å†…å®¹


UA_PRESETS: Dict[str, str] = {
    # å…¼å®¹æ—§è¡Œä¸ºï¼ˆä½†éƒ¨åˆ†ç«™ç‚¹ä¼šæ‹¦æˆªâ€œå·¥å…· UAâ€ï¼‰
    "tool": "Mozilla/5.0 (compatible; grab_web_to_md/1.0)",
    # å¸¸è§çœŸå®æµè§ˆå™¨ UAï¼ˆä¸è¿½æ±‚ç»å¯¹æœ€æ–°ï¼Œåªè¦â€œåƒâ€æµè§ˆå™¨å³å¯ï¼‰
    "edge-win": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36 Edg/120.0.0.0"
    ),
    "chrome-win": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "firefox-win": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:122.0) "
        "Gecko/20100101 Firefox/122.0"
    ),
    "chrome-mac": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "safari-mac": (
        "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/605.1.15 (KHTML, like Gecko) "
        "Version/17.3 Safari/605.1.15"
    ),
    "chrome-linux": (
        "Mozilla/5.0 (X11; Linux x86_64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
}


def _resolve_user_agent(user_agent: Optional[str], ua_preset: str) -> str:
    if user_agent and user_agent.strip():
        return user_agent.strip()
    return UA_PRESETS.get(ua_preset, UA_PRESETS["chrome-win"])


_DEFAULT_MAX_IMAGE_BYTES = 25 * 1024 * 1024  # 25MB/å¼ ï¼›è®¾ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶
_DEFAULT_MAX_HTML_BYTES = 10 * 1024 * 1024  # 10MB/é¡µï¼›è®¾ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶


def redact_url(url: str) -> str:
    """
    URL è„±æ•ï¼šé»˜è®¤ä»…ä¿ç•™ scheme://host/pathï¼Œç§»é™¤ query/fragmentã€‚

    - ä»…å¯¹ http/https ä¸”å« netloc çš„ URL ç”Ÿæ•ˆ
    - å…¶ä»–å½¢å¼ï¼ˆç›¸å¯¹è·¯å¾„ã€ç©ºå­—ç¬¦ä¸²ç­‰ï¼‰åŸæ ·è¿”å›
    """
    try:
        p = urlparse(url)
        if p.scheme in ("http", "https") and p.netloc:
            return p._replace(query="", fragment="").geturl()
    except Exception:
        pass
    return url


def _redact_url_to_local_map(url_to_local: Dict[str, str]) -> Dict[str, Union[str, List[str]]]:
    """
    å°† URL->æœ¬åœ°è·¯å¾„æ˜ å°„ä¸­çš„ URL è„±æ•ï¼ˆå» query/fragmentï¼‰ã€‚
    ä¸ºé¿å…è„±æ•å key å†²çªå¯¼è‡´è¦†ç›–ï¼Œå†²çªæ—¶æŠŠ value å˜æˆåˆ—è¡¨ã€‚
    """
    out: Dict[str, Union[str, List[str]]] = {}
    for raw_url, local_path in url_to_local.items():
        key = redact_url(raw_url)
        if key in out:
            prev = out[key]
            if isinstance(prev, list):
                if local_path not in prev:
                    prev.append(local_path)
            else:
                if local_path != prev:
                    out[key] = [prev, local_path]
        else:
            out[key] = local_path
    return out


_MD_HTTP_LINK_DEST_RE = re.compile(
    r"\]\(\s*(?P<langle><)?(?P<url>https?://[^)\s>]+)(?P<rangle>>)?(?P<title>\s+\"[^\"]*\")?\s*\)"
)
_HTML_HTTP_ATTR_RE = re.compile(r"(?P<prefix>\b(?:src|href)=['\"])(?P<url>https?://[^'\"]+)(?P<suffix>['\"])")


def redact_urls_in_markdown(md_text: str) -> str:
    """
    å¯¹ Markdown æ­£æ–‡ä¸­çš„ http/https URL åšè„±æ•ï¼ˆç§»é™¤ query/fragmentï¼‰ã€‚

    æ³¨æ„ï¼š
    - ä»…å¤„ç†è„šæœ¬è‡ªèº«ç”Ÿæˆ/å¸¸è§çš„ä¸¤ç±»å½¢å¼ï¼š
      1) è¡Œå†…é“¾æ¥/å›¾ç‰‡ï¼š...](https://...) æˆ– ...](<https://...>)
      2) HTML å±æ€§ï¼šsrc="https://..." / href="https://..."
    - ä¸å¤„ç†çº¯æ–‡æœ¬è£¸ URLã€srcset ç­‰å¤æ‚åœºæ™¯ï¼ˆé¿å…è¯¯ä¼¤ï¼‰ã€‚
    """
    if not md_text:
        return md_text

    def _md_repl(m: re.Match[str]) -> str:
        url = m.group("url")
        safe = redact_url(url)
        langle = m.group("langle") or ""
        rangle = m.group("rangle") or ""
        title = m.group("title") or ""
        return f"]({langle}{safe}{rangle}{title})"

    def _html_repl(m: re.Match[str]) -> str:
        url = m.group("url")
        return f"{m.group('prefix')}{redact_url(url)}{m.group('suffix')}"

    out = _MD_HTTP_LINK_DEST_RE.sub(_md_repl, md_text)
    out = _HTML_HTTP_ATTR_RE.sub(_html_repl, out)
    return out


def _host_of(url: str) -> str:
    try:
        return (urlparse(url).hostname or "").lower()
    except Exception:
        return ""


def _is_same_host(url_a: str, url_b: str) -> bool:
    ha = _host_of(url_a)
    hb = _host_of(url_b)
    return bool(ha) and ha == hb


def _create_anonymous_image_session(base_session: requests.Session) -> requests.Session:
    """
    åˆ›å»ºâ€œå¹²å‡€ sessionâ€ç”¨äºè·¨åŸŸå›¾ç‰‡ä¸‹è½½ï¼š
    - ä¸æºå¸¦ Cookie / Authorization / è‡ªå®šä¹‰ Header
    - åªä¿ç•™å°‘é‡å®‰å…¨ Headerï¼ˆå¦‚ UA / Accept-Languageï¼‰
    """
    s = requests.Session()
    # ç»§æ‰¿ç½‘ç»œé…ç½®ï¼Œé¿å…â€œé¡µé¢å¯è®¿é—®ä½†è·¨åŸŸå›¾ç‰‡å› ä»£ç†/è¯ä¹¦/adapter ä¸ä¸€è‡´è€Œå¤±è´¥â€
    try:
        s.trust_env = base_session.trust_env
    except Exception:
        pass
    try:
        s.proxies = dict(getattr(base_session, "proxies", {}) or {})
    except Exception:
        pass
    try:
        s.verify = getattr(base_session, "verify", True)
    except Exception:
        pass
    try:
        s.cert = getattr(base_session, "cert", None)
    except Exception:
        pass
    try:
        # å¤ç”¨ base_session çš„ adapterï¼ˆå¦‚è‡ªå®šä¹‰ TLS/é‡è¯•/ä»£ç†é€‚é…å™¨ï¼‰
        for prefix, adapter in getattr(base_session, "adapters", {}).items():
            s.mount(prefix, adapter)
    except Exception:
        pass

    ua = base_session.headers.get("User-Agent") or UA_PRESETS["chrome-win"]
    accept_lang = base_session.headers.get("Accept-Language") or "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7"
    s.headers.update(
        {
            "User-Agent": ua,
            "Accept": "image/avif,image/webp,image/apng,image/*,*/*;q=0.8",
            "Accept-Language": accept_lang,
        }
    )
    return s


# æœ€å¤§é‡å®šå‘æ¬¡æ•°ï¼Œé˜²æ­¢æ— é™å¾ªç¯
_MAX_REDIRECTS = 10


def _safe_image_get(
    img_url: str,
    page_url: str,
    session: requests.Session,
    anon_session: requests.Session,
    timeout_s: int,
    referer: str,
) -> requests.Response:
    """
    å®‰å…¨åœ° GET å›¾ç‰‡ URLï¼Œæ‰‹åŠ¨å¤„ç†é‡å®šå‘å¹¶åœ¨è·¨åŸŸæ—¶åˆ‡æ¢åˆ°å¹²å‡€ sessionã€‚
    
    é˜²æ­¢åŒåŸŸ URL é‡å®šå‘åˆ°ç¬¬ä¸‰æ–¹ CDN æ—¶æ³„éœ²æ•æ„Ÿè¯·æ±‚å¤´ã€‚
    """
    current_url = img_url
    current_session = session if _is_same_host(img_url, page_url) else anon_session
    
    for _ in range(_MAX_REDIRECTS):
        headers = {"Connection": "close"}
        if referer:
            headers["Referer"] = referer
        r = current_session.get(
            current_url,
            timeout=timeout_s,
            stream=True,
            allow_redirects=False,  # å…³é”®ï¼šç¦ç”¨è‡ªåŠ¨é‡å®šå‘
            headers=headers,
        )
        
        # éé‡å®šå‘å“åº”ï¼Œç›´æ¥è¿”å›
        if r.status_code not in (301, 302, 303, 307, 308):
            return r
        
        # è·å–é‡å®šå‘ç›®æ ‡
        location = r.headers.get("Location")
        if not location:
            # æ²¡æœ‰ Location å¤´ï¼šè§†ä¸ºé”™è¯¯ï¼Œé¿å… 3xx è¢«è¯¯å½“ä½œæˆåŠŸå†™å…¥å›¾ç‰‡
            try:
                r.close()
            except Exception:
                pass
            raise RuntimeError(f"å›¾ç‰‡é‡å®šå‘å“åº”ç¼ºå°‘ Location å¤´: {current_url} (status={r.status_code})")
        
        # å…³é—­å½“å‰å“åº”
        try:
            r.close()
        except Exception:
            pass
        
        # è§£æé‡å®šå‘ç›®æ ‡ï¼ˆå¯èƒ½æ˜¯ç›¸å¯¹è·¯å¾„ï¼‰
        next_url = urljoin(current_url, location)
        
        # æ¯æ¬¡é‡å®šå‘éƒ½æŒ‰â€œç›®æ ‡ URL æ˜¯å¦ä¸ page_url åŒ hostâ€é‡æ–°é€‰æ‹© sessionï¼š
        # - åŒ hostï¼šå…è®¸æºå¸¦ Cookie/Auth
        # - è·¨ hostï¼šä½¿ç”¨å¹²å‡€ session
        current_session = session if _is_same_host(next_url, page_url) else anon_session
        
        current_url = next_url
    
    # è¶…è¿‡æœ€å¤§é‡å®šå‘æ¬¡æ•°
    raise RuntimeError(f"å›¾ç‰‡ URL é‡å®šå‘æ¬¡æ•°è¶…è¿‡ {_MAX_REDIRECTS} æ¬¡: {img_url}")


def yaml_escape_str(s: str) -> str:
    """
    ç»Ÿä¸€çš„ YAML åŒå¼•å·å­—ç¬¦ä¸²è½¬ä¹‰ï¼ˆPhase 3-C å¢å¼ºï¼‰
    
    å¤„ç†ä»¥ä¸‹ç‰¹æ®Šå­—ç¬¦ï¼š
    - \\ -> \\\\  (åæ–œæ å¿…é¡»é¦–å…ˆå¤„ç†)
    - "  -> \\"   (åŒå¼•å·)
    - \\n -> ç©ºæ ¼  (æ¢è¡Œ)
    - \\r -> ç©ºæ ¼  (å›è½¦)
    - \\t -> ç©ºæ ¼  (åˆ¶è¡¨ç¬¦)
    """
    if not s:
        return ""
    # æ³¨æ„é¡ºåºï¼šå…ˆå¤„ç†åæ–œæ ï¼Œé¿å…äºŒæ¬¡è½¬ä¹‰
    s = s.replace("\\", "\\\\")
    s = s.replace('"', '\\"')
    s = s.replace("\n", " ")
    s = s.replace("\r", " ")
    s = s.replace("\t", " ")
    return s.strip()


def escape_markdown_link_text(text: str) -> str:
    """
    è½¬ä¹‰ Markdown é“¾æ¥æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆPhase 3-C å¢å¼ºï¼‰
    
    å¤„ç† [ å’Œ ] å­—ç¬¦ï¼Œé¿å…ç ´åé“¾æ¥è¯­æ³•ï¼š
    - [ -> \\[
    - ] -> \\]
    """
    if not text:
        return ""
    return text.replace("[", "\\[").replace("]", "\\]")


def generate_frontmatter(title: str, url: str, tags: Optional[List[str]] = None) -> str:
    """ç”Ÿæˆ YAML Frontmatter å…ƒæ•°æ®å¤´ï¼Œå…¼å®¹ Obsidian/Hugo/Jekyll ç­‰å·¥å…·ã€‚"""
    date_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    # ä½¿ç”¨ç»Ÿä¸€çš„ YAML è½¬ä¹‰
    safe_title = yaml_escape_str(title)
    safe_url = yaml_escape_str(url or "")
    lines = [
        "---",
        f'title: "{safe_title}"',
        f'source: "{safe_url}"',
        f'date: "{date_str}"',
    ]
    if tags:
        # å¯¹æ¯ä¸ªæ ‡ç­¾ä¹Ÿè¿›è¡Œè½¬ä¹‰
        tags_str = ", ".join(f'"{yaml_escape_str(t)}"' for t in tags)
        lines.append(f"tags: [{tags_str}]")
    lines.append("---")
    lines.append("")
    lines.append("")
    return "\n".join(lines)


def _sanitize_filename_part(text: str) -> str:
    text = text.strip()
    text = re.sub(r"[^\w.\-]+", "-", text, flags=re.UNICODE)
    text = re.sub(r"-{2,}", "-", text)
    return text.strip("-") or "untitled"


def auto_wrap_output_dir(output_path: str) -> str:
    """
    è‡ªåŠ¨ä¸ºè¾“å‡ºæ–‡ä»¶åˆ›å»ºåŒåä¸Šçº§ç›®å½•ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šç›®å½•ï¼‰
    
    è§„åˆ™ï¼š
    - å¦‚æœè¾“å‡ºè·¯å¾„åŒ…å«ç›®å½•ï¼ˆå¦‚ "docs/article.md"ï¼‰ï¼Œä¿æŒä¸å˜
    - å¦‚æœåªæœ‰æ–‡ä»¶åï¼ˆå¦‚ "article.md"ï¼‰ï¼Œåˆ›å»ºåŒåç›®å½• -> "article/article.md"
    
    Args:
        output_path: åŸå§‹è¾“å‡ºè·¯å¾„
    
    Returns:
        å¤„ç†åçš„è¾“å‡ºè·¯å¾„
    
    Examples:
        >>> auto_wrap_output_dir("article.md")
        'article/article.md'
        >>> auto_wrap_output_dir("docs/article.md")
        'docs/article.md'
        >>> auto_wrap_output_dir("./output.md")
        './output.md'
    """
    dirname = os.path.dirname(output_path)
    if dirname:  # ç”¨æˆ·æŒ‡å®šäº†ç›®å½•ï¼ˆåŒ…æ‹¬ "./" æˆ– "../"ï¼‰
        return output_path
    # æ²¡æœ‰ç›®å½•ï¼Œåˆ›å»ºåŒåç›®å½•
    basename = os.path.basename(output_path)
    name_without_ext = os.path.splitext(basename)[0]
    return os.path.join(name_without_ext, basename)


def _safe_path_length(base_dir: str, filename: str, max_total: int = 250) -> str:
    """ç¡®ä¿å®Œæ•´è·¯å¾„ä¸è¶…è¿‡ Windows é™åˆ¶ï¼Œå¿…è¦æ—¶æˆªæ–­æ–‡ä»¶åã€‚"""
    abs_path = os.path.abspath(os.path.join(base_dir, filename))
    if len(abs_path) <= max_total:
        return filename

    name, ext = os.path.splitext(filename)
    overflow = len(abs_path) - max_total
    # è‡³å°‘ä¿ç•™ 10 ä¸ªå­—ç¬¦çš„æ–‡ä»¶å
    truncated_len = max(10, len(name) - overflow - 8)
    truncated = name[:truncated_len]
    # æ·»åŠ å“ˆå¸Œåç¼€ä»¥ä¿è¯å”¯ä¸€æ€§
    suffix = hashlib.sha1(filename.encode("utf-8")).hexdigest()[:6]
    new_filename = f"{truncated}-{suffix}{ext}"
    return new_filename


def _default_basename(url: str, max_len: int = 80) -> str:
    parsed = urlparse(url)
    host = parsed.netloc.lower()
    if host.startswith("www."):
        host = host[4:]
    path = parsed.path.strip("/")
    if not path:
        base = host
    else:
        parts = [p for p in path.split("/") if p]
        base = "_".join([host] + parts)
    base = _sanitize_filename_part(base)
    if len(base) <= max_len:
        return base
    suffix = hashlib.sha1(url.encode("utf-8")).hexdigest()[:8]
    return (base[: max_len - 9] + "-" + suffix).rstrip("-")


def _find_best_section(html: str, tag: str) -> Optional[str]:
    pattern = re.compile(rf"<{tag}\b[^>]*>(.*?)</{tag}>", re.IGNORECASE | re.DOTALL)
    matches = list(pattern.finditer(html))
    if not matches:
        return None
    # é€‰æ‹©æœ€é•¿çš„é‚£ä¸ªï¼Œé¿å…æ‹¿åˆ°å¯¼èˆª/æ¨èæ¨¡å—ä¹‹ç±»çš„çŸ­ article
    best = max(matches, key=lambda m: len(m.group(1)))
    return best.group(1)


def extract_main_html(page_html: str) -> str:
    for tag in ("article", "main", "body"):
        section = _find_best_section(page_html, tag)
        if section:
            return section
    return page_html


class _TargetSectionExtractor(HTMLParser):
    def __init__(self, *, target_id: Optional[str], target_class: Optional[str]):
        super().__init__(convert_charrefs=True)
        self.target_id = (target_id or "").strip() or None
        self.target_class = (target_class or "").strip() or None
        self.depth = 0
        self.done = False
        self.buf: List[str] = []

    @staticmethod
    def _attrs_to_str(attrs_list: Sequence[Tuple[str, Optional[str]]]) -> str:
        parts = []
        for name, value in attrs_list:
            if value is None:
                parts.append(name)
            else:
                escaped = htmllib.escape(str(value), quote=True)
                parts.append(f'{name}="{escaped}"')
        return " ".join(parts)

    def _match(self, attrs: Dict[str, Optional[str]]) -> bool:
        if self.target_id and (attrs.get("id") or "").strip() == self.target_id:
            return True
        if self.target_class:
            classes = _class_list(attrs)
            if self.target_class in classes:
                return True
        return False

    def handle_starttag(self, tag: str, attrs_list: Sequence[Tuple[str, Optional[str]]]) -> None:
        if self.done:
            return
        tag = tag.lower()
        attrs = dict(attrs_list)
        if self.depth == 0:
            if not self._match(attrs):
                return
            self.depth = 1
        else:
            self.depth += 1
        attr_str = self._attrs_to_str(attrs_list)
        if attr_str:
            self.buf.append(f"<{tag} {attr_str}>")
        else:
            self.buf.append(f"<{tag}>")

    def handle_startendtag(self, tag: str, attrs_list: Sequence[Tuple[str, Optional[str]]]) -> None:
        if self.done:
            return
        tag = tag.lower()
        attrs = dict(attrs_list)
        if self.depth == 0:
            if not self._match(attrs):
                return
            self.done = True
        attr_str = self._attrs_to_str(attrs_list)
        if attr_str:
            self.buf.append(f"<{tag} {attr_str}/>")
        else:
            self.buf.append(f"<{tag}/>")

    def handle_endtag(self, tag: str) -> None:
        if self.done or self.depth == 0:
            return
        tag = tag.lower()
        self.buf.append(f"</{tag}>")
        self.depth -= 1
        if self.depth == 0:
            self.done = True

    def handle_data(self, data: str) -> None:
        if self.done or self.depth == 0 or not data:
            return
        self.buf.append(htmllib.escape(data, quote=False))


def extract_target_html(page_html: str, *, target_id: Optional[str], target_class: Optional[str]) -> Optional[str]:
    parser = _TargetSectionExtractor(target_id=target_id, target_class=target_class)
    parser.feed(page_html or "")
    out = "".join(parser.buf).strip()
    return out or None


def extract_target_html_multi(
    page_html: str,
    *,
    target_ids: Optional[str] = None,
    target_classes: Optional[str] = None,
) -> Tuple[Optional[str], Optional[str]]:
    """
    æ”¯æŒå¤šå€¼çš„æ­£æ–‡æå–ï¼ˆT2.1ï¼‰
    
    Args:
        page_html: HTML å†…å®¹
        target_ids: é€—å·åˆ†éš”çš„ ID åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯•
        target_classes: é€—å·åˆ†éš”çš„ class åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§ä¾æ¬¡å°è¯•
    
    Returns:
        (æå–çš„ HTML, åŒ¹é…çš„é€‰æ‹©å™¨æè¿°) æˆ– (None, None)
    """
    # è§£æå¤šå€¼
    ids = [s.strip() for s in (target_ids or "").split(",") if s.strip()]
    classes = [s.strip() for s in (target_classes or "").split(",") if s.strip()]
    
    # ä¼˜å…ˆå°è¯• ID
    for tid in ids:
        result = extract_target_html(page_html, target_id=tid, target_class=None)
        if result:
            return result, f"id={tid}"
    
    # ç„¶åå°è¯• class
    for tcls in classes:
        result = extract_target_html(page_html, target_id=None, target_class=tcls)
        if result:
            return result, f"class={tcls}"
    
    return None, None


# ============================================================================
# Phase 2: æ™ºèƒ½æ­£æ–‡å®¹å™¨å®šä½ï¼ˆT2.1 - T2.4ï¼‰
# ============================================================================

@dataclass
class DocsPreset:
    """æ–‡æ¡£æ¡†æ¶é¢„è®¾é…ç½®ï¼ˆT2.2ï¼‰"""
    name: str
    description: str
    # æ£€æµ‹ç‰¹å¾ï¼ˆä»»ä¸€åŒ¹é…å³å¯ï¼‰
    detect_patterns: List[str]  # HTML ä¸­çš„å…³é”®å­—ç¬¦ä¸²
    detect_classes: List[str]   # æ£€æµ‹çš„ class å
    detect_meta: List[str]      # meta æ ‡ç­¾å†…å®¹
    # æå–é…ç½®
    target_ids: List[str]       # æ­£æ–‡å®¹å™¨ IDï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    target_classes: List[str]   # æ­£æ–‡å®¹å™¨ classï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
    exclude_selectors: List[str]  # éœ€è¦æ’é™¤çš„é€‰æ‹©å™¨


# æ¡†æ¶é¢„è®¾é…ç½®
DOCS_PRESETS: Dict[str, DocsPreset] = {
    "docusaurus": DocsPreset(
        name="docusaurus",
        description="Docusaurus (Meta/Facebook)",
        detect_patterns=["docusaurus", "__docusaurus"],
        detect_classes=["docusaurus-wrapper", "theme-doc-markdown"],
        detect_meta=["generator.*docusaurus"],
        target_ids=["__docusaurus_skipToContent_fallback"],
        target_classes=["theme-doc-markdown", "markdown", "docMainContainer"],
        exclude_selectors=[
            ".theme-doc-sidebar-container",
            ".pagination-nav",
            ".theme-doc-toc-mobile",
            ".theme-doc-toc-desktop",
            ".theme-doc-breadcrumbs",
            "nav",
            "aside",
            ".table-of-contents",
        ],
    ),
    "mintlify": DocsPreset(
        name="mintlify",
        description="Mintlify",
        detect_patterns=["mintlify", "mintcdn.com"],
        detect_classes=["mintlify"],
        detect_meta=[],
        target_ids=["content-area"],
        target_classes=["prose", "article-content", "markdown-body"],
        exclude_selectors=[
            "nav",
            "aside",
            ".sidebar",
            ".on-this-page",
            ".page-navigation",
            "[data-testid='sidebar']",
        ],
    ),
    "gitbook": DocsPreset(
        name="gitbook",
        description="GitBook",
        detect_patterns=["gitbook", "app.gitbook.com"],
        detect_classes=["gb-root", "gitbook-root"],
        detect_meta=["generator.*gitbook"],
        target_ids=[],
        target_classes=["markdown-section", "page-inner", "book-body"],
        exclude_selectors=[
            ".book-summary",
            ".navigation",
            "nav",
            ".page-toc",
        ],
    ),
    "vuepress": DocsPreset(
        name="vuepress",
        description="VuePress",
        detect_patterns=["vuepress", "VuePress"],
        detect_classes=["theme-default-content", "vuepress"],
        detect_meta=["generator.*vuepress"],
        target_ids=[],
        target_classes=["theme-default-content", "page", "content__default"],
        exclude_selectors=[
            ".sidebar",
            ".page-nav",
            ".page-edit",
            "nav",
            ".table-of-contents",
        ],
    ),
    "mkdocs": DocsPreset(
        name="mkdocs",
        description="MkDocs / Material for MkDocs",
        detect_patterns=["mkdocs", "MkDocs"],
        detect_classes=["md-content", "md-main"],
        detect_meta=["generator.*mkdocs"],
        target_ids=["content"],
        target_classes=["md-content__inner", "md-typeset", "rst-content"],
        exclude_selectors=[
            ".md-sidebar",
            ".md-nav",
            ".md-footer",
            ".md-header",
            "nav",
        ],
    ),
    "readthedocs": DocsPreset(
        name="readthedocs",
        description="Read the Docs / Sphinx",
        detect_patterns=["readthedocs", "sphinx", "Read the Docs"],
        detect_classes=["rst-content", "wy-nav-content"],
        detect_meta=["generator.*sphinx"],
        target_ids=[],
        target_classes=["rst-content", "document", "body"],
        exclude_selectors=[
            ".wy-nav-side",
            ".wy-side-nav-search",
            ".rst-versions",
            "nav",
            ".toctree-wrapper",
        ],
    ),
    "notion": DocsPreset(
        name="notion",
        description="Notion (exported or public pages)",
        detect_patterns=["notion.so", "notion-static"],
        detect_classes=["notion-page-content", "notion-app"],
        detect_meta=[],
        target_ids=[],
        target_classes=["notion-page-content", "notion-scroller"],
        exclude_selectors=[
            ".notion-sidebar",
            ".notion-topbar",
            "nav",
        ],
    ),
    "confluence": DocsPreset(
        name="confluence",
        description="Atlassian Confluence",
        detect_patterns=["confluence", "atlassian"],
        detect_classes=["wiki-content", "confluence-content"],
        detect_meta=[],
        target_ids=["main-content", "content"],
        target_classes=["wiki-content", "confluence-content-body"],
        exclude_selectors=[
            "#navigation",
            ".aui-sidebar",
            ".page-metadata",
            "nav",
        ],
    ),
    "sphinx": DocsPreset(
        name="sphinx",
        description="Sphinx documentation",
        detect_patterns=["sphinx", "Sphinx"],
        detect_classes=["sphinxsidebar", "document"],
        detect_meta=["generator.*sphinx"],
        target_ids=["content", "main-content"],
        target_classes=["document", "body", "rst-content"],
        exclude_selectors=[
            ".sphinxsidebar",
            ".sphinxsidebarwrapper",
            ".related",
            "nav",
            ".toctree-wrapper",
        ],
    ),
    "generic": DocsPreset(
        name="generic",
        description="Generic documentation site",
        detect_patterns=[],
        detect_classes=[],
        detect_meta=[],
        target_ids=["content", "main-content", "main"],
        target_classes=["content", "main-content", "article-content", "markdown-body"],
        exclude_selectors=[
            "nav",
            "aside",
            ".sidebar",
            ".navigation",
            ".toc",
            ".table-of-contents",
        ],
    ),
}


def detect_docs_framework(page_html: str) -> Tuple[Optional[str], float, List[str]]:
    """
    è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ¡†æ¶ç±»å‹ï¼ˆT2.3ï¼‰
    
    Args:
        page_html: HTML å†…å®¹
    
    Returns:
        (æ¡†æ¶åç§°, ç½®ä¿¡åº¦ 0-1, åŒ¹é…çš„ç‰¹å¾åˆ—è¡¨) æˆ– (None, 0, [])
    """
    if not page_html:
        return None, 0.0, []
    
    html_lower = page_html.lower()
    best_match: Optional[str] = None
    best_score = 0.0
    best_signals: List[str] = []
    
    for name, preset in DOCS_PRESETS.items():
        signals: List[str] = []
        score = 0.0
        
        # æ£€æµ‹å…³é”®å­—ç¬¦ä¸²
        for pattern in preset.detect_patterns:
            if pattern.lower() in html_lower:
                signals.append(f"pattern:{pattern}")
                score += 0.3
        
        # æ£€æµ‹ class
        for cls in preset.detect_classes:
            if f'class="{cls}"' in page_html or f"class='{cls}'" in page_html or f' {cls}' in page_html:
                signals.append(f"class:{cls}")
                score += 0.25
        
        # æ£€æµ‹ metaï¼ˆæ­£åˆ™ï¼‰
        for meta_pattern in preset.detect_meta:
            if re.search(meta_pattern, page_html, re.IGNORECASE):
                signals.append(f"meta:{meta_pattern}")
                score += 0.35
        
        # å½’ä¸€åŒ–åˆ†æ•°ï¼ˆæœ€é«˜ 1.0ï¼‰
        score = min(1.0, score)
        
        if score > best_score:
            best_score = score
            best_match = name
            best_signals = signals
    
    # ç½®ä¿¡åº¦é˜ˆå€¼
    if best_score < 0.2:
        return None, 0.0, []
    
    return best_match, best_score, best_signals


def calculate_link_density(md_content: str) -> Tuple[float, int, int]:
    """
    è®¡ç®—å†…å®¹çš„é“¾æ¥å¯†åº¦ï¼ˆT2.4ï¼‰
    
    Args:
        md_content: Markdown å†…å®¹
    
    Returns:
        (é“¾æ¥å¯†åº¦æ¯”ä¾‹, é“¾æ¥æ•°é‡, æ€»å­—ç¬¦æ•°)
    """
    if not md_content:
        return 0.0, 0, 0
    
    # ç»Ÿè®¡ Markdown é“¾æ¥æ•°é‡
    link_pattern = r'\[[^\]]+\]\([^)]+\)'
    links = re.findall(link_pattern, md_content)
    link_count = len(links)
    
    # è®¡ç®—é“¾æ¥å ç”¨çš„å­—ç¬¦æ•°
    link_chars = sum(len(link) for link in links)
    
    total_chars = len(md_content)
    if total_chars == 0:
        return 0.0, 0, 0
    
    density = link_chars / total_chars
    return density, link_count, total_chars


def check_content_quality(
    md_content: str,
    url: str,
    density_threshold: float = 0.5,
) -> List[str]:
    """
    æ£€æŸ¥å†…å®¹è´¨é‡å¹¶ç”Ÿæˆè­¦å‘Šï¼ˆT2.4ï¼‰
    
    Args:
        md_content: Markdown å†…å®¹
        url: æ¥æº URL
        density_threshold: é“¾æ¥å¯†åº¦é˜ˆå€¼
    
    Returns:
        è­¦å‘Šæ¶ˆæ¯åˆ—è¡¨
    """
    warnings: List[str] = []
    
    density, link_count, total_chars = calculate_link_density(md_content)
    
    if density > density_threshold:
        warnings.append(
            f"âš ï¸ é“¾æ¥å¯†åº¦è¿‡é«˜ ({density:.1%})ï¼šå¯èƒ½åŒ…å«æœªç§»é™¤çš„å¯¼èˆªèœå•ã€‚"
            f"å»ºè®®ä½¿ç”¨ --strip-nav æˆ– --docs-preset"
        )
    
    # æ£€æµ‹è¿ç»­é“¾æ¥åˆ—è¡¨
    consecutive_links = re.findall(
        r'(?:^[ \t]*[-*]\s*\[[^\]]+\]\([^)]+\)\s*\n){10,}',
        md_content,
        re.MULTILINE
    )
    if consecutive_links:
        warnings.append(
            f"âš ï¸ æ£€æµ‹åˆ° {len(consecutive_links)} ä¸ªé•¿é“¾æ¥åˆ—è¡¨å—ã€‚"
            f"å»ºè®®ä½¿ç”¨ --anchor-list-threshold é™ä½é˜ˆå€¼"
        )
    
    return warnings


def apply_docs_preset(
    preset_name: str,
) -> Tuple[Optional[str], Optional[str], List[str]]:
    """
    åº”ç”¨æ–‡æ¡£æ¡†æ¶é¢„è®¾
    
    Args:
        preset_name: é¢„è®¾åç§°
    
    Returns:
        (target_ids, target_classes, exclude_selectors)
    """
    preset = DOCS_PRESETS.get(preset_name.lower())
    if not preset:
        return None, None, []
    
    target_ids = ",".join(preset.target_ids) if preset.target_ids else None
    target_classes = ",".join(preset.target_classes) if preset.target_classes else None
    exclude_selectors = preset.exclude_selectors
    
    return target_ids, target_classes, exclude_selectors


def get_available_presets() -> List[str]:
    """è·å–æ‰€æœ‰å¯ç”¨çš„é¢„è®¾åç§°"""
    return list(DOCS_PRESETS.keys())


# ============================================================================
# Phase 1: å¯¼èˆª/ç›®å½•å‰¥ç¦»åŠŸèƒ½ï¼ˆT1.1 - T1.5ï¼‰
# ============================================================================

@dataclass
class NavStripStats:
    """å¯¼èˆªå‰¥ç¦»ç»Ÿè®¡ä¿¡æ¯ï¼ˆT1.5 å¯è§‚æµ‹æ€§ï¼‰"""
    elements_removed: int = 0
    chars_before: int = 0
    chars_after: int = 0
    rules_matched: Dict[str, int] = field(default_factory=dict)
    anchor_lists_removed: int = 0
    anchor_lines_removed: int = 0
    
    @property
    def chars_saved(self) -> int:
        return self.chars_before - self.chars_after
    
    def add_rule_match(self, rule: str, count: int = 1) -> None:
        self.rules_matched[rule] = self.rules_matched.get(rule, 0) + count
    
    def print_summary(self, file=None) -> None:
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        if file is None:
            file = sys.stderr
        if self.elements_removed == 0 and self.anchor_lists_removed == 0:
            return
        print(f"\nğŸ“Š å¯¼èˆªå‰¥ç¦»ç»Ÿè®¡ï¼š", file=file)
        if self.elements_removed > 0:
            print(f"  â€¢ HTML å…ƒç´ ç§»é™¤ï¼š{self.elements_removed} ä¸ª", file=file)
        if self.anchor_lists_removed > 0:
            print(f"  â€¢ é”šç‚¹åˆ—è¡¨ç§»é™¤ï¼š{self.anchor_lists_removed} å—ï¼ˆå…± {self.anchor_lines_removed} è¡Œï¼‰", file=file)
        if self.chars_saved > 0:
            print(f"  â€¢ èŠ‚çœå­—ç¬¦æ•°ï¼š{self.chars_saved:,} å­—ç¬¦", file=file)
        if self.rules_matched:
            print(f"  â€¢ å‘½ä¸­è§„åˆ™ï¼š", file=file)
            for rule, count in sorted(self.rules_matched.items(), key=lambda x: -x[1]):
                print(f"    - {rule}: {count} æ¬¡", file=file)


# é»˜è®¤å¯¼èˆªå…ƒç´ é€‰æ‹©å™¨ï¼ˆ--strip-navï¼‰
DEFAULT_NAV_SELECTORS = [
    "nav",                      # <nav> æ ‡ç­¾
    "aside",                    # <aside> æ ‡ç­¾
    "[role=navigation]",        # role="navigation"
    "[role=complementary]",     # role="complementary"ï¼ˆä¾§è¾¹æ ï¼‰
    ".sidebar",                 # å¸¸è§ä¾§è¾¹æ ç±»å
    ".side-bar",
    ".sidenav",
    ".side-nav",
    ".nav-sidebar",
    ".menu",
    ".navigation",
    ".site-nav",
    ".doc-sidebar",
    ".theme-doc-sidebar-container",  # Docusaurus
    ".pagination-nav",               # Docusaurus åˆ†é¡µ
]

# é»˜è®¤é¡µå†…ç›®å½•é€‰æ‹©å™¨ï¼ˆ--strip-page-tocï¼‰
# æ³¨æ„ï¼šé¿å…ä½¿ç”¨è¿‡äºå®½æ³›çš„é€‰æ‹©å™¨ï¼ˆå¦‚ .contentsï¼‰ï¼Œå¯èƒ½è¯¯åˆ ä¸»è¦å†…å®¹
DEFAULT_TOC_SELECTORS = [
    ".toc",
    ".table-of-contents",
    ".on-this-page",
    ".page-toc",
    ".article-toc",
    # ".contents",  # å·²ç§»é™¤ï¼šä¸ Mintlify ç­‰æ¡†æ¶çš„å†…å®¹å®¹å™¨å†²çª
    "[data-toc]",
    ".theme-doc-toc-mobile",    # Docusaurus
    ".theme-doc-toc-desktop",   # Docusaurus
]


class _SimpleSelectorMatcher:
    """
    ç®€åŒ–é€‰æ‹©å™¨åŒ¹é…å™¨ï¼ˆT1.3ï¼‰
    
    æ”¯æŒçš„é€‰æ‹©å™¨è¯­æ³•ï¼š
    - tag: åŒ¹é…æ ‡ç­¾åï¼ˆå¦‚ nav, asideï¼‰
    - .class: åŒ¹é…ç±»åï¼ˆå¦‚ .sidebarï¼‰
    - #id: åŒ¹é… IDï¼ˆå¦‚ #navigationï¼‰
    - [attr]: åŒ¹é…å±æ€§å­˜åœ¨ï¼ˆå¦‚ [data-toc]ï¼‰
    - [attr=val]: åŒ¹é…å±æ€§å€¼ï¼ˆå¦‚ [role=navigation]ï¼‰
    - [attr*=val]: åŒ¹é…å±æ€§åŒ…å«å€¼ï¼ˆå¦‚ [class*=sidebar]ï¼‰
    """
    
    def __init__(self, selector: str):
        self.selector = selector.strip()
        self.tag: Optional[str] = None
        self.class_name: Optional[str] = None
        self.id_name: Optional[str] = None
        self.attr_name: Optional[str] = None
        self.attr_value: Optional[str] = None
        self.attr_contains: bool = False
        
        self._parse()
    
    def _parse(self) -> None:
        s = self.selector
        if not s:
            return
        
        if s.startswith("."):
            # .class
            self.class_name = s[1:]
        elif s.startswith("#"):
            # #id
            self.id_name = s[1:]
        elif s.startswith("[") and s.endswith("]"):
            # [attr], [attr=val], [attr*=val]
            inner = s[1:-1]
            if "*=" in inner:
                self.attr_name, self.attr_value = inner.split("*=", 1)
                self.attr_contains = True
            elif "=" in inner:
                self.attr_name, self.attr_value = inner.split("=", 1)
            else:
                self.attr_name = inner
            # Bug fix: å»é™¤å±æ€§åå’Œå±æ€§å€¼ä¸¤ä¾§çš„ç©ºç™½å’Œå¼•å·
            if self.attr_name:
                self.attr_name = self.attr_name.strip()
            if self.attr_value:
                self.attr_value = self.attr_value.strip()
                # å»é™¤æˆå¯¹çš„å•å¼•å·æˆ–åŒå¼•å·
                if len(self.attr_value) >= 2:
                    if (self.attr_value[0] == '"' and self.attr_value[-1] == '"') or \
                       (self.attr_value[0] == "'" and self.attr_value[-1] == "'"):
                        self.attr_value = self.attr_value[1:-1]
        else:
            # tag name
            self.tag = s.lower()
    
    def matches(self, tag: str, attrs: Dict[str, Optional[str]]) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ¹é…"""
        tag = tag.lower()
        
        # æ ‡ç­¾åŒ¹é…
        if self.tag and self.tag != tag:
            return False
        if self.tag and self.tag == tag:
            return True
        
        # ç±»ååŒ¹é…
        if self.class_name:
            classes = _class_list(attrs)
            if self.class_name not in classes:
                return False
            return True
        
        # ID åŒ¹é…
        if self.id_name:
            elem_id = (attrs.get("id") or "").strip()
            if elem_id != self.id_name:
                return False
            return True
        
        # å±æ€§åŒ¹é…
        if self.attr_name:
            attr_val = attrs.get(self.attr_name)
            if attr_val is None:
                return False
            if self.attr_value is None:
                return True  # ä»…æ£€æŸ¥å±æ€§å­˜åœ¨
            if self.attr_contains:
                return self.attr_value in attr_val
            return attr_val == self.attr_value
        
        return False
    
    def __repr__(self) -> str:
        return f"Selector({self.selector!r})"


class _HTMLElementStripper(HTMLParser):
    """
    HTML å…ƒç´ ç§»é™¤å™¨ï¼ˆT1.1, T1.2ï¼‰
    
    ç§»é™¤åŒ¹é…æŒ‡å®šé€‰æ‹©å™¨çš„ HTML å…ƒç´ åŠå…¶å†…å®¹ã€‚
    """
    
    # HTML5 void elements - è¿™äº›æ ‡ç­¾æ²¡æœ‰é—­åˆæ ‡ç­¾
    # https://html.spec.whatwg.org/multipage/syntax.html#void-elements
    VOID_ELEMENTS = frozenset({
        "area", "base", "br", "col", "embed", "hr", "img", "input",
        "link", "meta", "param", "source", "track", "wbr",
        # å·²åºŸå¼ƒä½†ä»å¸¸è§
        "command", "keygen", "menuitem",
    })
    
    def __init__(self, selectors: List[str]):
        super().__init__(convert_charrefs=True)
        self.matchers = [_SimpleSelectorMatcher(s) for s in selectors if s.strip()]
        self.buf: List[str] = []
        self.skip_depth = 0
        self.skip_tag: Optional[str] = None
        self.stats = NavStripStats()
    
    def _should_skip(self, tag: str, attrs: Dict[str, Optional[str]]) -> Optional[str]:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥è·³è¿‡è¯¥å…ƒç´ ï¼Œè¿”å›åŒ¹é…çš„é€‰æ‹©å™¨"""
        for matcher in self.matchers:
            if matcher.matches(tag, attrs):
                return matcher.selector
        return None
    
    @staticmethod
    def _attrs_to_str(attrs_list: Sequence[Tuple[str, Optional[str]]]) -> str:
        parts = []
        for name, value in attrs_list:
            if value is None:
                parts.append(name)
            else:
                escaped = htmllib.escape(str(value), quote=True)
                parts.append(f'{name}="{escaped}"')
        return " ".join(parts)
    
    def handle_starttag(self, tag: str, attrs_list: Sequence[Tuple[str, Optional[str]]]) -> None:
        tag = tag.lower()
        attrs = dict(attrs_list)
        
        if self.skip_depth > 0:
            # å·²ç»åœ¨è·³è¿‡çš„å…ƒç´ å†…éƒ¨
            # Bug fix: void æ ‡ç­¾æ²¡æœ‰ endtagï¼Œä¸åº”é€’å¢æ·±åº¦è®¡æ•°
            if tag not in self.VOID_ELEMENTS:
                self.skip_depth += 1
            return
        
        matched = self._should_skip(tag, attrs)
        if matched:
            # å¼€å§‹è·³è¿‡
            # Bug fix: å¦‚æœåŒ¹é…çš„æ˜¯ void æ ‡ç­¾ï¼Œç›´æ¥ç§»é™¤ä¸éœ€è¦æ·±åº¦è®¡æ•°
            if tag in self.VOID_ELEMENTS:
                self.stats.elements_removed += 1
                self.stats.add_rule_match(matched)
                return
            self.skip_depth = 1
            self.skip_tag = tag
            self.stats.elements_removed += 1
            self.stats.add_rule_match(matched)
            return
        
        # æ­£å¸¸è¾“å‡º
        attr_str = self._attrs_to_str(attrs_list)
        if attr_str:
            self.buf.append(f"<{tag} {attr_str}>")
        else:
            self.buf.append(f"<{tag}>")
    
    def handle_startendtag(self, tag: str, attrs_list: Sequence[Tuple[str, Optional[str]]]) -> None:
        tag = tag.lower()
        attrs = dict(attrs_list)
        
        if self.skip_depth > 0:
            return
        
        matched = self._should_skip(tag, attrs)
        if matched:
            self.stats.elements_removed += 1
            self.stats.add_rule_match(matched)
            return
        
        attr_str = self._attrs_to_str(attrs_list)
        if attr_str:
            self.buf.append(f"<{tag} {attr_str}/>")
        else:
            self.buf.append(f"<{tag}/>")
    
    def handle_endtag(self, tag: str) -> None:
        tag = tag.lower()
        
        if self.skip_depth > 0:
            self.skip_depth -= 1
            if self.skip_depth == 0:
                self.skip_tag = None
            return
        
        self.buf.append(f"</{tag}>")
    
    def handle_data(self, data: str) -> None:
        if self.skip_depth > 0:
            return
        self.buf.append(htmllib.escape(data, quote=False))
    
    def handle_comment(self, data: str) -> None:
        if self.skip_depth > 0:
            return
        self.buf.append(f"<!--{data}-->")
    
    def handle_decl(self, decl: str) -> None:
        if self.skip_depth > 0:
            return
        self.buf.append(f"<!{decl}>")
    
    def get_result(self) -> str:
        return "".join(self.buf)


def strip_html_elements(
    html_content: str,
    selectors: List[str],
    stats: Optional[NavStripStats] = None,
) -> Tuple[str, NavStripStats]:
    """
    ä» HTML ä¸­ç§»é™¤åŒ¹é…æŒ‡å®šé€‰æ‹©å™¨çš„å…ƒç´ 
    
    Args:
        html_content: HTML å†…å®¹
        selectors: é€‰æ‹©å™¨åˆ—è¡¨
        stats: å¯é€‰çš„ç»Ÿè®¡å¯¹è±¡ï¼ˆç”¨äºç´¯è®¡ç»Ÿè®¡ï¼‰
    
    Returns:
        (å¤„ç†åçš„ HTML, ç»Ÿè®¡ä¿¡æ¯)
    """
    if not selectors or not html_content:
        return html_content, stats or NavStripStats()
    
    if stats is None:
        stats = NavStripStats()
    
    stats.chars_before = len(html_content)
    
    stripper = _HTMLElementStripper(selectors)
    stripper.feed(html_content)
    result = stripper.get_result()
    
    # åˆå¹¶ç»Ÿè®¡
    stats.elements_removed += stripper.stats.elements_removed
    for rule, count in stripper.stats.rules_matched.items():
        stats.add_rule_match(rule, count)
    
    stats.chars_after = len(result)
    
    return result, stats


def strip_anchor_lists(
    md_content: str,
    threshold: int = 20,
    stats: Optional[NavStripStats] = None,
) -> Tuple[str, NavStripStats]:
    """
    ç§»é™¤é«˜é“¾æ¥å¯†åº¦çš„ç›®å½•å—ï¼ˆT1.4ï¼‰
    
    æ£€æµ‹è¿ç»­çš„é“¾æ¥åˆ—è¡¨ï¼Œè¶…è¿‡é˜ˆå€¼æ—¶ç§»é™¤ã€‚æ”¯æŒï¼š
    - å†…éƒ¨é”šç‚¹ï¼š`- [text](#anchor)`
    - å¤–éƒ¨é“¾æ¥ï¼š`- [text](https://...)`
    - å¸¦æ ‡é¢˜çš„å¯¼èˆªåŒºå—ï¼ˆæ ‡é¢˜ + é“¾æ¥åˆ—è¡¨æ€»è¡Œæ•°è¶…è¿‡é˜ˆå€¼æ—¶ï¼‰
    
    Args:
        md_content: Markdown å†…å®¹
        threshold: è¿ç»­é“¾æ¥è¡Œæ•°é˜ˆå€¼ï¼ˆé»˜è®¤ 20ï¼‰ï¼Œè®¾ä¸º 0 å…³é—­æ­¤åŠŸèƒ½
        stats: å¯é€‰çš„ç»Ÿè®¡å¯¹è±¡
    
    Returns:
        (å¤„ç†åçš„ Markdown, ç»Ÿè®¡ä¿¡æ¯)
    """
    if stats is None:
        stats = NavStripStats()
    
    if threshold <= 0 or not md_content:
        return md_content, stats
    
    removed_count = 0
    removed_lines = 0
    result = md_content
    
    # æ¨¡å¼ 1: ç§»é™¤å¸¦æ ‡é¢˜çš„å¯¼èˆªåŒºå—ï¼ˆå¦‚ "##### Start Here" åè·Ÿé“¾æ¥åˆ—è¡¨ï¼‰
    # å…³é”®ä¿®å¤ï¼šnav_section_pattern ç°åœ¨ä¹Ÿå— threshold æ§åˆ¶
    # æ ‡é¢˜å  1 è¡Œï¼Œæ‰€ä»¥é“¾æ¥åˆ—è¡¨è‡³å°‘éœ€è¦ (threshold - 1) è¡Œ
    nav_min_links = max(3, threshold - 1)  # è‡³å°‘ 3 è¡Œï¼Œé¿å…è¯¯åˆ çŸ­åˆ—è¡¨
    nav_section_pattern = (
        r'(#{3,6}\s+[^\n]+\n\n?'  # æ ‡é¢˜è¡Œï¼ˆ##### ç­‰ï¼‰
        r'(?:[ \t]*[-*]\s*\[[^\]]+\]\([^)]+\)\s*\n){' + str(nav_min_links) + r',})'
    )
    
    def replace_nav_section(match: re.Match) -> str:
        nonlocal removed_count, removed_lines
        block = match.group(0)
        lines = block.count('\n')
        removed_count += 1
        removed_lines += lines
        return ''  # å®Œå…¨ç§»é™¤ï¼Œä¸ç•™æ³¨é‡Š
    
    result = re.sub(nav_section_pattern, replace_nav_section, result, flags=re.MULTILINE)
    
    # æ¨¡å¼ 2: ç§»é™¤ç‹¬ç«‹çš„é•¿é“¾æ¥åˆ—è¡¨ï¼ˆè¶…è¿‡é˜ˆå€¼ï¼‰
    list_pattern = r'((?:^[ \t]*(?:[-*]|\d+\.)\s*\[[^\]]+\]\([^)]+\)\s*\n){' + str(threshold) + r',})'
    
    def replace_list(match: re.Match) -> str:
        nonlocal removed_count, removed_lines
        block = match.group(0)
        lines = block.count('\n')
        removed_count += 1
        removed_lines += lines
        return ''  # å®Œå…¨ç§»é™¤
    
    result = re.sub(list_pattern, replace_list, result, flags=re.MULTILINE)
    
    # æ¨¡å¼ 3: æ¸…ç†å­¤ç«‹çš„æ ‡é¢˜ï¼ˆæ ‡é¢˜åé¢åªæœ‰ç©ºè¡Œæˆ–å¦ä¸€ä¸ªæ ‡é¢˜ï¼‰
    # ä»…åœ¨ç§»é™¤äº†å¯¼èˆªåŒºå—åæ‰æ‰§è¡Œï¼Œé¿å…è¯¯åˆ æ­£å¸¸æ ‡é¢˜
    if removed_count > 0:
        orphan_title_pattern = r'#{3,6}\s+[^\n]+\n\n(?=#{3,6}\s+|$|\n*---)'
        result = re.sub(orphan_title_pattern, '', result, flags=re.MULTILINE)
    
    # æ¨¡å¼ 4: æ¸…ç†è¿ç»­çš„ç©ºè¡Œï¼ˆè¶…è¿‡ 2 ä¸ªï¼‰
    result = re.sub(r'\n{4,}', '\n\n\n', result)
    
    stats.anchor_lists_removed += removed_count
    stats.anchor_lines_removed += removed_lines
    if removed_count > 0:
        stats.add_rule_match(f"nav-block-strip", removed_count)
    
    return result, stats


def get_strip_selectors(
    strip_nav: bool = False,
    strip_page_toc: bool = False,
    exclude_selectors: Optional[str] = None,
) -> List[str]:
    """
    æ ¹æ®å‚æ•°ç»„åˆç”Ÿæˆé€‰æ‹©å™¨åˆ—è¡¨
    
    Args:
        strip_nav: æ˜¯å¦ç§»é™¤å¯¼èˆªå…ƒç´ 
        strip_page_toc: æ˜¯å¦ç§»é™¤é¡µå†…ç›®å½•
        exclude_selectors: è‡ªå®šä¹‰é€‰æ‹©å™¨ï¼ˆé€—å·åˆ†éš”ï¼‰
    
    Returns:
        é€‰æ‹©å™¨åˆ—è¡¨
    """
    selectors: List[str] = []
    
    if strip_nav:
        selectors.extend(DEFAULT_NAV_SELECTORS)
    
    if strip_page_toc:
        selectors.extend(DEFAULT_TOC_SELECTORS)
    
    if exclude_selectors:
        # è§£æé€—å·åˆ†éš”çš„è‡ªå®šä¹‰é€‰æ‹©å™¨
        custom = [s.strip() for s in exclude_selectors.split(",") if s.strip()]
        selectors.extend(custom)
    
    # å»é‡ä½†ä¿æŒé¡ºåº
    seen = set()
    unique = []
    for s in selectors:
        if s not in seen:
            seen.add(s)
            unique.append(s)
    
    return unique


class _TextLenExtractor(HTMLParser):
    def __init__(self) -> None:
        super().__init__(convert_charrefs=True)
        self.n = 0

    def handle_data(self, data: str) -> None:
        if not data or data.isspace():
            return
        self.n += len(re.sub(r"\s+", " ", data.strip()))


def html_text_len(html: str) -> int:
    parser = _TextLenExtractor()
    parser.feed(html or "")
    return parser.n


def sniff_ext(data: bytes) -> Optional[str]:
    if data.startswith(b"\x89PNG\r\n\x1a\n"):
        return ".png"
    if data.startswith(b"\xff\xd8\xff"):
        return ".jpg"
    if data.startswith(b"GIF87a") or data.startswith(b"GIF89a"):
        return ".gif"
    if data.startswith(b"RIFF") and len(data) >= 12 and data[8:12] == b"WEBP":
        return ".webp"
    head = data[:200].lstrip()
    if head.startswith(b"<?xml") or head.startswith(b"<svg"):
        return ".svg"
    return None


def ext_from_content_type(content_type: Optional[str]) -> Optional[str]:
    if not content_type:
        return None
    ct = content_type.split(";", 1)[0].strip().lower()
    return {
        "image/jpeg": ".jpg",
        "image/jpg": ".jpg",
        "image/png": ".png",
        "image/webp": ".webp",
        "image/gif": ".gif",
        "image/svg+xml": ".svg",
        "image/avif": ".avif",
    }.get(ct)


def is_probable_icon(url: str) -> bool:
    low = url.lower()
    return (
        "favicon" in low
        or "/icon/" in low
        or low.endswith(".ico")
        or "pinned-octocat" in low
        or "/apple-touch-icon" in low
    )


class ImageURLCollector(HTMLParser):
    def __init__(self, base_url: str):
        super().__init__(convert_charrefs=True)
        self.base_url = base_url
        self.image_urls: List[str] = []
        self._in_picture = False
        self._picture_sources: List[str] = []

    def _add_url(self, raw: Optional[str]) -> None:
        if not raw:
            return
        raw = htmllib.unescape(raw).strip()
        if not raw or raw.startswith("data:"):
            return
        full = urljoin(self.base_url, raw)
        if is_probable_icon(full):
            return
        self.image_urls.append(full)

    def handle_starttag(self, tag: str, attrs_list: Sequence[Tuple[str, Optional[str]]]) -> None:
        tag = tag.lower()
        attrs = dict(attrs_list)

        if tag == "picture":
            self._in_picture = True
            self._picture_sources = []
            return

        if tag == "source" and self._in_picture:
            srcset = attrs.get("srcset")
            if srcset:
                first = srcset.split(",")[0].strip().split(" ")[0]
                self._picture_sources.append(first)
            return

        if tag == "img":
            # ä¼˜å…ˆ picture/source çš„ srcset
            if self._in_picture and self._picture_sources:
                self._add_url(self._picture_sources[0])
                self._picture_sources = []
                return

            candidates = [
                attrs.get("src"),
                attrs.get("data-src"),
                attrs.get("data-original"),
                attrs.get("data-lazy-src"),
            ]
            src = next((c for c in candidates if c), None)
            if (not src) or (src and src.startswith("data:")):
                srcset = attrs.get("srcset")
                if srcset:
                    src = srcset.split(",")[0].strip().split(" ")[0]
            self._add_url(src)

    def handle_endtag(self, tag: str) -> None:
        tag = tag.lower()
        if tag == "picture":
            self._in_picture = False
            self._picture_sources = []


def uniq_preserve_order(items: Iterable[str]) -> List[str]:
    seen = set()
    out: List[str] = []
    for it in items:
        if it in seen:
            continue
        seen.add(it)
        out.append(it)
    return out


def download_images(
    session: requests.Session,
    image_urls: Sequence[str],
    assets_dir: str,
    md_dir: str,
    timeout_s: int,
    retries: int = 3,
    best_effort: bool = False,
    *,
    page_url: str,
    redact_urls: bool = True,
    max_image_bytes: int = _DEFAULT_MAX_IMAGE_BYTES,
) -> Dict[str, str]:
    os.makedirs(assets_dir, exist_ok=True)
    url_to_local: Dict[str, str] = {}
    anon_session = _create_anonymous_image_session(session)
    referer = redact_url(page_url) if redact_urls else page_url
    max_bytes: Optional[int] = max_image_bytes if (max_image_bytes and max_image_bytes > 0) else None
    known_image_exts = {".png", ".jpg", ".jpeg", ".gif", ".webp", ".svg", ".avif", ".bmp", ".ico"}

    for idx, img_url in enumerate(image_urls, start=1):
        if not img_url:
            continue
        parsed_img = urlparse(img_url)
        if parsed_img.scheme not in ("http", "https"):
            # ä¸æ”¯æŒ data/file ç­‰ schemeï¼ˆä¹Ÿé¿å…è§¦è¾¾æœ¬åœ° file://ï¼‰
            continue

        last_err: Optional[Exception] = None
        r: Optional[requests.Response] = None
        for attempt in range(1, retries + 1):
            try:
                # ä½¿ç”¨å®‰å…¨çš„å›¾ç‰‡è·å–å‡½æ•°ï¼Œæ‰‹åŠ¨å¤„ç†é‡å®šå‘å¹¶åœ¨è·¨åŸŸæ—¶åˆ‡æ¢åˆ°å¹²å‡€ session
                r = _safe_image_get(
                    img_url=img_url,
                    page_url=page_url,
                    session=session,
                    anon_session=anon_session,
                    timeout_s=timeout_s,
                    referer=referer,
                )
                r.raise_for_status()
                break
            except Exception as e:  # noqa: BLE001 - CLI tool wants retries on network errors
                last_err = e
                # å…³é”®ï¼šå¦‚æœ raise_for_status() å¤±è´¥ï¼ˆ4xx/5xxï¼‰ï¼Œr è™½ç„¶é None ä½†å†…å®¹æ— æ•ˆ
                # å¿…é¡»é‡ç½®ä¸º Noneï¼Œå¦åˆ™åç»­ `if r is None:` è¯¯åˆ¤ä¸ºæˆåŠŸ
                if r is not None:
                    try:
                        r.close()
                    except Exception:
                        pass
                    r = None
                if attempt >= retries:
                    break
                time.sleep(min(2.0, 0.4 * attempt))

        if r is None:
            if best_effort:
                print(f"è­¦å‘Šï¼šå›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œå·²è·³è¿‡ï¼š{img_url}\n  - é”™è¯¯ï¼š{last_err}", file=sys.stderr)
                continue
            raise last_err or RuntimeError("image download failed")

        try:
            # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶åï¼ˆæ‰©å±•åä¼˜å…ˆå– URL pathï¼Œå…¶æ¬¡ Content-Typeï¼Œå†æ¬¡å—…æ¢é¦–å—å†…å®¹ï¼‰
            base = os.path.basename(parsed_img.path.rstrip("/"))
            base = unquote(base) or f"image-{idx}"
            name_root, name_ext = os.path.splitext(base)

            it = r.iter_content(chunk_size=1024 * 64)
            head = b""
            for chunk in it:
                if chunk:
                    head = chunk
                    break

            if (not name_ext) or (name_ext.lower() not in known_image_exts):
                detected = ext_from_content_type(r.headers.get("Content-Type") if r else None) or sniff_ext(head or b"")
                if detected:
                    name_ext = detected
                elif not name_ext:
                    name_ext = ".bin"

            safe_root = _sanitize_filename_part(name_root)
            filename = f"{idx:02d}-{safe_root}{name_ext}"
            filename = _safe_path_length(assets_dir, filename)
            local_path = os.path.join(assets_dir, filename)
            tmp_path = local_path + ".part"

            size = 0
            try:
                with open(tmp_path, "wb") as f:
                    if head:
                        f.write(head)
                        size += len(head)
                        if max_bytes is not None and size > max_bytes:
                            raise RuntimeError(f"å›¾ç‰‡è¿‡å¤§ï¼ˆ>{max_bytes} bytesï¼‰")
                    for chunk in it:
                        if not chunk:
                            continue
                        size += len(chunk)
                        if max_bytes is not None and size > max_bytes:
                            raise RuntimeError(f"å›¾ç‰‡è¿‡å¤§ï¼ˆ>{max_bytes} bytesï¼‰")
                        f.write(chunk)
                os.replace(tmp_path, local_path)
            finally:
                if os.path.exists(tmp_path):
                    try:
                        os.remove(tmp_path)
                    except OSError:
                        pass
        except Exception as e:
            if best_effort:
                print(f"è­¦å‘Šï¼šå›¾ç‰‡ä¿å­˜å¤±è´¥ï¼Œå·²è·³è¿‡ï¼š{img_url}\n  - é”™è¯¯ï¼š{e}", file=sys.stderr)
                continue
            raise
        finally:
            try:
                r.close()
            except Exception:
                pass

        local_abs = os.path.abspath(local_path)
        md_dir_abs = os.path.abspath(md_dir or ".")
        rel = os.path.relpath(local_abs, start=md_dir_abs)
        url_to_local[img_url] = rel.replace("\\", "/")

    return url_to_local


VOID_TAGS = {
    "area",
    "base",
    "br",
    "col",
    "embed",
    "hr",
    "img",
    "input",
    "link",
    "meta",
    "param",
    "source",
    "track",
    "wbr",
    # svg è‡ªé—­åˆå¸¸è§å½¢çŠ¶
    "path",
    "rect",
    "circle",
    "polygon",
    "polyline",
    "line",
    "ellipse",
}

SKIP_TAGS = {
    "script",
    "style",
    "svg",
    "video",
    "audio",
}


def _class_list(attrs: Dict[str, Optional[str]]) -> List[str]:
    cls = attrs.get("class")
    if not cls:
        return []
    if isinstance(cls, str):
        return [c for c in cls.split() if c]
    return [str(cls)]


class HTMLToMarkdown(HTMLParser):
    def __init__(self, base_url: str, url_to_local: Dict[str, str], keep_html: bool = False):
        super().__init__(convert_charrefs=True)
        self.base_url = base_url
        self.url_to_local = url_to_local
        self.keep_html = keep_html  # æ˜¯å¦å¯¹å¤æ‚è¡¨æ ¼ä¿ç•™ HTMLï¼ˆcolspan/rowspan/nested tableï¼‰
        self.out: List[str] = []

        self.skip_stack: List[str] = []

        self.in_heading = False
        self.heading_out_start: Optional[int] = None
        self.heading_text: List[str] = []

        self.in_pre = False
        self.pre_buf: List[str] = []
        self.pre_lang: str = ""

        self.in_math_script = False
        self.math_script_display = False
        self.math_script_buf: List[str] = []

        self.in_annotation_tex = False
        self.annotation_display = False
        self.annotation_buf: List[str] = []

        self.tag_stack: List[Tuple[str, bool, bool]] = []
        self.katex_depth = 0
        self.katex_display_depth = 0

        self.in_inline_code = False
        self.inline_code_buf: List[str] = []

        self.in_a = False
        self.a_href: Optional[str] = None
        self.a_text: List[str] = []

        # å…¼å®¹ Python < 3.10ï¼šé¿å… PEP604 `int | str` å†™æ³•
        self.list_stack: List[Dict[str, Union[int, str]]] = []

        self.in_table = False
        self.table_depth = 0
        self.table_rows: List[List[str]] = []
        self.current_row: Optional[List[str]] = None
        self.in_cell = False
        self.cell_buf: List[str] = []
        self.table_in_a = False
        self.table_a_href: Optional[str] = None
        self.table_a_text: List[str] = []

        # å¤æ‚è¡¨æ ¼çš„ HTML åŸæ ·ä¿ç•™æ¨¡å¼
        self.raw_table_mode = False
        self.raw_table_buf: List[str] = []
        self.raw_table_depth = 0
        self.table_capture_html = False
        self.table_capture_buf: List[str] = []
        self.table_capture_depth = 0
        self.table_is_complex = False

    @staticmethod
    def _is_complex_table_attrs(attrs: Dict[str, Optional[str]]) -> bool:
        """æ£€æµ‹è¡¨æ ¼å•å…ƒæ ¼å±æ€§æ˜¯å¦åŒ…å« colspan/rowspanï¼ˆå¤æ‚è¡¨æ ¼æ ‡å¿—ï¼‰ã€‚"""
        colspan = attrs.get("colspan")
        rowspan = attrs.get("rowspan")
        if colspan and colspan != "1":
            return True
        if rowspan and rowspan != "1":
            return True
        return False

    @staticmethod
    def _attrs_to_str(attrs_list: Sequence[Tuple[str, Optional[str]]]) -> str:
        """å°†å±æ€§åˆ—è¡¨è½¬æ¢ä¸º HTML å±æ€§å­—ç¬¦ä¸²ã€‚"""
        parts = []
        for name, value in attrs_list:
            safe_name = (name or "").strip()
            if not safe_name:
                continue
            low = safe_name.lower()

            # å®‰å…¨å‡€åŒ–ï¼šç§»é™¤äº‹ä»¶å±æ€§ï¼ˆonclick/onerror/...ï¼‰
            if low.startswith("on"):
                continue

            # å®‰å…¨å‡€åŒ–ï¼šè¿‡æ»¤ javascript: / vbscript:ï¼›è¿‡æ»¤ file:ï¼ˆé¿å…åç»­æ¸²æŸ“é“¾è·¯è§¦è¾¾æœ¬åœ°æ–‡ä»¶ï¼‰
            if value is not None and low in ("href", "src", "xlink:href", "srcset"):
                v = str(value).strip()
                if re.match(r"(?i)^(?:javascript|vbscript):", v):
                    continue
                if low in ("src", "xlink:href") and v.lower().startswith("file:"):
                    continue

            if value is None:
                parts.append(safe_name)
            else:
                escaped = htmllib.escape(str(value), quote=True)
                parts.append(f'{safe_name}="{escaped}"')
        return " ".join(parts)

    @staticmethod
    def _extract_code_language(attrs: Dict[str, Optional[str]]) -> str:
        # å¸¸è§å½¢æ€ï¼šclass="language-python" / class="lang-python" / data-language="python" / class="python"
        for key in ("data-language", "data-lang", "lang"):
            val = (attrs.get(key) or "").strip()
            if val:
                return val.split()[0]

        classes = _class_list(attrs)
        for c in classes:
            m = re.match(r"^(?:language|lang)[-_]([A-Za-z0-9_+.-]+)$", c)
            if m:
                return m.group(1)

        # å…œåº•ï¼šéƒ¨åˆ†ç«™ç‚¹ä¼šç›´æ¥ç”¨ class="python"
        known = {
            "bash",
            "c",
            "cpp",
            "csharp",
            "css",
            "go",
            "html",
            "java",
            "javascript",
            "js",
            "json",
            "kotlin",
            "perl",
            "php",
            "python",
            "py",
            "ruby",
            "rust",
            "scala",
            "shell",
            "sh",
            "sql",
            "swift",
            "toml",
            "typescript",
            "ts",
            "xml",
            "yaml",
            "yml",
        }
        for c in classes:
            low = c.lower()
            if low in known:
                return low

        return ""

    @staticmethod
    def _sanitize_fence_language(lang: str) -> str:
        parts = (lang or "").strip().split()
        lang = parts[0] if parts else ""
        if not lang:
            return ""
        if not re.match(r"^[A-Za-z0-9_+.-]+$", lang):
            return ""
        return lang

    def _tail(self) -> str:
        return "".join(self.out[-8:]) if self.out else ""

    def _ensure_blank_line(self) -> None:
        if not self.out:
            return
        tail = self._tail()
        if not tail.endswith("\n\n"):
            if tail.endswith("\n"):
                self.out.append("\n")
            else:
                self.out.append("\n\n")

    def _append_text(self, text: str) -> None:
        if not text:
            return
        text = re.sub(r"[ \t\r\f\v]+", " ", text)
        # ä¸€äº›ç«™ç‚¹åœ¨ <strong>/<em>/<code> åä¼šå¸¦ç©ºæ ¼ï¼Œé¿å…è¾“å‡ºæˆ "** foo**"
        if self.out:
            tail = self._tail()
            if tail.endswith(("**", "*", "`")):
                text = text.lstrip()
        # é¿å…æŠŠä¸¤ä¸ªâ€œè¯â€ç²˜åœ¨ä¸€èµ·
        if self.out:
            prev = self._tail()[-1:]
            if prev and prev not in ("\n", " ", "(", "[", "*", "`", "_") and text[:1] not in (" ", "\n", ".", ",", ":", ";", ")", "]"):
                self.out.append(" ")
        self.out.append(text)

    def _table_append(self, text: str) -> None:
        if not text:
            return
        text = re.sub(r"[ \t\r\f\v]+", " ", text)
        self.cell_buf.append(text)

    def _switch_to_raw_table_mode(self, current_attrs: Sequence[Tuple[str, Optional[str]]]) -> None:
        """åˆ‡æ¢åˆ°åŸå§‹ HTML æ¨¡å¼ï¼Œé‡å»ºä¹‹å‰å·²å¤„ç†çš„è¡¨æ ¼å†…å®¹ã€‚"""
        self.raw_table_mode = True
        self.raw_table_buf = ["<table>"]
        # é‡å»ºä¹‹å‰å·²å¤„ç†çš„è¡Œ
        for row in self.table_rows:
            self.raw_table_buf.append("<tr>")
            for cell in row:
                # ä¹‹å‰çš„å†…å®¹éƒ½å½“ä½œ td å¤„ç†
                self.raw_table_buf.append(f"<td>{cell}</td>")
            self.raw_table_buf.append("</tr>")
        # é‡å»ºå½“å‰è¡Œï¼ˆå¦‚æœæœ‰ï¼‰
        if self.current_row is not None:
            self.raw_table_buf.append("<tr>")
            for cell in self.current_row:
                self.raw_table_buf.append(f"<td>{cell}</td>")
            # å½“å‰å•å…ƒæ ¼çš„å†…å®¹
            if self.cell_buf:
                cell_content = "".join(self.cell_buf)
                self.raw_table_buf.append(f"<td>{cell_content}</td>")
        # æ·»åŠ è§¦å‘åˆ‡æ¢çš„å•å…ƒæ ¼
        attr_str = self._attrs_to_str(current_attrs)
        tag = "td"  # é»˜è®¤ tdï¼Œå®é™…ä¸Šå¯èƒ½æ˜¯ th
        if attr_str:
            self.raw_table_buf.append(f"<{tag} {attr_str}>")
        else:
            self.raw_table_buf.append(f"<{tag}>")
        # é‡ç½®æ™®é€šè¡¨æ ¼çŠ¶æ€
        self.table_rows = []
        self.current_row = None
        self.in_cell = False
        self.cell_buf = []

    def _should_skip(self, tag: str, attrs: Dict[str, Optional[str]]) -> bool:
        if tag == "script":
            # MathJax å¸¸ç”¨ï¼š<script type="math/tex"> æˆ– <script type="math/tex; mode=display">
            t = (attrs.get("type") or "").strip().lower()
            if t.startswith("math/tex"):
                return False
        if tag in SKIP_TAGS:
            return True

        # Ghost ç­‰ç«™ç‚¹çš„ video/file/audio UIï¼ˆä½†ä¿ç•™ figure/figcaption çš„æ­£æ–‡ï¼‰
        classes = _class_list(attrs)
        if classes and tag not in ("figure", "figcaption"):
            if any(c.startswith(("kg-video-", "kg-audio-", "kg-file-")) for c in classes):
                return True
            if any("kg-video" in c for c in classes):
                return True

        # çº¯äº¤äº’å…ƒç´ 
        if tag in ("button",):
            return True

        return False

    def _enter_skip(self, tag: str) -> None:
        if tag in VOID_TAGS:
            return
        self.skip_stack.append(tag)

    def handle_starttag(self, tag: str, attrs_list: Sequence[Tuple[str, Optional[str]]]) -> None:
        tag = tag.lower()
        attrs = dict(attrs_list)

        # è¿½è¸ª tag åµŒå¥—ï¼Œç”¨äºåˆ¤æ–­ KaTeX display/inlineï¼Œå¹¶é¿å…è¾“å‡º KaTeX æ¸²æŸ“åçš„é‡å¤æ–‡æœ¬ã€‚
        # æ³¨æ„ï¼šVOID_TAGS ä¸å‹æ ˆï¼Œå› ä¸ºå®ƒä»¬æ²¡æœ‰å¯¹åº”çš„ç»“æŸæ ‡ç­¾ã€‚
        if tag not in VOID_TAGS:
            is_katex = False
            is_katex_display = False
            if tag == "span":
                classes = _class_list(attrs)
                is_katex_display = "katex-display" in classes
                is_katex = is_katex_display or ("katex" in classes)
            self.tag_stack.append((tag, is_katex, is_katex_display))
            if is_katex:
                self.katex_depth += 1
            if is_katex_display:
                self.katex_display_depth += 1

        if self.skip_stack:
            if self._should_skip(tag, attrs):
                self._enter_skip(tag)
            return

        if self._should_skip(tag, attrs):
            self._enter_skip(tag)
            return

        # tableï¼ˆè‹¥ table å†…å†å‡ºç° tableï¼Œè§†ä¸ºå¤æ‚ç»“æ„ï¼šä¸è¦é‡ç½®çŠ¶æ€ï¼‰
        if tag == "table" and self.in_table:
            self.table_depth += 1
            if self.keep_html:
                self.table_is_complex = True
            if self.table_capture_html:
                attr_str = self._attrs_to_str(attrs_list)
                if attr_str:
                    self.table_capture_buf.append(f"<table {attr_str}>")
                else:
                    self.table_capture_buf.append("<table>")
                self.table_capture_depth += 1
                self.table_is_complex = True
            return

        # tableï¼ˆé¡¶å±‚ï¼‰
        if tag == "table":
            self._ensure_blank_line()
            self.in_table = True
            self.table_depth = 1
            self.table_rows = []
            # å¦‚æœå¯ç”¨ keep_htmlï¼šä» table å¼€å§‹å°±åŒæ­¥æ•è· HTMLï¼Œé‡åˆ°å¤æ‚ç»“æ„æ—¶ç›´æ¥è¾“å‡ºæ•è·å†…å®¹ã€‚
            self.raw_table_mode = False
            self.raw_table_buf = []
            self.raw_table_depth = 1
            self.table_capture_html = bool(self.keep_html)
            self.table_is_complex = False
            if self.table_capture_html:
                attr_str = self._attrs_to_str(attrs_list)
                if attr_str:
                    self.table_capture_buf = [f"<table {attr_str}>"]
                else:
                    self.table_capture_buf = ["<table>"]
                self.table_capture_depth = 1
            return

        # å¤æ‚è¡¨æ ¼çš„åŸå§‹ HTML æ¨¡å¼ï¼šç›´æ¥è®°å½•æ‰€æœ‰å†…å®¹
        if self.raw_table_mode:
            attr_str = self._attrs_to_str(attrs_list)
            if attr_str:
                self.raw_table_buf.append(f"<{tag} {attr_str}>")
            else:
                self.raw_table_buf.append(f"<{tag}>")
            if tag == "table":
                self.raw_table_depth += 1
            return

        if self.in_table:
            # åµŒå¥— table å†…éƒ¨ï¼šé¿å…æŠŠå†…å±‚ tr/td è¯¯å½“å¤–å±‚è¡¨æ ¼ç»“æ„è§£æï¼›åªåš HTML æ•è·ã€‚
            if self.table_depth > 1:
                if self.table_capture_html:
                    attr_str = self._attrs_to_str(attrs_list)
                    if attr_str:
                        self.table_capture_buf.append(f"<{tag} {attr_str}>")
                    else:
                        self.table_capture_buf.append(f"<{tag}>")
                    if tag == "table":
                        self.table_capture_depth += 1
                        self.table_is_complex = True
                return

            if self.table_capture_html:
                attr_str = self._attrs_to_str(attrs_list)
                if attr_str:
                    self.table_capture_buf.append(f"<{tag} {attr_str}>")
                else:
                    self.table_capture_buf.append(f"<{tag}>")

            if tag == "tr":
                self.current_row = []
            elif tag in ("th", "td"):
                # æ£€æµ‹æ˜¯å¦ä¸ºå¤æ‚è¡¨æ ¼ï¼ˆå« colspan/rowspanï¼‰
                if self.keep_html and self._is_complex_table_attrs(attrs):
                    self.table_is_complex = True
                self.in_cell = True
                self.cell_buf = []
            elif tag == "br" and self.in_cell:
                # Markdown è¡¨æ ¼å•å…ƒæ ¼é‡Œä¿ç•™æ¢è¡Œï¼šç”¨ <br>ï¼ˆå¤šæ•°æ¸²æŸ“å™¨æ”¯æŒï¼‰
                if self.cell_buf and (self.cell_buf[-1].strip().lower() != "<br>"):
                    self.cell_buf.append("<br>")
            elif tag in ("p", "div", "li") and self.in_cell:
                # è¡¨æ ¼å†…çš„å—çº§/åˆ—è¡¨å…ƒç´ éœ€è¦ä¸€ä¸ªâ€œè½¯æ¢è¡Œâ€ï¼Œé¿å…å†…å®¹ç²˜è¿
                if self.cell_buf and (self.cell_buf[-1].strip().lower() != "<br>"):
                    self.cell_buf.append("<br>")
            elif tag == "a" and self.in_cell:
                self.table_in_a = True
                self.table_a_href = attrs.get("href")
                self.table_a_text = []
            elif tag == "img" and self.in_cell:
                # è¡¨æ ¼å•å…ƒæ ¼å†…çš„å›¾ç‰‡
                src = (
                    attrs.get("src")
                    or attrs.get("data-src")
                    or attrs.get("data-original")
                    or attrs.get("data-lazy-src")
                )
                if (not src) and attrs.get("srcset"):
                    src = attrs["srcset"].split(",")[0].strip().split(" ")[0]
                if src:
                    img_url = urljoin(self.base_url, htmllib.unescape(src))
                    if not is_probable_icon(img_url):
                        alt = (attrs.get("alt") or "").strip()
                        # æ¸…ç† alt ä¸­çš„æ–¹æ‹¬å·ï¼Œé¿å…ç”Ÿæˆ ![[xxx]] è¿™ç§éæ ‡å‡†è¯­æ³•
                        alt = alt.replace("[", "").replace("]", "")
                        local = self.url_to_local.get(img_url, img_url)
                        self.cell_buf.append(f"![{alt}]({local})")
            return

        # block-ish tags
        if tag in ("p",):
            # åˆ—è¡¨é¡¹å†…çš„ <p> å¾ˆå¸¸è§ï¼›å¼ºè¡Œ blank line ä¼šæŠŠ "1. " å’Œå†…å®¹æ‹†å¼€ï¼Œé€ æˆç©ºæ¡ç›®
            if not self.list_stack:
                self._ensure_blank_line()
        elif tag == "br":
            self.out.append("\n")
        elif tag == "hr":
            self._ensure_blank_line()
            self.out.append("---\n\n")
        elif tag in ("h1", "h2", "h3", "h4", "h5", "h6"):
            self._ensure_blank_line()
            level = int(tag[1])
            self.in_heading = True
            self.heading_out_start = len(self.out)
            self.heading_text = []
            self.out.append("#" * level + " ")
        elif tag == "script":
            t = (attrs.get("type") or "").strip().lower()
            if t.startswith("math/tex"):
                self.in_math_script = True
                self.math_script_display = "mode=display" in t
                self.math_script_buf = []
                return
            self._enter_skip(tag)
        elif tag == "pre":
            self._ensure_blank_line()
            self.in_pre = True
            self.pre_buf = []
            self.pre_lang = self._sanitize_fence_language(self._extract_code_language(attrs))
        elif tag == "code":
            if self.in_pre:
                if not self.pre_lang:
                    self.pre_lang = self._sanitize_fence_language(self._extract_code_language(attrs))
                return
            self.in_inline_code = True
            self.inline_code_buf = []
        elif tag == "annotation":
            enc = (attrs.get("encoding") or "").strip().lower()
            if enc in ("application/x-tex", "text/tex"):
                self.in_annotation_tex = True
                self.annotation_display = self.katex_display_depth > 0
                self.annotation_buf = []
                return
        elif tag == "strong" or tag == "b":
            self.out.append("**")
        elif tag == "em" or tag == "i":
            self.out.append("*")
        elif tag == "a":
            self.in_a = True
            self.a_href = attrs.get("href")
            self.a_text = []
        elif tag == "img":
            src = (
                attrs.get("src")
                or attrs.get("data-src")
                or attrs.get("data-original")
                or attrs.get("data-lazy-src")
            )
            if (not src) and attrs.get("srcset"):
                src = attrs["srcset"].split(",")[0].strip().split(" ")[0]
            if not src:
                return
            img_url = urljoin(self.base_url, htmllib.unescape(src))
            if is_probable_icon(img_url):
                return
            alt = (attrs.get("alt") or "").strip()
            # æ¸…ç† alt ä¸­çš„æ–¹æ‹¬å·ï¼Œé¿å…ç”Ÿæˆ ![[xxx]] è¿™ç§éæ ‡å‡†è¯­æ³•
            alt = alt.replace("[", "").replace("]", "")
            local = self.url_to_local.get(img_url, img_url)
            self._ensure_blank_line()
            self.out.append(f"![{alt}]({local})\n")
        elif tag in ("ul", "ol"):
            # åµŒå¥—åˆ—è¡¨ä¸è¦å¼ºè¡Œæ’å…¥ç©ºè¡Œï¼Œå¦åˆ™å¯èƒ½ç ´åæ¸²æŸ“ï¼›åªç¡®ä¿æ¢è¡Œå³å¯ã€‚
            if self.list_stack:
                if not self._tail().endswith("\n"):
                    self.out.append("\n")
            else:
                self._ensure_blank_line()
            self.list_stack.append({"type": tag, "n": 0})
        elif tag == "li":
            if self.list_stack:
                if self.out and (not self._tail().endswith("\n")):
                    self.out.append("\n")
                self.list_stack[-1]["n"] = int(self.list_stack[-1]["n"]) + 1
                indent = "  " * (len(self.list_stack) - 1)
                if self.list_stack[-1]["type"] == "ol":
                    prefix = f"{self.list_stack[-1]['n']}. "
                else:
                    prefix = "- "
                self.out.append(indent + prefix)
        elif tag == "blockquote":
            self._ensure_blank_line()
            self.out.append("> ")

    def handle_startendtag(self, tag: str, attrs_list: Sequence[Tuple[str, Optional[str]]]) -> None:
        # å¤„ç† <tag/>ï¼Œé¿å… skip_stack å›  void/self-closing å½¢æ€ä¸ä¸€è‡´è€Œæ³„æ¼
        tag = tag.lower()
        self.handle_starttag(tag, attrs_list)
        self.handle_endtag(tag)

    def handle_endtag(self, tag: str) -> None:
        tag = tag.lower()

        # VOID_TAGS ä¸åº”æœ‰ç»“æŸæ ‡ç­¾ï¼›å¦‚æœé‡åˆ°ï¼Œç›´æ¥å¿½ç•¥ï¼ˆä¸å‡ºæ ˆï¼‰ã€‚
        if tag in VOID_TAGS:
            pass
        elif self.tag_stack:
            # å°è¯•åŒ¹é…æ ˆé¡¶ tagï¼›å¦‚æœä¸åŒ¹é…ï¼Œå¯èƒ½æ˜¯ HTML ä¸è§„èŒƒæˆ–æœ‰æœªé—­åˆæ ‡ç­¾ã€‚
            # ç­–ç•¥ï¼šå‘ä¸‹æœç´¢æ ˆï¼Œæ‰¾åˆ°åŒ¹é…çš„ tag å¹¶å¼¹å‡ºå®ƒåŠå…¶ä¸Šæ–¹çš„æ‰€æœ‰å…ƒç´ ï¼ˆå®¹é”™ï¼‰ã€‚
            matched_idx = -1
            for idx in range(len(self.tag_stack) - 1, -1, -1):
                if self.tag_stack[idx][0] == tag:
                    matched_idx = idx
                    break
            if matched_idx >= 0:
                # å¼¹å‡ºä»åŒ¹é…ä½ç½®åˆ°æ ˆé¡¶çš„æ‰€æœ‰å…ƒç´ 
                for _ in range(len(self.tag_stack) - matched_idx):
                    _, is_katex, is_katex_display = self.tag_stack.pop()
                    if is_katex:
                        self.katex_depth = max(0, self.katex_depth - 1)
                    if is_katex_display:
                        self.katex_display_depth = max(0, self.katex_display_depth - 1)

        if self.skip_stack:
            if tag == self.skip_stack[-1]:
                self.skip_stack.pop()
            return

        # å¤æ‚è¡¨æ ¼çš„åŸå§‹ HTML æ¨¡å¼
        if self.raw_table_mode:
            if tag == "table":
                self.raw_table_depth -= 1
                if self.raw_table_depth <= 0:
                    # è¡¨æ ¼ç»“æŸï¼Œè¾“å‡ºåŸå§‹ HTML
                    self.raw_table_buf.append("</table>")
                    self.out.append("\n".join(self.raw_table_buf))
                    self.out.append("\n\n")
                    self.raw_table_mode = False
                    self.raw_table_buf = []
                    self.in_table = False
                else:
                    self.raw_table_buf.append("</table>")
            elif tag not in VOID_TAGS:
                self.raw_table_buf.append(f"</{tag}>")
            return

        if self.in_table:
            if self.table_capture_html:
                if tag not in VOID_TAGS:
                    self.table_capture_buf.append(f"</{tag}>")
                if tag == "table":
                    self.table_capture_depth -= 1

            if tag == "table":
                self.table_depth = max(0, self.table_depth - 1)
            elif self.table_depth > 1:
                # åµŒå¥— table å†…çš„ç»“æŸæ ‡ç­¾ï¼šä¸å‚ä¸ Markdown è¡¨æ ¼çŠ¶æ€æœº
                return

            if tag == "a" and self.table_in_a:
                text = "".join(self.table_a_text).strip() or (self.table_a_href or "")
                href = self.table_a_href
                if href:
                    href = urljoin(self.base_url, href)
                    self._table_append(f"[{text}]({href})")
                else:
                    self._table_append(text)
                self.table_in_a = False
                self.table_a_href = None
                self.table_a_text = []
            elif tag in ("p", "div", "li") and self.in_cell:
                if self.cell_buf and (self.cell_buf[-1].strip().lower() != "<br>"):
                    self.cell_buf.append("<br>")
            elif tag in ("th", "td") and self.in_cell:
                cell = "".join(self.cell_buf)
                cell = cell.replace("\r\n", "\n").replace("\r", "\n")
                cell = re.sub(r"[ \t\f\v]+", " ", cell)
                cell = re.sub(r"\s*\n\s*", "<br>", cell)
                cell = re.sub(r"\s*<br>\s*", "<br>", cell, flags=re.IGNORECASE)
                cell = re.sub(r"(<br>){2,}", "<br>", cell, flags=re.IGNORECASE)
                cell = re.sub(r"^(<br>)+", "", cell, flags=re.IGNORECASE)
                cell = re.sub(r"(<br>)+$", "", cell, flags=re.IGNORECASE)
                cell = cell.strip()
                if self.current_row is not None:
                    self.current_row.append(cell)
                self.in_cell = False
                self.cell_buf = []
            elif tag == "tr":
                if self.current_row is not None and any(c.strip() for c in self.current_row):
                    self.table_rows.append(self.current_row)
                self.current_row = None
            elif tag == "table":
                # åµŒå¥—è¡¨æ ¼ï¼šä»…å‡å°‘æ·±åº¦ï¼Œä¸åœ¨è¿™é‡Œç»“æŸæ•´ä¸ªè¡¨æ ¼è§£æï¼ˆå¤æ‚è¡¨æ ¼å»ºè®® --keep-htmlï¼‰ã€‚
                if self.table_depth > 0:
                    return
                rows = self.table_rows
                self.in_table = False
                self.table_rows = []
                self.current_row = None

                if self.table_capture_html and self.table_capture_depth <= 0:
                    # åªæœ‰åœ¨é¡¶å±‚ table å®Œæ•´é—­åˆæ—¶æ‰å†³å®šè¾“å‡ºç­–ç•¥
                    if self.keep_html and self.table_is_complex:
                        self.out.append("".join(self.table_capture_buf))
                        self.out.append("\n\n")
                        self.table_capture_html = False
                        self.table_capture_buf = []
                        self.table_capture_depth = 0
                        self.table_is_complex = False
                        return
                    self.table_capture_html = False
                    self.table_capture_buf = []
                    self.table_capture_depth = 0
                    self.table_is_complex = False

                if rows:
                    cols = max(len(r) for r in rows)
                    norm = [r + [""] * (cols - len(r)) for r in rows]
                    header = norm[0]
                    body = norm[1:]
                    self.out.append("| " + " | ".join(h.replace("|", r"\|") for h in header) + " |\n")
                    self.out.append("| " + " | ".join(["---"] * cols) + " |\n")
                    for r in body:
                        self.out.append("| " + " | ".join(c.replace("|", r"\|") for c in r) + " |\n")
                    self.out.append("\n")
            return

        if tag in ("h1", "h2", "h3", "h4", "h5", "h6"):
            # è¿‡æ»¤ç©ºæ ‡é¢˜ï¼ˆä¸€äº›ç«™ç‚¹ä¼šç”Ÿæˆæ— æ–‡æœ¬ headingï¼Œæˆ–åªåŒ…å«â€œ#â€é”šç‚¹ï¼‰
            if self.in_heading and (self.heading_out_start is not None):
                heading_text = "".join(self.heading_text).strip()
                if not heading_text:
                    del self.out[self.heading_out_start :]
                else:
                    self.out.append("\n\n")
            else:
                self.out.append("\n\n")
            self.in_heading = False
            self.heading_out_start = None
            self.heading_text = []
        elif tag == "p":
            self.out.append("\n\n")
        elif tag == "annotation" and self.in_annotation_tex:
            tex = "".join(self.annotation_buf).strip()
            self.in_annotation_tex = False
            self.annotation_buf = []
            if tex:
                if self.annotation_display:
                    self._ensure_blank_line()
                    self.out.append(f"$$\n{tex}\n$$\n\n")
                else:
                    self._append_text(f"${tex.replace(chr(10), ' ')}$")
            self.annotation_display = False
        elif tag == "script" and self.in_math_script:
            tex = "".join(self.math_script_buf).strip()
            display = self.math_script_display
            self.in_math_script = False
            self.math_script_display = False
            self.math_script_buf = []
            if tex:
                if display:
                    self._ensure_blank_line()
                    self.out.append(f"$$\n{tex}\n$$\n\n")
                else:
                    self._append_text(f"${tex.replace(chr(10), ' ')}$")
        elif tag == "pre":
            code = "".join(self.pre_buf)
            code = code.replace("\r\n", "\n").replace("\r", "\n").strip("\n")
            fence_lang = self._sanitize_fence_language(self.pre_lang)
            self.out.append(f"```{fence_lang}\n" + code + "\n```\n\n")
            self.in_pre = False
            self.pre_buf = []
            self.pre_lang = ""
        elif tag == "code":
            if self.in_pre:
                return
            code = "".join(self.inline_code_buf).strip()
            self.out.append("`" + code.replace("`", r"\`") + "`")
            self.in_inline_code = False
            self.inline_code_buf = []
        elif tag == "strong" or tag == "b":
            self.out.append("**")
        elif tag == "em" or tag == "i":
            self.out.append("*")
        elif tag == "a":
            text = "".join(self.a_text).strip() or (self.a_href or "")
            href = self.a_href

            # heading çš„å°é”šç‚¹ï¼ˆ# / Â¶ ç­‰ï¼‰å±äºå™ªéŸ³
            if href:
                full = urljoin(self.base_url, href)
                if text.strip() in ("#", "Â¶", "Â§") and (href.startswith("#") or full.startswith(self.base_url + "#")):
                    self.in_a = False
                    self.a_href = None
                    self.a_text = []
                    return

            # Ghost çš„ heading å°é”šç‚¹é€šå¸¸æ¸²æŸ“æˆâ€œtagâ€ï¼Œå±äºå™ªéŸ³
            if text.lower() == "tag" and href and (href.startswith("#") or href.startswith(self.base_url + "#")):
                self.in_a = False
                self.a_href = None
                self.a_text = []
                return

            if href:
                href = urljoin(self.base_url, href)
                self.out.append(f"[{text}]({href})")
            else:
                self.out.append(text)
            self.in_a = False
            self.a_href = None
            self.a_text = []
        elif tag in ("ul", "ol"):
            if self.list_stack:
                self.list_stack.pop()
            self.out.append("\n")
        elif tag == "li":
            self.out.append("\n")
        elif tag == "blockquote":
            self.out.append("\n\n")

    def handle_data(self, data: str) -> None:
        if self.skip_stack:
            return
        # å¤æ‚è¡¨æ ¼çš„åŸå§‹ HTML æ¨¡å¼
        if self.raw_table_mode:
            if data:
                self.raw_table_buf.append(htmllib.escape(data))
            return
        if self.in_annotation_tex:
            self.annotation_buf.append(data or "")
            return
        if self.in_math_script:
            self.math_script_buf.append(data or "")
            return
        # KaTeX æ¸²æŸ“å‡ºæ¥çš„ HTML æ–‡æœ¬ä¼šå¯¼è‡´å…¬å¼é‡å¤è¾“å‡ºï¼›åªä¿ç•™ annotation çš„ TeX æºã€‚
        if self.katex_depth > 0:
            return
        # <pre> å†…å¿…é¡»ä¿ç•™å…¨éƒ¨å†…å®¹ï¼ˆåŒ…æ‹¬ç©ºç™½è¡Œ/ç¼©è¿›ï¼‰ï¼Œå¦åˆ™ä¼šå‡ºç° token ç²˜è¿ï¼ˆä¾‹å¦‚ loopwhileï¼‰ã€‚
        if self.in_table and self.table_capture_html and data:
            self.table_capture_buf.append(htmllib.escape(data, quote=False))
        if self.in_pre:
            self.pre_buf.append(data or "")
            return
        if self.in_table and self.table_depth > 1:
            # åµŒå¥— table å†…éƒ¨ï¼šä¸æŠŠæ–‡æœ¬æ‹¼è¿›å¤–å±‚ Markdown table cell
            return
        if self.in_table:
            if self.table_in_a:
                self.table_a_text.append(data)
                return
            if self.in_cell:
                self._table_append(data)
            return
        if self.in_inline_code:
            self.inline_code_buf.append(data)
            return
        if self.in_a:
            if self.in_heading and data and (not data.isspace()) and data.strip() not in ("#", "Â¶", "Â§"):
                self.heading_text.append(data)
            self.a_text.append(data)
            return
        if not data or data.isspace():
            return
        if self.in_heading:
            self.heading_text.append(data)
        self._append_text(data)


def _convert_latex_delimiters_outside_code(md: str) -> str:
    # æŠŠ \(...\)/\[...\] ç»Ÿä¸€è½¬ä¸º $/$$ï¼Œå¹¶è·³è¿‡ fenced code blockã€‚
    out_lines: List[str] = []
    in_fence = False
    in_inline_code = False
    inline_tick_len = 0
    for line in md.splitlines(True):
        stripped = line.lstrip()
        if stripped.startswith("```"):
            in_fence = not in_fence
            out_lines.append(line)
            continue
        if in_fence:
            out_lines.append(line)
            continue

        i = 0
        converted: List[str] = []
        while i < len(line):
            if line[i] == "`":
                j = i
                while j < len(line) and line[j] == "`":
                    j += 1
                ticks = j - i
                converted.append(line[i:j])
                if not in_inline_code:
                    in_inline_code = True
                    inline_tick_len = ticks
                elif ticks == inline_tick_len:
                    in_inline_code = False
                    inline_tick_len = 0
                i = j
                continue

            j = line.find("`", i)
            if j == -1:
                j = len(line)
            seg = line[i:j]
            if not in_inline_code:
                seg = seg.replace(r"\[", "$$").replace(r"\]", "$$")
                seg = seg.replace(r"\(", "$").replace(r"\)", "$")
            converted.append(seg)
            i = j

        out_lines.append("".join(converted))
    return "".join(out_lines)


def html_to_markdown(article_html: str, base_url: str, url_to_local: Dict[str, str], keep_html: bool = False) -> str:
    parser = HTMLToMarkdown(base_url=base_url, url_to_local=url_to_local, keep_html=keep_html)
    parser.feed(article_html)
    md = "".join(parser.out)
    md = md.replace("\r\n", "\n")
    md = re.sub(r"\n{3,}", "\n\n", md)
    md = re.sub(r"\n\s*/\s*\n", "\n\n", md)  # å°‘æ•°ç«™ç‚¹çš„æ®‹ç•™ UI ç¬¦å·
    md = _convert_latex_delimiters_outside_code(md)
    # å»é™¤ç©ºæ ‡é¢˜è¡Œï¼ˆä¾‹å¦‚å•ç‹¬çš„ "###" / "# "ï¼‰
    md = re.sub(r"(?m)^\s*#{1,6}\s*$\n?", "", md)
    # å»é™¤æ ‡é¢˜ä¸­çš„å°é”šç‚¹å™ªéŸ³ï¼ˆä¾‹å¦‚ "Heading[#](...)"ï¼‰
    md = re.sub(r"(?m)^(#{1,6}\s+.*?)(\s*\[\s*[#Â¶Â§]\s*\]\([^)]+\))+\s*$", r"\1", md)
    return md.strip() + "\n"


def _path_to_file_uri(path: str) -> str:
    p = os.path.abspath(path).replace("\\", "/")
    # Windows: C:/x -> file:///C:/x
    if re.match(r"^[A-Za-z]:/", p):
        return "file:///" + p
    # POSIX: /x -> file:///x
    if p.startswith("/"):
        return "file://" + p
    return "file:///" + p


def _find_pdf_browser() -> Optional[str]:
    # ä¼˜å…ˆä½¿ç”¨å·²å®‰è£…çš„ Chromium ç³»æµè§ˆå™¨ï¼ˆEdge/Chromeï¼‰ã€‚è¿™æ˜¯â€œå°½é‡ä½¿ç”¨æ ‡å‡†åº“â€çš„ç°å®å–èˆï¼š
    # Python æ ‡å‡†åº“æœ¬èº«ä¸æä¾›é«˜ä¿çœŸ Markdownâ†’PDF æ¸²æŸ“èƒ½åŠ›ã€‚
    candidates = [
        shutil.which("msedge"),
        shutil.which("chrome"),
        shutil.which("chrome.exe"),
        shutil.which("msedge.exe"),
    ]

    # Windows å¸¸è§å®‰è£…è·¯å¾„å…œåº•
    candidates += [
        r"C:\Program Files (x86)\Microsoft\Edge\Application\msedge.exe",
        r"C:\Program Files\Microsoft\Edge\Application\msedge.exe",
        r"C:\Program Files\Google\Chrome\Application\chrome.exe",
        r"C:\Program Files (x86)\Google\Chrome\Application\chrome.exe",
    ]

    for c in candidates:
        if not c:
            continue
        if os.path.isfile(c):
            return c
    return None


def _markdown_css() -> str:
    # è½»é‡ CSSï¼šå°½é‡æ¥è¿‘å¸¸è§ Markdown é¢„è§ˆé£æ ¼ï¼ˆæ ‡é¢˜/ä»£ç /è¡¨æ ¼/å¼•ç”¨/å›¾ç‰‡ï¼‰ã€‚
    return """
    :root { color-scheme: light; }
    body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Arial, sans-serif; line-height: 1.6; }
    .markdown-body { max-width: 920px; margin: 0 auto; padding: 32px 20px; color: #1f2328; }
    h1,h2,h3,h4,h5,h6 { margin: 24px 0 12px; line-height: 1.25; }
    h1 { font-size: 2em; border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }
    h2 { font-size: 1.5em; border-bottom: 1px solid #d0d7de; padding-bottom: 0.3em; }
    p { margin: 0 0 12px; }
    a { color: #0969da; text-decoration: none; }
    a:hover { text-decoration: underline; }
    code { font-family: ui-monospace, SFMono-Regular, Menlo, Consolas, "Liberation Mono", monospace; background: #f6f8fa; padding: 0.15em 0.3em; border-radius: 4px; }
    pre { background: #0b1021; color: #e6edf3; padding: 12px 14px; border-radius: 6px; overflow-x: auto; }
    pre code { background: transparent; padding: 0; color: inherit; }
    blockquote { margin: 0 0 12px; padding: 0 1em; color: #57606a; border-left: 0.25em solid #d0d7de; }
    ul,ol { margin: 0 0 12px 1.2em; }
    li { margin: 0.25em 0; }
    table { border-collapse: collapse; margin: 0 0 12px; width: 100%; }
    th, td { border: 1px solid #d0d7de; padding: 6px 10px; vertical-align: top; }
    th { background: #f6f8fa; }
    img { max-width: 100%; height: auto; }
    hr { border: 0; border-top: 1px solid #d0d7de; margin: 20px 0; }
    """


def _escape_html(text: str) -> str:
    return htmllib.escape(text, quote=False)


def _md_fallback_to_html(md: str) -> str:
    # ä»…è¦†ç›–æœ¬è„šæœ¬äº§å‡ºçš„å¸¸è§å­é›†ï¼šæ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ã€å¼•ç”¨ã€ä»£ç å—ã€å›¾ç‰‡ã€é“¾æ¥ã€è¡¨æ ¼ã€‚
    lines = md.replace("\r\n", "\n").replace("\r", "\n").split("\n")
    html_parts: List[str] = []

    in_pre = False
    pre_lang = ""
    pre_buf: List[str] = []

    in_ul = False
    in_ol = False
    in_blockquote = False

    def close_lists() -> None:
        nonlocal in_ul, in_ol
        if in_ul:
            html_parts.append("</ul>")
            in_ul = False
        if in_ol:
            html_parts.append("</ol>")
            in_ol = False

    def close_blockquote() -> None:
        nonlocal in_blockquote
        if in_blockquote:
            html_parts.append("</blockquote>")
            in_blockquote = False

    def flush_pre() -> None:
        nonlocal in_pre, pre_lang, pre_buf
        if not in_pre:
            return
        code = "\n".join(pre_buf)
        cls = f' class="language-{_escape_html(pre_lang)}"' if pre_lang else ""
        html_parts.append(f"<pre><code{cls}>{_escape_html(code)}</code></pre>")
        in_pre = False
        pre_lang = ""
        pre_buf = []

    def render_inlines(text: str) -> str:
        br_token = "\0__BR__\0"
        raw = re.sub(r"<br\s*/?>", br_token, text, flags=re.IGNORECASE)

        # å›¾ç‰‡ï¼š![alt](src)
        def img_repl(m: re.Match[str]) -> str:
            alt = _escape_html(m.group(1))
            src = m.group(2).strip()
            return f'<img alt="{alt}" src="{_escape_html(src)}">'

        # é“¾æ¥ï¼š[text](href)
        def link_repl(m: re.Match[str]) -> str:
            label = _escape_html(m.group(1))
            href = m.group(2).strip()
            return f'<a href="{_escape_html(href)}">{label}</a>'

        # è¡Œå†… codeï¼š`...`
        def code_repl(m: re.Match[str]) -> str:
            return f"<code>{_escape_html(m.group(1))}</code>"

        # ç²—ä½“ **...** / æ–œä½“ *...*ï¼ˆç®€åŒ–ï¼Œé¿å…è·¨è¡Œ/åµŒå¥—å¤æ‚æƒ…å†µï¼‰
        out = _escape_html(raw).replace(br_token, "<br/>")
        out = re.sub(r"!\[([^\]]*)\]\(([^)]+)\)", img_repl, out)
        out = re.sub(r"\[([^\]]+)\]\(([^)]+)\)", link_repl, out)
        out = re.sub(r"`([^`]+)`", code_repl, out)
        out = re.sub(r"\*\*([^*]+)\*\*", r"<strong>\1</strong>", out)
        out = re.sub(r"(?<!\*)\*([^*]+)\*(?!\*)", r"<em>\1</em>", out)
        return out

    i = 0
    while i < len(lines):
        line = lines[i]

        # é€ä¼ å¤æ‚è¡¨æ ¼ä¿ç•™ä¸‹æ¥çš„ raw HTML table
        if line.lstrip().startswith("<table"):
            block: List[str] = [line]
            if "</table>" not in line.lower():
                i += 1
                while i < len(lines):
                    block.append(lines[i])
                    if "</table>" in lines[i].lower():
                        break
                    i += 1
            html_parts.append("\n".join(block))
            i += 1
            continue

        fence = re.match(r"^```(\S+)?\s*$", line.strip())
        if fence:
            if in_pre:
                flush_pre()
            else:
                close_lists()
                close_blockquote()
                in_pre = True
                pre_lang = (fence.group(1) or "").strip()
                pre_buf = []
            i += 1
            continue

        if in_pre:
            pre_buf.append(line)
            i += 1
            continue

        if line.strip() == "":
            close_lists()
            close_blockquote()
            i += 1
            continue

        # è¡¨æ ¼ï¼ˆpipe tableï¼‰
        if "|" in line and line.strip().startswith("|"):
            # æ”¶é›†è¿ç»­çš„ |...| è¡Œ
            table_lines: List[str] = []
            while i < len(lines) and lines[i].strip().startswith("|") and ("|" in lines[i]):
                if lines[i].strip() == "":
                    break
                table_lines.append(lines[i].strip())
                i += 1
            if len(table_lines) >= 2 and re.match(r"^\|\s*---", table_lines[1]):
                rows: List[List[str]] = []
                for tl in table_lines:
                    parts = [p.strip() for p in tl.strip("|").split("|")]
                    rows.append(parts)
                header = rows[0]
                body = rows[2:] if len(rows) >= 3 else []
                html_parts.append("<table>")
                html_parts.append("<thead><tr>" + "".join(f"<th>{render_inlines(c)}</th>" for c in header) + "</tr></thead>")
                if body:
                    html_parts.append("<tbody>")
                    for r in body:
                        html_parts.append("<tr>" + "".join(f"<td>{render_inlines(c)}</td>" for c in r) + "</tr>")
                    html_parts.append("</tbody>")
                html_parts.append("</table>")
                continue
            # ä¸æ˜¯æ ‡å‡†è¡¨æ ¼å°±æŒ‰æ™®é€šè¡Œå¤„ç†

        # æ ‡é¢˜
        m = re.match(r"^(#{1,6})\s+(.*)$", line)
        if m:
            close_lists()
            close_blockquote()
            level = len(m.group(1))
            html_parts.append(f"<h{level}>{render_inlines(m.group(2).strip())}</h{level}>")
            i += 1
            continue

        # å¼•ç”¨
        if line.lstrip().startswith("> "):
            close_lists()
            if not in_blockquote:
                html_parts.append("<blockquote>")
                in_blockquote = True
            html_parts.append(f"<p>{render_inlines(line.lstrip()[2:])}</p>")
            i += 1
            continue

        # åˆ—è¡¨ï¼ˆç®€åŒ–ï¼šåªæ”¯æŒæœ€å¸¸è§çš„ - / 1. ä¸”ä¸åšæ·±å±‚åµŒå¥— HTML ç»“æ„ï¼‰
        m_ul = re.match(r"^\s*-\s+(.*)$", line)
        m_ol = re.match(r"^\s*(\d+)\.\s+(.*)$", line)
        if m_ul:
            close_blockquote()
            if in_ol:
                html_parts.append("</ol>")
                in_ol = False
            if not in_ul:
                html_parts.append("<ul>")
                in_ul = True
            html_parts.append(f"<li>{render_inlines(m_ul.group(1).strip())}</li>")
            i += 1
            continue
        if m_ol:
            close_blockquote()
            if in_ul:
                html_parts.append("</ul>")
                in_ul = False
            if not in_ol:
                html_parts.append("<ol>")
                in_ol = True
            html_parts.append(f"<li>{render_inlines(m_ol.group(2).strip())}</li>")
            i += 1
            continue

        # åˆ†å‰²çº¿
        if line.strip() == "---":
            close_lists()
            close_blockquote()
            html_parts.append("<hr>")
            i += 1
            continue

        close_lists()
        close_blockquote()
        html_parts.append(f"<p>{render_inlines(line.strip())}</p>")
        i += 1

    flush_pre()
    close_lists()
    close_blockquote()
    return "\n".join(html_parts)


# æ¨¡å—çº§åˆ«æ£€æµ‹ markdown åº“æ˜¯å¦å¯ç”¨ï¼ˆåªæ£€æµ‹ä¸€æ¬¡ï¼‰
_HAS_MARKDOWN_LIB = False
try:
    import markdown as _markdown_lib  # type: ignore
    _HAS_MARKDOWN_LIB = True
except ImportError:
    _markdown_lib = None  # type: ignore


def markdown_to_html(md_text: str, verbose: bool = False) -> str:
    """
    å°† Markdown æ–‡æœ¬è½¬æ¢ä¸º HTMLã€‚
    
    ä¼˜å…ˆä½¿ç”¨ python-markdown åº“ï¼ˆå¦‚å·²å®‰è£…ï¼‰ï¼Œå¦åˆ™å›é€€åˆ°å†…ç½®ç®€æ˜“è½¬æ¢ã€‚
    """
    if _HAS_MARKDOWN_LIB and _markdown_lib is not None:
        if verbose:
            print("ä½¿ç”¨ python-markdown åº“è¿›è¡Œ Markdownâ†’HTML è½¬æ¢")
        try:
            html = _markdown_lib.markdown(
                md_text,
                extensions=[
                    "fenced_code",
                    "tables",
                    "sane_lists",
                ],
                output_format="html5",
            )
            return html
        except Exception:
            # markdown åº“è°ƒç”¨å‡ºé”™æ—¶å›é€€åˆ°å†…ç½®å®ç°
            pass
    if verbose:
        print("ä½¿ç”¨å†…ç½® Markdownâ†’HTML è½¬æ¢ï¼ˆå¦‚éœ€æ›´å¥½çš„æ¸²æŸ“æ•ˆæœï¼Œå¯å®‰è£… python-markdownï¼špip install markdownï¼‰")
    return _md_fallback_to_html(md_text)


def strip_yaml_frontmatter(md_text: str) -> str:
    text = md_text.replace("\r\n", "\n").replace("\r", "\n")
    if not text.startswith("---\n"):
        return md_text
    lines = text.split("\n")
    for i in range(1, min(len(lines), 2000)):
        if lines[i].strip() == "---":
            return "\n".join(lines[i + 1 :]).lstrip("\n")
    return md_text


def generate_pdf_from_markdown(md_path: str, pdf_path: str, *, allow_file_access: bool = False) -> None:
    browser = _find_pdf_browser()
    if not browser:
        raise RuntimeError("æœªæ‰¾åˆ°å¯ç”¨äºæ‰“å° PDF çš„æµè§ˆå™¨ï¼ˆmsedge/chromeï¼‰ã€‚è¯·å®‰è£… Edge/Chrome æˆ–åŠ å…¥ PATHã€‚")

    with open(md_path, "r", encoding="utf-8") as f:
        md_text = f.read()

    body_html = markdown_to_html(md_text, verbose=True)
    html_doc = f"""<!doctype html>
<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <style>{_markdown_css()}</style>
  </head>
  <body>
    <main class="markdown-body">
      {body_html}
    </main>
  </body>
</html>
"""

    out_dir = os.path.dirname(os.path.abspath(pdf_path)) or "."
    os.makedirs(out_dir, exist_ok=True)

    # HTML å¿…é¡»ä¸ md åŒç›®å½•ï¼Œæ‰èƒ½è®©ç›¸å¯¹å›¾ç‰‡è·¯å¾„ï¼ˆassets/xx.pngï¼‰æ­£ç¡®è§£æã€‚
    md_dir = os.path.dirname(os.path.abspath(md_path)) or "."
    html_tmp = None
    try:
        with tempfile.NamedTemporaryFile("w", encoding="utf-8", suffix=".html", dir=md_dir, delete=False) as tf:
            tf.write(html_doc)
            html_tmp = tf.name

        url = _path_to_file_uri(html_tmp)

        pdf_abs = os.path.abspath(pdf_path)
        common = [
            "--disable-gpu",
            "--no-first-run",
            "--no-default-browser-check",
        ]
        if allow_file_access:
            # å®‰å…¨æç¤ºï¼šè¯¥å‚æ•°ä¼šæ”¾å®½ file:// èµ„æºè®¿é—®é™åˆ¶ï¼›ä»…åœ¨ç¡®æœ‰éœ€è¦æ—¶å¼€å¯ã€‚
            common.append("--allow-file-access-from-files")
        common += [
            f"--print-to-pdf={pdf_abs}",
            url,
        ]

        variants = [
            ["--headless=new", "--print-to-pdf-no-header", *common],
            ["--headless=new", *common],
            ["--headless", "--print-to-pdf-no-header", *common],
            ["--headless", *common],
        ]

        last_err: Optional[Exception] = None
        last_stderr = ""
        for argv in variants:
            try:
                p = subprocess.run(
                    [browser, *argv],
                    check=True,
                    capture_output=True,
                    text=True,
                    timeout=120,
                )
                if p.stderr:
                    # å°‘æ•°ç‰ˆæœ¬ä¼šåœ¨ stderr è¾“å‡ºè­¦å‘Šï¼Œä½†ä»æˆåŠŸç”Ÿæˆ PDFï¼›ä¸å½“ä½œå¤±è´¥ã€‚
                    last_stderr = p.stderr
                last_err = None
                break
            except Exception as e:  # noqa: BLE001
                last_err = e
                try:
                    if isinstance(e, subprocess.CalledProcessError) and e.stderr:
                        last_stderr = str(e.stderr)
                except Exception:
                    pass

        if last_err is not None:
            raise RuntimeError(f"æµè§ˆå™¨æ‰“å° PDF å¤±è´¥ï¼š{last_err}\n{last_stderr}".strip())
    finally:
        if html_tmp and os.path.isfile(html_tmp):
            try:
                os.remove(html_tmp)
            except OSError:
                pass


def _normalize_title(text: str) -> str:
    text = (text or "").strip().lower()
    text = re.sub(r"\s+", " ", text)
    text = re.sub(r"[^\w\s]", "", text)
    return text.strip()


def strip_duplicate_h1(md_body: str, title: str, max_scan_lines: int = 80) -> str:
    """
    é¡¶éƒ¨ä¼šå†™å…¥ "# {title}"ï¼›è€Œæ­£æ–‡æŠ½å–å¸¸å¸¸åŒ…å«åŒå <h1>ã€‚
    è¿™é‡Œåœ¨æ­£æ–‡ä¸­æ‰«æå‰ N è¡Œï¼Œåˆ é™¤ç¬¬ä¸€ä¸ªåŒ¹é… title çš„ "# ..." è¡Œï¼Œé¿å…é‡å¤ã€‚
    """
    title_n = _normalize_title(title)
    if not title_n:
        return md_body

    lines = md_body.splitlines()
    scan = min(len(lines), max_scan_lines)
    for i in range(scan):
        line = lines[i].strip()
        if not line:
            continue
        if re.fullmatch(r"#{1,6}", line):
            # ç©ºæ ‡é¢˜
            del lines[i : i + 1]
            break
        if line.startswith("# "):
            if _normalize_title(line[2:]) == title_n:
                j = i + 1
                while j < len(lines) and not lines[j].strip():
                    j += 1
                del lines[i:j]
                break
    return "\n".join(lines).lstrip("\n").rstrip() + "\n"


def extract_title(page_html: str) -> Optional[str]:
    # <title>...</title>
    m = re.search(r"<title\b[^>]*>(.*?)</title>", page_html, re.IGNORECASE | re.DOTALL)
    if not m:
        return None
    title = re.sub(r"\s+", " ", htmllib.unescape(m.group(1))).strip()
    return title or None


# ============================================================================
# JS åçˆ¬æ£€æµ‹
# ============================================================================

@dataclass
class JSChallengeResult:
    """JS åçˆ¬æ£€æµ‹ç»“æœ"""
    is_challenge: bool  # æ˜¯å¦ä¸º JS æŒ‘æˆ˜é¡µé¢
    confidence: str  # "high", "medium", "low"
    signals: List[str]  # æ£€æµ‹åˆ°çš„ä¿¡å·
    
    def get_suggestions(self, url: str) -> List[str]:
        """æ ¹æ®æ£€æµ‹ç»“æœç”Ÿæˆå»ºè®®"""
        return [
            "1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¯¥ URLï¼Œç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½",
            "2. å³é”®ç‚¹å‡»é¡µé¢ â†’ ã€Œå¦å­˜ä¸ºã€æˆ–ã€Œå­˜å‚¨ä¸ºã€â†’ ä¿å­˜ä¸º .html æ–‡ä»¶",
            "3. ä½¿ç”¨ --local-html å‚æ•°å¤„ç†æœ¬åœ°æ–‡ä»¶ï¼š",
            f"   python grab_web_to_md.py --local-html saved.html --base-url \"{url}\" --out output.md",
        ]


def detect_js_challenge(html: str, title: Optional[str] = None) -> JSChallengeResult:
    """
    æ£€æµ‹é¡µé¢æ˜¯å¦ä¸º JS åçˆ¬æŒ‘æˆ˜é¡µé¢ï¼ˆå¦‚ Cloudflareã€Akamai ç­‰ï¼‰ã€‚
    
    è¿”å› JSChallengeResultï¼ŒåŒ…å«æ˜¯å¦ä¸ºæŒ‘æˆ˜é¡µé¢ã€ç½®ä¿¡åº¦å’Œæ£€æµ‹åˆ°çš„ä¿¡å·ã€‚
    """
    signals: List[str] = []
    
    # æå–æ ‡é¢˜ï¼ˆå¦‚æœæœªæä¾›ï¼‰
    if title is None:
        title = extract_title(html) or ""
    title_lower = title.lower()
    
    # ------------------------------------------------------------------
    # é«˜ç½®ä¿¡åº¦ä¿¡å·
    # ------------------------------------------------------------------
    
    # Cloudflare ç‰¹å¾
    if "__cf_chl_opt" in html or "cf-browser-verification" in html:
        signals.append("å‘ç° Cloudflare éªŒè¯ç‰¹å¾ (__cf_chl_opt / cf-browser-verification)")
    
    if "challenges.cloudflare.com" in html:
        signals.append("å‘ç° Cloudflare æŒ‘æˆ˜åŸŸåå¼•ç”¨")
    
    # æ ‡é¢˜ç‰¹å¾
    challenge_titles = [
        ("challenge", "æ ‡é¢˜åŒ…å« 'Challenge'"),
        ("just a moment", "æ ‡é¢˜åŒ…å« 'Just a moment'"),
        ("checking your browser", "æ ‡é¢˜åŒ…å« 'Checking your browser'"),
        ("please wait", "æ ‡é¢˜åŒ…å« 'Please wait'"),
        ("attention required", "æ ‡é¢˜åŒ…å« 'Attention Required'"),
        ("ddos protection", "æ ‡é¢˜åŒ…å« 'DDoS Protection'"),
    ]
    for keyword, desc in challenge_titles:
        if keyword in title_lower:
            signals.append(desc)
            break
    
    # JavaScript å¿…éœ€æç¤º
    js_required_patterns = [
        (r"javascript\s+is\s+(disabled|required)", "é¡µé¢æç¤º JavaScript å¿…éœ€/è¢«ç¦ç”¨"),
        (r"please\s+(enable|turn\s+on)\s+javascript", "é¡µé¢æç¤ºè¯·å¯ç”¨ JavaScript"),
        (r"browser.*does\s+not\s+support.*javascript", "é¡µé¢æç¤ºæµè§ˆå™¨ä¸æ”¯æŒ JavaScript"),
    ]
    html_lower = html.lower()
    for pattern, desc in js_required_patterns:
        if re.search(pattern, html_lower):
            signals.append(desc)
            break
    
    # Akamai Bot Manager
    if "akamai" in html_lower and ("bot" in html_lower or "challenge" in html_lower):
        signals.append("å‘ç° Akamai Bot Manager ç‰¹å¾")
    
    # PerimeterX
    if "_pxhd" in html or "perimeterx" in html_lower:
        signals.append("å‘ç° PerimeterX åçˆ¬ç‰¹å¾")
    
    # ------------------------------------------------------------------
    # ä¸­ç½®ä¿¡åº¦ä¿¡å·ï¼šå†…å®¹æçŸ­ + åŒ…å«ç‰¹å®šå…³é”®è¯
    # ------------------------------------------------------------------
    
    # è®¡ç®—æ­£æ–‡é•¿åº¦ï¼ˆå»é™¤ script/style/æ³¨é‡Šï¼‰
    body_text = re.sub(r"<script[^>]*>.*?</script>", "", html, flags=re.IGNORECASE | re.DOTALL)
    body_text = re.sub(r"<style[^>]*>.*?</style>", "", body_text, flags=re.IGNORECASE | re.DOTALL)
    body_text = re.sub(r"<!--.*?-->", "", body_text, flags=re.DOTALL)
    body_text = re.sub(r"<[^>]+>", " ", body_text)
    body_text = re.sub(r"\s+", " ", body_text).strip()
    
    if len(body_text) < 200:
        # å†…å®¹å¾ˆçŸ­ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰åçˆ¬ç›¸å…³è¯æ±‡
        short_content_keywords = ["browser", "javascript", "enable", "loading", "redirect", "verify"]
        found_keywords = [kw for kw in short_content_keywords if kw in body_text.lower()]
        if found_keywords:
            signals.append(f"é¡µé¢æ­£æ–‡æçŸ­ï¼ˆ{len(body_text)} å­—ç¬¦ï¼‰ä¸”åŒ…å«å…³é”®è¯: {', '.join(found_keywords)}")
    
    # <noscript> ä¸­çš„è­¦å‘Š
    noscript_match = re.search(r"<noscript[^>]*>(.*?)</noscript>", html, re.IGNORECASE | re.DOTALL)
    if noscript_match:
        noscript_content = noscript_match.group(1).lower()
        if "javascript" in noscript_content or "enable" in noscript_content:
            signals.append("å‘ç° <noscript> ä¸­çš„ JavaScript è­¦å‘Š")
    
    # ------------------------------------------------------------------
    # åˆ¤å®šç»“æœ
    # ------------------------------------------------------------------
    
    if not signals:
        return JSChallengeResult(is_challenge=False, confidence="none", signals=[])
    
    # æ ¹æ®ä¿¡å·æ•°é‡å’Œç±»å‹åˆ¤æ–­ç½®ä¿¡åº¦
    high_confidence_keywords = ["cloudflare", "akamai", "perimeterx", "challenge", "just a moment"]
    has_high_signal = any(
        any(kw in sig.lower() for kw in high_confidence_keywords) 
        for sig in signals
    )
    
    if has_high_signal or len(signals) >= 2:
        confidence = "high"
    elif len(signals) == 1:
        confidence = "medium"
    else:
        confidence = "low"
    
    return JSChallengeResult(is_challenge=True, confidence=confidence, signals=signals)


def print_js_challenge_warning(result: JSChallengeResult, url: str) -> None:
    """æ‰“å° JS åçˆ¬æ£€æµ‹è­¦å‘Šä¿¡æ¯"""
    confidence_map = {"high": "é«˜", "medium": "ä¸­", "low": "ä½"}
    
    print(file=sys.stderr)
    print("=" * 70, file=sys.stderr)
    print(f"âš ï¸  æ£€æµ‹åˆ° JavaScript åçˆ¬ä¿æŠ¤ï¼ˆç½®ä¿¡åº¦ï¼š{confidence_map.get(result.confidence, result.confidence)}ï¼‰", file=sys.stderr)
    print("=" * 70, file=sys.stderr)
    print(file=sys.stderr)
    print("æ£€æµ‹åˆ°çš„ä¿¡å·ï¼š", file=sys.stderr)
    for sig in result.signals:
        print(f"  â€¢ {sig}", file=sys.stderr)
    print(file=sys.stderr)
    print("è¯´æ˜ï¼š", file=sys.stderr)
    print("  è¯¥ç½‘ç«™ä½¿ç”¨äº† JavaScript åçˆ¬æœºåˆ¶ï¼ˆå¦‚ Cloudflareï¼‰æ¥éªŒè¯è®¿é—®è€…ã€‚", file=sys.stderr)
    print("  çº¯ HTTP è¯·æ±‚æ— æ³•é€šè¿‡æ­¤éªŒè¯ï¼Œéœ€è¦æµè§ˆå™¨ç¯å¢ƒæ‰§è¡Œ JavaScriptã€‚", file=sys.stderr)
    print("  è¿™è¶…å‡ºäº†æœ¬å·¥å…·ï¼ˆä»…ä¾èµ– requestsï¼‰çš„èƒ½åŠ›èŒƒå›´ã€‚", file=sys.stderr)
    print(file=sys.stderr)
    print("å»ºè®®æ“ä½œï¼š", file=sys.stderr)
    for suggestion in result.get_suggestions(url):
        print(f"  {suggestion}", file=sys.stderr)
    print(file=sys.stderr)
    print("å¦‚æœæ‚¨ç¡®å®šè¦å¼ºåˆ¶å¤„ç†å½“å‰è·å–åˆ°çš„å†…å®¹ï¼ˆå¯èƒ½ä¸ºç©ºæˆ–ä¸å®Œæ•´ï¼‰ï¼Œ", file=sys.stderr)
    print("è¯·æ·»åŠ  --force å‚æ•°ã€‚", file=sys.stderr)
    print("=" * 70, file=sys.stderr)
    print(file=sys.stderr)


class _H1Extractor(HTMLParser):
    def __init__(self) -> None:
        super().__init__(convert_charrefs=True)
        self.in_h1 = False
        self.done = False
        self.buf: List[str] = []

    def handle_starttag(self, tag: str, attrs: Sequence[Tuple[str, Optional[str]]]) -> None:
        if self.done:
            return
        if tag.lower() == "h1":
            self.in_h1 = True

    def handle_endtag(self, tag: str) -> None:
        if self.done:
            return
        if tag.lower() == "h1" and self.in_h1:
            self.in_h1 = False
            self.done = True

    def handle_data(self, data: str) -> None:
        if self.done or (not self.in_h1) or (not data) or data.isspace():
            return
        self.buf.append(data)


def extract_h1(article_html: str) -> Optional[str]:
    parser = _H1Extractor()
    parser.feed(article_html)
    title = re.sub(r"\s+", " ", "".join(parser.buf)).strip()
    return title or None


@dataclass
class ValidationResult:
    image_refs: int
    local_image_refs: int
    asset_files: int
    missing_files: List[str]


def validate_markdown(md_path: str, assets_dir: str) -> ValidationResult:
    with open(md_path, "r", encoding="utf-8") as f:
        text = f.read()

    # ä»…æ ¡éªŒå›¾ç‰‡å¼•ç”¨ï¼š![](...)
    refs = re.findall(r"!\[[^\]]*\]\(([^)]+)\)", text)
    refs = [r.strip() for r in refs]
    local_refs = [r for r in refs if not re.match(r"^[a-z]+://", r, re.IGNORECASE)]

    missing: List[str] = []
    for r in local_refs:
        # æ”¯æŒç›¸å¯¹è·¯å¾„
        if os.path.isabs(r) or re.match(r"^[A-Za-z]:[\\/]", r):
            p = os.path.normpath(r)
        else:
            p = os.path.normpath(os.path.join(os.path.dirname(md_path), r))
        if not os.path.exists(p):
            missing.append(r)

    asset_files = 0
    if os.path.isdir(assets_dir):
        asset_files = len([f for f in os.listdir(assets_dir) if os.path.isfile(os.path.join(assets_dir, f))])

    return ValidationResult(
        image_refs=len(refs),
        local_image_refs=len(local_refs),
        asset_files=asset_files,
        missing_files=missing,
    )


# ============================================================================
# æ‰¹é‡ URL å¤„ç†åŠŸèƒ½
# ============================================================================


@dataclass
class BatchPageResult:
    """å•ä¸ªé¡µé¢çš„å¤„ç†ç»“æœ"""
    url: str
    title: str
    md_content: str
    success: bool
    error: Optional[str] = None
    order: int = 0  # ç”¨äºä¿æŒåŸå§‹é¡ºåº
    image_urls: List[str] = field(default_factory=list)  # æ”¶é›†åˆ°çš„å›¾ç‰‡ URL


@dataclass
class BatchConfig:
    """æ‰¹é‡å¤„ç†é…ç½®"""
    max_workers: int = 3
    delay: float = 1.0
    skip_errors: bool = False
    timeout: int = 60
    retries: int = 3
    max_html_bytes: int = _DEFAULT_MAX_HTML_BYTES
    best_effort_images: bool = True
    keep_html: bool = False
    target_id: Optional[str] = None
    target_class: Optional[str] = None
    clean_wiki_noise: bool = False  # æ¸…ç† Wiki ç³»ç»Ÿå™ªéŸ³ï¼ˆç¼–è¾‘æŒ‰é’®ã€å¯¼èˆªé“¾æ¥ç­‰ï¼‰
    download_images: bool = False  # æ˜¯å¦ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ°
    wechat: bool = False  # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ¨¡å¼
    # Phase 1: å¯¼èˆªå‰¥ç¦»å‚æ•°
    strip_nav: bool = False  # ç§»é™¤å¯¼èˆªå…ƒç´ 
    strip_page_toc: bool = False  # ç§»é™¤é¡µå†…ç›®å½•
    exclude_selectors: Optional[str] = None  # è‡ªå®šä¹‰ç§»é™¤é€‰æ‹©å™¨
    anchor_list_threshold: int = 0  # è¿ç»­é”šç‚¹åˆ—è¡¨ç§»é™¤é˜ˆå€¼ï¼Œé»˜è®¤ 0ï¼ˆå…³é—­ï¼‰
    # Phase 2: æ™ºèƒ½æ­£æ–‡å®šä½å‚æ•°
    docs_preset: Optional[str] = None  # æ–‡æ¡£æ¡†æ¶é¢„è®¾
    auto_detect: bool = False  # è‡ªåŠ¨æ£€æµ‹æ¡†æ¶


# ============================================================================
# å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ”¯æŒ
# ============================================================================


def is_wechat_article_url(url: str) -> bool:
    """
    æ£€æµ‹ URL æ˜¯å¦ä¸ºå¾®ä¿¡å…¬ä¼—å·æ–‡ç« é“¾æ¥
    
    æ”¯æŒçš„æ ¼å¼ï¼š
    - https://mp.weixin.qq.com/s/xxx
    - https://mp.weixin.qq.com/s?__biz=xxx
    - http://mp.weixin.qq.com/s/xxx
    """
    if not url:
        return False
    parsed = urlparse(url)
    return parsed.netloc in ("mp.weixin.qq.com", "weixin.qq.com")


def is_wechat_article_html(html: str) -> bool:
    """
    æ£€æµ‹ HTML å†…å®¹æ˜¯å¦å…·æœ‰å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ç‰¹å¾
    
    æ£€æµ‹ä»¥ä¸‹ç‰¹å¾ï¼š
    - åŒ…å« rich_media_content class
    - åŒ…å« js_article_data
    - åŒ…å«å¾®ä¿¡ç‰¹æœ‰çš„ meta æ ‡ç­¾
    """
    if not html:
        return False
    
    # æ£€æµ‹å¾®ä¿¡å…¬ä¼—å·ç‰¹æœ‰çš„ class å’Œæ ‡è¯†
    wechat_markers = [
        'class="rich_media_content"',
        "class='rich_media_content'",
        'id="js_article"',
        'data-mptype="article"',
        'var biz =',
        '__biz',
        'mp.weixin.qq.com',
    ]
    
    html_lower = html.lower()
    return any(marker.lower() in html_lower for marker in wechat_markers)


def extract_wechat_title(html: str) -> Optional[str]:
    """
    ä»å¾®ä¿¡å…¬ä¼—å· HTML ä¸­æå–æ–‡ç« æ ‡é¢˜
    
    å¾®ä¿¡å…¬ä¼—å·æ ‡é¢˜é€šå¸¸åœ¨ä»¥ä¸‹ä½ç½®ï¼š
    - <h1 class="rich_media_title">æ ‡é¢˜</h1>
    - <meta property="og:title" content="æ ‡é¢˜">
    - <title>æ ‡é¢˜</title>
    """
    if not html:
        return None
    
    # æ–¹æ³•1ï¼šä» rich_media_title æå–
    m = re.search(
        r'<h1[^>]*class=["\'][^"\']*rich_media_title[^"\']*["\'][^>]*>(.*?)</h1>',
        html,
        re.IGNORECASE | re.DOTALL
    )
    if m:
        title = re.sub(r'<[^>]+>', '', m.group(1))  # ç§»é™¤å†…éƒ¨æ ‡ç­¾
        title = re.sub(r'\s+', ' ', htmllib.unescape(title)).strip()
        if title:
            return title
    
    # æ–¹æ³•2ï¼šä» og:title meta æ ‡ç­¾æå–
    m = re.search(
        r'<meta[^>]*property=["\']og:title["\'][^>]*content=["\']([^"\']+)["\']',
        html,
        re.IGNORECASE
    )
    if m:
        title = htmllib.unescape(m.group(1)).strip()
        if title:
            return title
    
    # æ–¹æ³•3ï¼šä» twitter:title meta æ ‡ç­¾æå–
    m = re.search(
        r'<meta[^>]*name=["\']twitter:title["\'][^>]*content=["\']([^"\']+)["\']',
        html,
        re.IGNORECASE
    )
    if m:
        title = htmllib.unescape(m.group(1)).strip()
        if title:
            return title
    
    return None


def clean_wechat_noise(md_content: str) -> str:
    """
    æ¸…ç†å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ä¸­çš„å™ªéŸ³å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
    - ç‚¹èµã€åœ¨çœ‹ã€åˆ†äº«ç­‰äº¤äº’æŒ‰é’®æ–‡å­—
    - å°ç¨‹åºå¡ç‰‡æç¤º
    - æ‰«ç å…³æ³¨æç¤º
    - é˜…è¯»åŸæ–‡é“¾æ¥å™ªéŸ³
    - å…¶ä»–å¾®ä¿¡ç‰¹æœ‰çš„ UI å…ƒç´ 
    
    Args:
        md_content: åŸå§‹ Markdown å†…å®¹
    
    Returns:
        æ¸…ç†åçš„ Markdown å†…å®¹
    """
    result = md_content
    
    # 1. æ¸…ç†å¾®ä¿¡äº¤äº’æŒ‰é’®æ–‡å­—
    # å¦‚ï¼šVideo Mini Program Like ï¼Œè½»ç‚¹ä¸¤ä¸‹å–æ¶ˆèµ Wow ï¼Œè½»ç‚¹ä¸¤ä¸‹å–æ¶ˆåœ¨çœ‹
    result = re.sub(
        r'[,ï¼Œ\s]*(?:Video|Mini Program|Like|Wow|Share|Comment|Favorite|å¬è¿‡)\s*[,ï¼Œ]?\s*',
        '',
        result,
        flags=re.IGNORECASE
    )
    result = re.sub(
        r'[,ï¼Œ\s]*è½»ç‚¹ä¸¤ä¸‹å–æ¶ˆ(?:èµ|åœ¨çœ‹)\s*',
        '',
        result
    )
    
    # 2. æ¸…ç†å°ç¨‹åº/æ‰«ç æç¤º
    result = re.sub(
        r'(?:Scan to Follow|Scan with Weixin to\s*use this Mini Program|å¾®ä¿¡æ‰«ä¸€æ‰«å¯æ‰“å¼€æ­¤å†…å®¹.*?ä½¿ç”¨å®Œæ•´æœåŠ¡)\s*',
        '',
        result,
        flags=re.IGNORECASE | re.DOTALL
    )
    
    # 3. æ¸…ç† Cancel/Allow æŒ‰é’®æ–‡å­—
    result = re.sub(
        r'\[(?:Cancel|Allow|Got It)\]\(javascript:[^)]*\)\s*',
        '',
        result,
        flags=re.IGNORECASE
    )
    
    # 4. æ¸…ç† javascript:void(0) é“¾æ¥
    result = re.sub(
        r'\[([^\]]*)\]\(javascript:(?:void\(0\)|;)\)\s*',
        r'\1',
        result,
        flags=re.IGNORECASE
    )
    
    # 5. æ¸…ç†"ç»§ç»­æ»‘åŠ¨çœ‹ä¸‹ä¸€ä¸ª"ç­‰æç¤º
    result = re.sub(
        r'(?:ç»§ç»­æ»‘åŠ¨çœ‹ä¸‹ä¸€ä¸ª|å‘ä¸Šæ»‘åŠ¨çœ‹ä¸‹ä¸€ä¸ª|é¢„è§ˆæ—¶æ ‡ç­¾ä¸å¯ç‚¹)\s*',
        '',
        result
    )
    
    # 6. æ¸…ç†"åœ¨å°è¯´é˜…è¯»å™¨ä¸­æ²‰æµ¸é˜…è¯»"ç­‰æç¤º
    result = re.sub(
        r'(?:\*{2,}\s*)?åœ¨å°è¯´é˜…è¯»å™¨ä¸­æ²‰æµ¸é˜…è¯»\s*',
        '',
        result
    )
    
    # 7. æ¸…ç†å¾®ä¿¡ç‰¹æœ‰çš„ç¬¦å·å™ªéŸ³è¡Œ
    # å¦‚ï¼š: ï¼Œ ï¼Œ ï¼Œ ï¼Œ ï¼Œ ï¼Œ ï¼Œ ï¼Œ ï¼Œ ï¼Œ ï¼Œ ï¼Œ.
    result = re.sub(
        r'^[\s:,ï¼Œã€‚\.]+$',
        '',
        result,
        flags=re.MULTILINE
    )
    
    # 8. æ¸…ç†"ä½œè€…å¤´åƒ"ç­‰å›¾ç‰‡è¯´æ˜
    result = re.sub(
        r'!\[ä½œè€…å¤´åƒ\]\([^)]+\)',
        '',
        result
    )
    
    # 9. æ¸…ç†æ–‡æœ«è¿ç»­çš„äº¤äº’å…ƒç´ æ®‹ç•™
    # åŒ¹é…æ–‡æœ«å¯èƒ½å‡ºç°çš„å¤šä½™ç©ºç™½å’Œç¬¦å·
    result = re.sub(
        r'\n[\s:,ï¼Œã€‚\.\*]*$',
        '',
        result
    )
    
    # 10. æ¸…ç†è¿ç»­çš„ç©ºè¡Œï¼ˆæ¸…ç†åå¯èƒ½äº§ç”Ÿå¤šä½™ç©ºè¡Œï¼‰
    result = re.sub(r'\n{3,}', '\n\n', result)
    
    # 11. æ¸…ç†è¡Œé¦–è¡Œå°¾å¤šä½™ç©ºæ ¼
    result = re.sub(r'\n[ \t]+\n', '\n\n', result)
    
    return result.strip()


def clean_wiki_noise(md_content: str) -> str:
    """
    æ¸…ç† Wiki ç³»ç»Ÿäº§ç”Ÿçš„å™ªéŸ³å†…å®¹ï¼ŒåŒ…æ‹¬ï¼š
    - PukiWiki/MediaWiki ç¼–è¾‘å›¾æ ‡å’Œé“¾æ¥
    - è¿”å›é¡¶éƒ¨å¯¼èˆªé“¾æ¥
    - æ ‡é¢˜ä¸­çš„é”šç‚¹é“¾æ¥
    - å…¶ä»–å¸¸è§ Wiki UI å…ƒç´ 
    
    Args:
        md_content: åŸå§‹ Markdown å†…å®¹
    
    Returns:
        æ¸…ç†åçš„ Markdown å†…å®¹
    """
    result = md_content
    
    # 1. æ¸…ç†ç¼–è¾‘å›¾æ ‡å›¾ç‰‡ï¼š![Edit](xxx/paraedit.png) æˆ–ç±»ä¼¼çš„ç¼–è¾‘å›¾æ ‡
    # åŒ¹é…å„ç§ç¼–è¾‘å›¾æ ‡ï¼šparaedit.png, edit.png, pencil.png ç­‰
    result = re.sub(
        r'!\[(?:Edit|edit|ç·¨é›†|ç¼–è¾‘)?\]\([^)]*(?:paraedit|edit|pencil|secedit)[^)]*\)\s*\n?',
        '',
        result,
        flags=re.IGNORECASE
    )
    
    # 2. æ¸…ç†ç¼–è¾‘é“¾æ¥ï¼š[https://xxx/cmd=secedit...](xxx) æˆ– [ç¼–è¾‘](xxx?cmd=edit...)
    # è¿™ç§æ ¼å¼æ˜¯é“¾æ¥æ–‡æœ¬å°±æ˜¯ URL çš„æƒ…å†µ
    result = re.sub(
        r'\[https?://[^\]]*(?:cmd=(?:sec)?edit|action=edit)[^\]]*\]\([^)]+\)\s*\n?',
        '',
        result,
        flags=re.IGNORECASE
    )
    # æ™®é€šç¼–è¾‘é“¾æ¥
    result = re.sub(
        r'\[(?:ç·¨é›†|ç¼–è¾‘|Edit|edit)\]\([^)]*(?:cmd=(?:sec)?edit|action=edit)[^)]*\)\s*\n?',
        '',
        result,
        flags=re.IGNORECASE
    )
    
    # 3. æ¸…ç†è¿”å›é¡¶éƒ¨é“¾æ¥ï¼š[â†‘](xxx#navigator) æˆ– [â†‘](xxx#top)
    result = re.sub(
        r'\[â†‘\]\([^)]*#(?:navigator|top|head|pagetop)[^)]*\)\s*\n?',
        '',
        result,
        flags=re.IGNORECASE
    )
    
    # 4. æ¸…ç†æ ‡é¢˜ä¸­çš„é”šç‚¹é“¾æ¥ï¼š## æ ‡é¢˜ [â€ ](xxx#anchor) æˆ– [Â¶](xxx)
    # ä¿ç•™æ ‡é¢˜æ–‡æœ¬ï¼Œåªç§»é™¤é”šç‚¹é“¾æ¥éƒ¨åˆ†
    result = re.sub(
        r'(\#{1,6}\s+[^\n\[]+)\s*\[(?:â€ |Â¶|#)\]\([^)]+\)',
        r'\1',
        result
    )
    
    # 5. æ¸…ç†ç‹¬ç«‹çš„é”šç‚¹ç¬¦å·é“¾æ¥ï¼ˆä¸åœ¨æ ‡é¢˜ä¸­çš„ï¼‰
    result = re.sub(
        r'\s*\[(?:â€ |Â¶)\]\([^)]+\)',
        '',
        result
    )
    
    # 5.5. æ¸…ç†è¯„è®ºåŒºç¼–è¾‘é“¾æ¥ï¼š[?](xxx?cmd=edit...) æˆ–ç±»ä¼¼çš„é—®å·é“¾æ¥
    result = re.sub(
        r'\[\?\]\([^)]*(?:cmd=edit|action=edit)[^)]*\)',
        '',
        result,
        flags=re.IGNORECASE
    )
    
    # 6. æ¸…ç† PukiWiki ç‰¹æœ‰çš„å¯¼èˆª/å·¥å…·æ é“¾æ¥å—
    # å¦‚ï¼š[ [ãƒˆãƒƒãƒ—](xxx) ] è¿™ç§æ ¼å¼
    result = re.sub(
        r'\[\s*\[[^\]]+\]\([^)]+\)\s*\]\s*',
        '',
        result
    )
    
    # 7. æ¸…ç†è¿ç»­çš„ç©ºè¡Œï¼ˆæ¸…ç†åå¯èƒ½äº§ç”Ÿå¤šä½™ç©ºè¡Œï¼‰
    result = re.sub(r'\n{3,}', '\n\n', result)
    
    # 8. æ¸…ç†è¡Œé¦–å¤šä½™ç©ºæ ¼ï¼ˆæŸäº›æ¸…ç†åå¯èƒ½ç•™ä¸‹ï¼‰
    result = re.sub(r'\n[ \t]+\n', '\n\n', result)
    
    return result.strip()


class LinkExtractor(HTMLParser):
    """ä» HTML ä¸­æå–é“¾æ¥"""
    
    def __init__(self, base_url: str, pattern: Optional[str] = None, same_domain: bool = True):
        super().__init__(convert_charrefs=True)
        self.base_url = base_url
        self.base_domain = urlparse(base_url).netloc
        self.pattern = re.compile(pattern) if pattern else None
        self.same_domain = same_domain
        self.links: List[Tuple[str, str]] = []  # (url, text)
        self._in_a = False
        self._current_href: Optional[str] = None
        self._current_text: List[str] = []
    
    def handle_starttag(self, tag: str, attrs_list: Sequence[Tuple[str, Optional[str]]]) -> None:
        if tag.lower() == "a":
            attrs = dict(attrs_list)
            href = attrs.get("href")
            if href:
                self._in_a = True
                self._current_href = href
                self._current_text = []
    
    def handle_endtag(self, tag: str) -> None:
        if tag.lower() == "a" and self._in_a:
            if self._current_href:
                full_url = urljoin(self.base_url, self._current_href)
                text = "".join(self._current_text).strip()
                
                # æ£€æŸ¥åŸŸå
                if self.same_domain:
                    link_domain = urlparse(full_url).netloc
                    if link_domain != self.base_domain:
                        self._in_a = False
                        self._current_href = None
                        self._current_text = []
                        return
                
                # æ£€æŸ¥æ¨¡å¼åŒ¹é…
                if self.pattern:
                    if not self.pattern.search(full_url):
                        self._in_a = False
                        self._current_href = None
                        self._current_text = []
                        return
                
                # è·³è¿‡é”šç‚¹é“¾æ¥å’Œç¼–è¾‘é“¾æ¥
                if (self._current_href.startswith("#") or 
                    "cmd=edit" in full_url or 
                    "cmd=secedit" in full_url):
                    self._in_a = False
                    self._current_href = None
                    self._current_text = []
                    return
                
                self.links.append((full_url, text or full_url))
            
            self._in_a = False
            self._current_href = None
            self._current_text = []
    
    def handle_data(self, data: str) -> None:
        if self._in_a and data:
            self._current_text.append(data)


def extract_links_from_html(
    html: str,
    base_url: str,
    pattern: Optional[str] = None,
    same_domain: bool = True
) -> List[Tuple[str, str]]:
    """ä» HTML ä¸­æå–é“¾æ¥åˆ—è¡¨"""
    parser = LinkExtractor(base_url, pattern, same_domain)
    parser.feed(html)
    # å»é‡å¹¶ä¿æŒé¡ºåº
    seen = set()
    unique_links = []
    for url, text in parser.links:
        if url not in seen:
            seen.add(url)
            unique_links.append((url, text))
    return unique_links


def read_urls_file(filepath: str) -> List[Tuple[str, Optional[str]]]:
    """
    è¯»å– URL åˆ—è¡¨æ–‡ä»¶
    
    æ”¯æŒæ ¼å¼ï¼š
    - æ¯è¡Œä¸€ä¸ª URL
    - # å¼€å¤´ä¸ºæ³¨é‡Š
    - URL | æ ‡é¢˜  æ ¼å¼æŒ‡å®šè‡ªå®šä¹‰æ ‡é¢˜
    - ç©ºè¡Œå¿½ç•¥
    """
    urls: List[Tuple[str, Optional[str]]] = []
    with open(filepath, "r", encoding="utf-8") as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            # è·³è¿‡ç©ºè¡Œå’Œæ³¨é‡Š
            if not line or line.startswith("#"):
                continue
            
            # æ”¯æŒ URL | æ ‡é¢˜ æ ¼å¼
            if "|" in line:
                parts = line.split("|", 1)
                url = parts[0].strip()
                title = parts[1].strip() if len(parts) > 1 else None
            else:
                url = line
                title = None
            
            # éªŒè¯ URL æ ¼å¼
            if not url.startswith(("http://", "https://")):
                print(f"è­¦å‘Šï¼šç¬¬ {line_num} è¡Œä¸æ˜¯æœ‰æ•ˆçš„ URLï¼Œå·²è·³è¿‡ï¼š{url}", file=sys.stderr)
                continue
            
            urls.append((url, title))
    
    return urls


def _make_anchor_id(text: str) -> str:
    """ç”Ÿæˆ Markdown é”šç‚¹ ID"""
    # è½¬å°å†™ï¼Œç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œç©ºæ ¼è½¬è¿å­—ç¬¦
    anchor = text.lower()
    anchor = re.sub(r"[^\w\s\u4e00-\u9fff-]", "", anchor)  # ä¿ç•™ä¸­æ—¥éŸ©å­—ç¬¦
    anchor = re.sub(r"\s+", "-", anchor)
    anchor = re.sub(r"-+", "-", anchor)
    return anchor.strip("-") or "section"


# ============================================================================
# Phase 3-A: é”šç‚¹å†²çªæ£€æµ‹ä¸ä¿®å¤
# ============================================================================

@dataclass
class AnchorCollisionStats:
    """é”šç‚¹å†²çªç»Ÿè®¡ä¿¡æ¯"""
    total_anchors: int = 0
    unique_anchors: int = 0
    collision_count: int = 0  # å‘ç”Ÿå†²çªçš„é”šç‚¹æ•°é‡
    collision_examples: List[Tuple[str, int]] = field(default_factory=list)  # (åŸå§‹é”šç‚¹, é‡å¤æ¬¡æ•°)
    
    @property
    def has_collisions(self) -> bool:
        return self.collision_count > 0
    
    def print_summary(self, file=None, max_examples: int = 5) -> None:
        """æ‰“å°ç»Ÿè®¡æ‘˜è¦"""
        if file is None:
            file = sys.stderr
        if not self.has_collisions:
            return
        print(f"\nâš ï¸  é”šç‚¹å†²çªæ£€æµ‹ï¼š", file=file)
        print(f"  â€¢ æ€»é”šç‚¹æ•°ï¼š{self.total_anchors}", file=file)
        print(f"  â€¢ å”¯ä¸€é”šç‚¹ï¼š{self.unique_anchors}", file=file)
        print(f"  â€¢ å†²çªé”šç‚¹ï¼š{self.collision_count} ä¸ªï¼ˆå·²è‡ªåŠ¨ä¿®å¤ï¼‰", file=file)
        if self.collision_examples:
            print(f"  â€¢ å†²çªç¤ºä¾‹ï¼ˆæ˜¾ç¤ºå‰ {min(len(self.collision_examples), max_examples)} ä¸ªï¼‰ï¼š", file=file)
            for anchor, count in self.collision_examples[:max_examples]:
                print(f"    - #{anchor} â†’ #{anchor}, #{anchor}-2, ... #{anchor}-{count}", file=file)


class AnchorManager:
    """
    é”šç‚¹ç®¡ç†å™¨ - è´Ÿè´£é”šç‚¹ç”Ÿæˆã€å†²çªæ£€æµ‹ä¸å»é‡ï¼ˆPhase 3-Aï¼‰
    
    ä½¿ç”¨æ–¹å¼ï¼š
    1. åˆ›å»ºå®ä¾‹
    2. å¯¹æ¯ä¸ªæ ‡é¢˜è°ƒç”¨ register(title) è·å–å»é‡åçš„é”šç‚¹
    3. è°ƒç”¨ get_stats() è·å–å†²çªç»Ÿè®¡
    
    ç¤ºä¾‹ï¼š
        manager = AnchorManager()
        anchor1 = manager.register("Introduction")  # -> "introduction"
        anchor2 = manager.register("Introduction")  # -> "introduction-2"
        anchor3 = manager.register("Introduction")  # -> "introduction-3"
    """
    
    def __init__(self):
        self._anchor_counts: Dict[str, int] = {}  # åŸºç¡€é”šç‚¹ -> å·²ä½¿ç”¨æ¬¡æ•°
        self._title_to_anchor: Dict[str, str] = {}  # åŸå§‹æ ‡é¢˜ -> åˆ†é…çš„é”šç‚¹ï¼ˆä»…ç¬¬ä¸€æ¬¡ï¼‰
        self._all_anchors: List[str] = []  # æ‰€æœ‰ç”Ÿæˆçš„é”šç‚¹ï¼ˆæŒ‰é¡ºåºï¼‰
        self._collisions: Dict[str, int] = {}  # å‘ç”Ÿå†²çªçš„åŸºç¡€é”šç‚¹ -> æ€»æ¬¡æ•°
    
    def register(self, title: str, url: Optional[str] = None) -> str:
        """
        æ³¨å†Œæ ‡é¢˜å¹¶è¿”å›å»é‡åçš„é”šç‚¹ ID
        
        Args:
            title: æ ‡é¢˜æ–‡æœ¬
            url: å¯é€‰çš„é¡µé¢ URLï¼ˆç”¨äºæ›´ç²¾ç¡®çš„å»é‡ï¼Œæš‚æœªä½¿ç”¨ï¼‰
        
        Returns:
            å»é‡åçš„é”šç‚¹ IDï¼ˆå¦‚ "introduction" æˆ– "introduction-2"ï¼‰
        """
        base_anchor = _make_anchor_id(title)
        
        if base_anchor not in self._anchor_counts:
            # é¦–æ¬¡å‡ºç°ï¼Œç›´æ¥ä½¿ç”¨
            self._anchor_counts[base_anchor] = 1
            self._all_anchors.append(base_anchor)
            return base_anchor
        else:
            # å·²å­˜åœ¨ï¼Œéœ€è¦æ·»åŠ åç¼€
            count = self._anchor_counts[base_anchor] + 1
            self._anchor_counts[base_anchor] = count
            
            # è®°å½•å†²çª
            if base_anchor not in self._collisions:
                self._collisions[base_anchor] = 2  # ç¬¬ä¸€æ¬¡å†²çªæ—¶ï¼Œå·²ç»æœ‰ 2 ä¸ª
            else:
                self._collisions[base_anchor] = count
            
            unique_anchor = f"{base_anchor}-{count}"
            self._all_anchors.append(unique_anchor)
            return unique_anchor
    
    def get_anchor_for_title(self, title: str) -> Optional[str]:
        """
        è·å–å·²æ³¨å†Œæ ‡é¢˜çš„é”šç‚¹ï¼ˆä¸æ³¨å†Œæ–°çš„ï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•ç”¨äºæŸ¥è¯¢ï¼Œä¸ä¼šåˆ›å»ºæ–°é”šç‚¹
        """
        base_anchor = _make_anchor_id(title)
        if base_anchor in self._anchor_counts:
            return base_anchor
        return None
    
    def get_stats(self) -> AnchorCollisionStats:
        """è·å–é”šç‚¹å†²çªç»Ÿè®¡ä¿¡æ¯"""
        stats = AnchorCollisionStats(
            total_anchors=len(self._all_anchors),
            unique_anchors=len(self._anchor_counts),
            collision_count=len(self._collisions),
        )
        
        # æŒ‰å†²çªæ¬¡æ•°æ’åºï¼Œå–å‰ 10 ä¸ªä½œä¸ºç¤ºä¾‹
        sorted_collisions = sorted(
            self._collisions.items(),
            key=lambda x: -x[1]
        )
        stats.collision_examples = sorted_collisions[:10]
        
        return stats
    
    def reset(self) -> None:
        """é‡ç½®ç®¡ç†å™¨çŠ¶æ€"""
        self._anchor_counts.clear()
        self._title_to_anchor.clear()
        self._all_anchors.clear()
        self._collisions.clear()


def process_single_url(
    session: requests.Session,
    url: str,
    config: BatchConfig,
    custom_title: Optional[str] = None,
    order: int = 0,
) -> BatchPageResult:
    """å¤„ç†å•ä¸ª URLï¼Œè¿”å›ç»“æœ"""
    try:
        # è·å–é¡µé¢
        page_html = fetch_html(
            session=session,
            url=url,
            timeout_s=config.timeout,
            retries=config.retries,
            max_html_bytes=config.max_html_bytes,
        )
        
        # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« è‡ªåŠ¨æ£€æµ‹
        is_wechat = config.wechat
        if not is_wechat and is_wechat_article_url(url):
            is_wechat = True
        elif not is_wechat and is_wechat_article_html(page_html):
            is_wechat = True
        
        # ç¡®å®šæ­£æ–‡æå–ç­–ç•¥
        target_id = config.target_id
        target_class = config.target_class
        exclude_selectors = config.exclude_selectors
        strip_nav = config.strip_nav
        strip_page_toc = config.strip_page_toc
        anchor_list_threshold = config.anchor_list_threshold
        
        # å¾®ä¿¡æ¨¡å¼ä¸‹ï¼Œå¦‚æœæœªæŒ‡å®š targetï¼Œè‡ªåŠ¨ä½¿ç”¨ rich_media_content
        if is_wechat and not target_id and not target_class:
            target_class = "rich_media_content"
        
        # Phase 2: è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ¡†æ¶
        detected_preset: Optional[str] = None
        if config.auto_detect and not config.docs_preset:
            detected_preset, confidence, signals = detect_docs_framework(page_html)
            if detected_preset and confidence >= 0.5:
                preset = DOCS_PRESETS.get(detected_preset)
                if preset:
                    # é«˜ç½®ä¿¡åº¦æ—¶åº”ç”¨é¢„è®¾
                    if not target_id and preset.target_ids:
                        target_id = ",".join(preset.target_ids)
                    if not target_class and preset.target_classes:
                        target_class = ",".join(preset.target_classes)
                    # æ‰¹é‡æ¨¡å¼ä¸‹ auto-detect ä¹Ÿåº”å°½é‡å¤ç”¨é¢„è®¾çš„â€œå»å¯¼èˆªâ€èƒ½åŠ›ï¼Œä¿æŒä¸å•é¡µæ¨¡å¼ä¸€è‡´
                    preset_excludes = ",".join(preset.exclude_selectors) if preset.exclude_selectors else ""
                    if preset_excludes:
                        if exclude_selectors:
                            exclude_selectors = f"{exclude_selectors},{preset_excludes}"
                        else:
                            exclude_selectors = preset_excludes
                    strip_nav = True
                    strip_page_toc = True
                    if anchor_list_threshold == 0:
                        anchor_list_threshold = 10
        
        # æå–æ­£æ–‡ï¼ˆæ”¯æŒå¤šå€¼ targetï¼ŒT2.1ï¼‰
        if target_id or target_class:
            # ä½¿ç”¨å¤šå€¼æå–
            article_html, matched = extract_target_html_multi(
                page_html, 
                target_ids=target_id, 
                target_classes=target_class
            )
            if not article_html:
                # å›é€€åˆ°å•å€¼æå–ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
                article_html = extract_target_html(
                    page_html,
                    target_id=target_id.split(",")[0] if target_id else None,
                    target_class=target_class.split(",")[0] if target_class else None,
                ) or ""
            if not article_html:
                article_html = extract_main_html(page_html)
        else:
            article_html = extract_main_html(page_html)
        
        # Phase 1: HTML å¯¼èˆªå…ƒç´ å‰¥ç¦»ï¼ˆåœ¨æå–æ­£æ–‡åã€è½¬æ¢ Markdown å‰ï¼‰
        strip_selectors = get_strip_selectors(
            strip_nav=strip_nav,
            strip_page_toc=strip_page_toc,
            exclude_selectors=exclude_selectors,
        )
        if strip_selectors:
            article_html, _ = strip_html_elements(article_html, strip_selectors)
        
        # æå–æ ‡é¢˜ï¼ˆå¾®ä¿¡æ¨¡å¼ä¸‹ä¼˜å…ˆä½¿ç”¨ä¸“ç”¨æå–å‡½æ•°ï¼‰
        if custom_title:
            title = custom_title
        elif is_wechat:
            title = extract_wechat_title(page_html) or extract_h1(article_html) or extract_title(page_html) or "Untitled"
        else:
            title = extract_h1(article_html) or extract_title(page_html) or "Untitled"
        
        # æ”¶é›†å›¾ç‰‡ URLï¼ˆå¦‚æœéœ€è¦ä¸‹è½½å›¾ç‰‡ï¼‰
        image_urls: List[str] = []
        if config.download_images:
            collector = ImageURLCollector(base_url=url)
            collector.feed(article_html)
            image_urls = uniq_preserve_order(collector.image_urls)
        
        # è½¬æ¢ä¸º Markdownï¼ˆæ‰¹é‡æ¨¡å¼å…ˆä¸æ›¿æ¢å›¾ç‰‡è·¯å¾„ï¼Œåç»­ç»Ÿä¸€å¤„ç†ï¼‰
        md_body = html_to_markdown(
            article_html=article_html,
            base_url=url,
            url_to_local={},  # å…ˆä¸æ›¿æ¢å›¾ç‰‡è·¯å¾„
            keep_html=config.keep_html,
        )
        md_body = strip_duplicate_h1(md_body, title)
        
        # æ¸…ç†å™ªéŸ³å†…å®¹
        if is_wechat:
            md_body = clean_wechat_noise(md_body)
        if config.clean_wiki_noise:
            md_body = clean_wiki_noise(md_body)
        
        # Phase 1: Markdown é”šç‚¹åˆ—è¡¨å‰¥ç¦»
        if anchor_list_threshold > 0:
            md_body, _ = strip_anchor_lists(md_body, anchor_list_threshold)
        
        return BatchPageResult(
            url=url,
            title=title,
            md_content=md_body,
            success=True,
            order=order,
            image_urls=image_urls,
        )
    
    except Exception as e:
        return BatchPageResult(
            url=url,
            title=custom_title or url,
            md_content="",
            success=False,
            error=str(e),
            order=order,
        )


def batch_process_urls(
    session: requests.Session,
    urls: List[Tuple[str, Optional[str]]],
    config: BatchConfig,
    progress_callback: Optional[Callable[[int, int, str], None]] = None,
) -> List[BatchPageResult]:
    """
    æ‰¹é‡å¤„ç† URL åˆ—è¡¨
    
    Args:
        session: requests.Session
        urls: [(url, custom_title), ...]
        config: æ‰¹é‡å¤„ç†é…ç½®
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, url)
    
    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨
    """
    results: List[BatchPageResult] = []
    total = len(urls)
    lock = threading.Lock()
    last_request_time = [0.0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
    
    def process_with_delay(args: Tuple[int, str, Optional[str]]) -> BatchPageResult:
        idx, url, custom_title = args
        
        # æ§åˆ¶è¯·æ±‚é—´éš”
        with lock:
            now = time.time()
            elapsed = now - last_request_time[0]
            if elapsed < config.delay:
                time.sleep(config.delay - elapsed)
            last_request_time[0] = time.time()
        
        if progress_callback:
            progress_callback(idx + 1, total, url)
        
        return process_single_url(
            session=session,
            url=url,
            config=config,
            custom_title=custom_title,
            order=idx,
        )
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=config.max_workers) as executor:
        args_list = [(i, url, title) for i, (url, title) in enumerate(urls)]
        futures = {executor.submit(process_with_delay, args): args for args in args_list}
        
        for future in as_completed(futures):
            result = future.result()
            results.append(result)
            
            if not result.success and not config.skip_errors:
                # å–æ¶ˆå‰©ä½™ä»»åŠ¡
                for f in futures:
                    f.cancel()
                raise RuntimeError(f"å¤„ç†å¤±è´¥ï¼š{result.url}\né”™è¯¯ï¼š{result.error}")
    
    # æŒ‰åŸå§‹é¡ºåºæ’åº
    results.sort(key=lambda r: r.order)
    return results


def batch_download_images(
    session: requests.Session,
    results: List[BatchPageResult],
    assets_dir: str,
    md_dir: str,
    timeout_s: int = 60,
    retries: int = 3,
    best_effort: bool = True,
    progress_callback: Optional[Callable[[int, int, str], None]] = None,
    *,
    redact_urls: bool = True,
    max_image_bytes: int = _DEFAULT_MAX_IMAGE_BYTES,
) -> Dict[str, str]:
    """
    æ‰¹é‡ä¸‹è½½æ‰€æœ‰é¡µé¢çš„å›¾ç‰‡åˆ°ç»Ÿä¸€çš„ assets ç›®å½•
    
    Args:
        session: requests.Session
        results: æ‰¹é‡å¤„ç†ç»“æœåˆ—è¡¨
        assets_dir: å›¾ç‰‡ä¿å­˜ç›®å½•
        md_dir: Markdown æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ˆç”¨äºè®¡ç®—ç›¸å¯¹è·¯å¾„ï¼‰
        timeout_s: è¯·æ±‚è¶…æ—¶
        retries: é‡è¯•æ¬¡æ•°
        best_effort: å¤±è´¥æ—¶æ˜¯å¦ç»§ç»­
        progress_callback: è¿›åº¦å›è°ƒ (current, total, url)
    
    Returns:
        URL åˆ°æœ¬åœ°ç›¸å¯¹è·¯å¾„çš„æ˜ å°„å­—å…¸
    """
    # æ”¶é›†æ‰€æœ‰å”¯ä¸€çš„å›¾ç‰‡ URL
    all_image_urls: List[str] = []
    seen: set = set()
    for result in results:
        if result.success:
            for url in result.image_urls:
                if url not in seen:
                    all_image_urls.append(url)
                    seen.add(url)
    
    if not all_image_urls:
        return {}
    
    os.makedirs(assets_dir, exist_ok=True)
    url_to_local: Dict[str, str] = {}
    total = len(all_image_urls)
    anon_session = _create_anonymous_image_session(session)
    max_bytes: Optional[int] = max_image_bytes if (max_image_bytes and max_image_bytes > 0) else None
    known_image_exts = {".png", ".jpg", ".jpeg", ".gif", ".webp", ".svg", ".avif", ".bmp", ".ico"}

    # è®°å½•æ¯ä¸ªå›¾ç‰‡ URL çš„â€œå½’å±é¡µé¢â€ï¼Œç”¨äº Referer ä¸åŒåŸŸåˆ¤æ–­
    img_referer: Dict[str, str] = {}
    for result in results:
        if not result.success:
            continue
        for u in result.image_urls:
            if u and u not in img_referer:
                img_referer[u] = result.url
    
    for idx, img_url in enumerate(all_image_urls, start=1):
        if progress_callback:
            progress_callback(idx, total, img_url)

        if not img_url:
            continue
        parsed_img = urlparse(img_url)
        if parsed_img.scheme not in ("http", "https"):
            continue

        referer_url = img_referer.get(img_url) or ""
        referer = redact_url(referer_url) if (redact_urls and referer_url) else referer_url
        last_err: Optional[Exception] = None
        r: Optional[requests.Response] = None
        
        for attempt in range(1, retries + 1):
            try:
                # ä½¿ç”¨å®‰å…¨çš„å›¾ç‰‡è·å–å‡½æ•°ï¼Œæ‰‹åŠ¨å¤„ç†é‡å®šå‘å¹¶åœ¨è·¨åŸŸæ—¶åˆ‡æ¢åˆ°å¹²å‡€ session
                r = _safe_image_get(
                    img_url=img_url,
                    page_url=referer_url or "",  # ç”¨ referer_url åˆ¤æ–­åŒåŸŸ
                    session=session,
                    anon_session=anon_session,
                    timeout_s=timeout_s,
                    referer=referer,
                )
                r.raise_for_status()
                break
            except Exception as e:
                last_err = e
                # å…³é”®ï¼šå¦‚æœ raise_for_status() å¤±è´¥ï¼ˆ4xx/5xxï¼‰ï¼Œr è™½ç„¶é None ä½†å†…å®¹æ— æ•ˆ
                # å¿…é¡»é‡ç½®ä¸º Noneï¼Œå¦åˆ™åç»­ `if r is None:` è¯¯åˆ¤ä¸ºæˆåŠŸ
                if r is not None:
                    try:
                        r.close()
                    except Exception:
                        pass
                    r = None
                if attempt >= retries:
                    break
                time.sleep(min(2.0, 0.4 * attempt))
        
        if r is None:
            if best_effort:
                print(f"  è­¦å‘Šï¼šå›¾ç‰‡ä¸‹è½½å¤±è´¥ï¼Œå·²è·³è¿‡ï¼š{img_url[:60]}...", file=sys.stderr)
                continue
            raise last_err or RuntimeError("image download failed")
        
        try:
            # ç”Ÿæˆæœ¬åœ°æ–‡ä»¶åï¼ˆæ‰©å±•åä¼˜å…ˆå– URL pathï¼Œå…¶æ¬¡ Content-Typeï¼Œå†æ¬¡å—…æ¢é¦–å—å†…å®¹ï¼‰
            base = os.path.basename(parsed_img.path.rstrip("/"))
            base = unquote(base) or f"image-{idx}"
            name_root, name_ext = os.path.splitext(base)

            it = r.iter_content(chunk_size=1024 * 64)
            head = b""
            for chunk in it:
                if chunk:
                    head = chunk
                    break

            if (not name_ext) or (name_ext.lower() not in known_image_exts):
                detected = ext_from_content_type(r.headers.get("Content-Type") if r else None) or sniff_ext(head or b"")
                if detected:
                    name_ext = detected
                elif not name_ext:
                    name_ext = ".bin"

            safe_root = _sanitize_filename_part(name_root)
            filename = f"{idx:03d}-{safe_root}{name_ext}"
            filename = _safe_path_length(assets_dir, filename)
            local_path = os.path.join(assets_dir, filename)
            tmp_path = local_path + ".part"

            size = 0
            try:
                with open(tmp_path, "wb") as f:
                    if head:
                        f.write(head)
                        size += len(head)
                        if max_bytes is not None and size > max_bytes:
                            raise RuntimeError(f"å›¾ç‰‡è¿‡å¤§ï¼ˆ>{max_bytes} bytesï¼‰")
                    for chunk in it:
                        if not chunk:
                            continue
                        size += len(chunk)
                        if max_bytes is not None and size > max_bytes:
                            raise RuntimeError(f"å›¾ç‰‡è¿‡å¤§ï¼ˆ>{max_bytes} bytesï¼‰")
                        f.write(chunk)
                os.replace(tmp_path, local_path)
            finally:
                if os.path.exists(tmp_path):
                    try:
                        os.remove(tmp_path)
                    except OSError:
                        pass
        except Exception as e:
            if best_effort:
                print(f"  è­¦å‘Šï¼šå›¾ç‰‡ä¿å­˜å¤±è´¥ï¼Œå·²è·³è¿‡ï¼š{img_url[:60]}...\n    é”™è¯¯ï¼š{e}", file=sys.stderr)
                continue
            raise
        finally:
            try:
                r.close()
            except Exception:
                pass
        
        # è®¡ç®—ç›¸å¯¹è·¯å¾„
        local_abs = os.path.abspath(local_path)
        md_dir_abs = os.path.abspath(md_dir or ".")
        rel = os.path.relpath(local_abs, start=md_dir_abs)
        url_to_local[img_url] = rel.replace("\\", "/")
    
    return url_to_local


def replace_image_urls_in_markdown(md_content: str, url_to_local: Dict[str, str]) -> str:
    """
    æ›¿æ¢ Markdown å†…å®¹ä¸­çš„å›¾ç‰‡ URL ä¸ºæœ¬åœ°è·¯å¾„
    
    Args:
        md_content: Markdown å†…å®¹
        url_to_local: URL åˆ°æœ¬åœ°è·¯å¾„çš„æ˜ å°„
    
    Returns:
        æ›¿æ¢åçš„ Markdown å†…å®¹
    """
    result = md_content
    for url, local_path in url_to_local.items():
        # æ–¹æ³•1ï¼šç›´æ¥å­—ç¬¦ä¸²æ›¿æ¢ï¼ˆæœ€å¯é ï¼‰
        # åŒ¹é… ](url) æ¨¡å¼ï¼Œå°† url æ›¿æ¢ä¸ºæœ¬åœ°è·¯å¾„
        result = result.replace(f"]({url})", f"]({local_path})")
        
        # æ–¹æ³•2ï¼šä¹Ÿæ›¿æ¢å¯èƒ½çš„ URL ç¼–ç å˜ä½“
        # å°è¯•æ›¿æ¢ URL ç¼–ç ç‰ˆæœ¬
        encoded_url = quote(url, safe=':/?&=#')
        if encoded_url != url:
            result = result.replace(f"]({encoded_url})", f"]({local_path})")
        # å°è¯•æ›¿æ¢è§£ç ç‰ˆæœ¬
        decoded_url = unquote(url)
        if decoded_url != url:
            result = result.replace(f"]({decoded_url})", f"]({local_path})")
    
    return result


def build_url_to_anchor_map(results: List[BatchPageResult]) -> Dict[str, str]:
    """
    æ„å»º URL åˆ°é”šç‚¹ ID çš„æ˜ å°„è¡¨
    
    Args:
        results: æ‰¹é‡å¤„ç†ç»“æœåˆ—è¡¨
    
    Returns:
        URL -> é”šç‚¹ ID çš„æ˜ å°„å­—å…¸
    """
    url_to_anchor: Dict[str, str] = {}
    for result in results:
        if result.success:
            anchor = _make_anchor_id(result.title)
            # æ·»åŠ åŸå§‹ URL
            url_to_anchor[result.url] = anchor
            # æ·»åŠ å¸¸è§çš„ URL å˜ä½“ï¼ˆå¸¦/ä¸å¸¦ç«¯å£ã€ç¼–ç å˜ä½“ç­‰ï¼‰
            parsed = urlparse(result.url)
            # ä¸å¸¦ç«¯å£çš„ç‰ˆæœ¬
            if parsed.port:
                no_port_url = f"{parsed.scheme}://{parsed.hostname}{parsed.path}"
                if parsed.query:
                    no_port_url += f"?{parsed.query}"
                url_to_anchor[no_port_url] = anchor
            # å¸¦é»˜è®¤ç«¯å£çš„ç‰ˆæœ¬
            if parsed.scheme == "https" and not parsed.port:
                with_port = f"{parsed.scheme}://{parsed.hostname}:443{parsed.path}"
                if parsed.query:
                    with_port += f"?{parsed.query}"
                url_to_anchor[with_port] = anchor
            elif parsed.scheme == "http" and not parsed.port:
                with_port = f"{parsed.scheme}://{parsed.hostname}:80{parsed.path}"
                if parsed.query:
                    with_port += f"?{parsed.query}"
                url_to_anchor[with_port] = anchor
    return url_to_anchor


def build_url_to_anchor_map_with_manager(
    results: List[BatchPageResult],
    result_anchors: List[Tuple[BatchPageResult, str]],
) -> Dict[str, str]:
    """
    æ„å»º URL åˆ°é”šç‚¹ ID çš„æ˜ å°„è¡¨ï¼ˆä½¿ç”¨ AnchorManager ç”Ÿæˆçš„å»é‡é”šç‚¹ï¼‰
    
    Phase 3-A: æ­¤å‡½æ•°ä½¿ç”¨é¢„å…ˆæ³¨å†Œçš„å»é‡é”šç‚¹ï¼Œç¡®ä¿é“¾æ¥æ”¹å†™æ—¶æŒ‡å‘æ­£ç¡®çš„é”šç‚¹ã€‚
    
    Args:
        results: æ‰¹é‡å¤„ç†ç»“æœåˆ—è¡¨
        result_anchors: (result, anchor) å¯¹åˆ—è¡¨ï¼ŒåŒ…å«å»é‡åçš„é”šç‚¹
    
    Returns:
        URL -> é”šç‚¹ ID çš„æ˜ å°„å­—å…¸
    """
    url_to_anchor: Dict[str, str] = {}
    
    for result, anchor in result_anchors:
        if not result.success or not anchor:
            continue
        
        # æ·»åŠ åŸå§‹ URL
        url_to_anchor[result.url] = anchor
        
        # æ·»åŠ å¸¸è§çš„ URL å˜ä½“ï¼ˆå¸¦/ä¸å¸¦ç«¯å£ã€ç¼–ç å˜ä½“ç­‰ï¼‰
        parsed = urlparse(result.url)
        
        # ä¸å¸¦ç«¯å£çš„ç‰ˆæœ¬
        if parsed.port:
            no_port_url = f"{parsed.scheme}://{parsed.hostname}{parsed.path}"
            if parsed.query:
                no_port_url += f"?{parsed.query}"
            url_to_anchor[no_port_url] = anchor
        
        # å¸¦é»˜è®¤ç«¯å£çš„ç‰ˆæœ¬
        if parsed.scheme == "https" and not parsed.port:
            with_port = f"{parsed.scheme}://{parsed.hostname}:443{parsed.path}"
            if parsed.query:
                with_port += f"?{parsed.query}"
            url_to_anchor[with_port] = anchor
        elif parsed.scheme == "http" and not parsed.port:
            with_port = f"{parsed.scheme}://{parsed.hostname}:80{parsed.path}"
            if parsed.query:
                with_port += f"?{parsed.query}"
            url_to_anchor[with_port] = anchor
    
    return url_to_anchor


def rewrite_internal_links(md_content: str, url_to_anchor: Dict[str, str]) -> Tuple[str, int]:
    """
    å°† Markdown ä¸­çš„å¤–éƒ¨é“¾æ¥æ”¹å†™ä¸ºå†…éƒ¨é”šç‚¹é“¾æ¥
    
    Args:
        md_content: Markdown å†…å®¹
        url_to_anchor: URL åˆ°é”šç‚¹çš„æ˜ å°„
    
    Returns:
        (æ”¹å†™åçš„å†…å®¹, æ”¹å†™çš„é“¾æ¥æ•°é‡)
    """
    if not url_to_anchor:
        return md_content, 0
    
    rewrite_count = 0
    result = md_content
    
    # åŒ¹é… Markdown é“¾æ¥è¯­æ³•ï¼š[text](url)
    # ä½†ä¸åŒ¹é…å›¾ç‰‡è¯­æ³• ![alt](url)
    link_pattern = re.compile(r'(?<!!)\[([^\]]+)\]\(([^)]+)\)')
    
    def replace_link(match: re.Match) -> str:
        nonlocal rewrite_count
        text = match.group(1)
        url = match.group(2)
        
        candidates: List[str] = [url]

        # å°è¯• URL è§£ç ååŒ¹é…
        try:
            decoded_url = unquote(url)
            if decoded_url and decoded_url != url:
                candidates.append(decoded_url)
        except Exception:
            pass

        # å¸¸è§å˜ä½“ï¼šå» fragmentï¼ˆ#/sectionï¼‰ï¼Œä»¥åŠæŒ‰è„±æ•è§„åˆ™å»æ‰ query/fragment
        try:
            parsed = urlparse(url)
            if parsed.fragment:
                candidates.append(parsed._replace(fragment="").geturl())
        except Exception:
            pass
        try:
            candidates.append(redact_url(url))
        except Exception:
            pass

        for c in candidates:
            if not c:
                continue
            anchor = url_to_anchor.get(c)
            if anchor:
                rewrite_count += 1
                return f"[{text}](#{anchor})"
        
        return match.group(0)  # ä¿æŒåŸæ ·
    
    result = link_pattern.sub(replace_link, result)
    return result, rewrite_count


def generate_merged_markdown(
    results: List[BatchPageResult],
    include_toc: bool = True,
    main_title: Optional[str] = None,
    source_url: Optional[str] = None,
    rewrite_links: bool = False,
    show_source_summary: bool = True,
    redact_urls: bool = True,
) -> Tuple[str, AnchorCollisionStats]:
    """
    å°†å¤šä¸ªé¡µé¢ç»“æœåˆå¹¶ä¸ºå•ä¸ª Markdown æ–‡æ¡£
    
    Args:
        results: å¤„ç†ç»“æœåˆ—è¡¨
        include_toc: æ˜¯å¦åŒ…å«ç›®å½•
        main_title: æ–‡æ¡£ä¸»æ ‡é¢˜
        source_url: æ¥æº URLï¼ˆç”¨äº frontmatterï¼‰
        rewrite_links: æ˜¯å¦å°†ç«™å†…é“¾æ¥æ”¹å†™ä¸ºé”šç‚¹
        show_source_summary: æ˜¯å¦æ˜¾ç¤ºæ¥æºä¿¡æ¯æ±‡æ€»
    
    Returns:
        (åˆå¹¶åçš„ Markdown å†…å®¹, é”šç‚¹å†²çªç»Ÿè®¡)
    """
    parts: List[str] = []
    
    # Phase 3-A: ä½¿ç”¨ AnchorManager è¿›è¡Œé”šç‚¹å»é‡
    anchor_manager = AnchorManager()
    
    # å…ˆä¸ºæ‰€æœ‰ç»“æœæ³¨å†Œé”šç‚¹ï¼ˆç¡®ä¿ç›®å½•å’Œå†…å®¹ä½¿ç”¨ç›¸åŒçš„é”šç‚¹ï¼‰
    result_anchors: List[Tuple[BatchPageResult, str]] = []
    for result in results:
        if result.success:
            anchor = anchor_manager.register(result.title, result.url)
        else:
            anchor = ""  # å¤±è´¥çš„ç»“æœä¸éœ€è¦é”šç‚¹
        result_anchors.append((result, anchor))
    
    # æ„å»º URL åˆ°é”šç‚¹çš„æ˜ å°„ï¼ˆç”¨äºé“¾æ¥æ”¹å†™ï¼‰- ä½¿ç”¨å»é‡åçš„é”šç‚¹
    url_to_anchor: Dict[str, str] = {}
    total_rewrite_count = 0
    if rewrite_links:
        url_to_anchor = build_url_to_anchor_map_with_manager(results, result_anchors)
    
    # ç”Ÿæˆ frontmatterï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ YAML è½¬ä¹‰ï¼‰
    date_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    title = main_title or "æ‰¹é‡å¯¼å‡ºæ–‡æ¡£"
    safe_title = yaml_escape_str(title)
    safe_source_url = redact_url(source_url) if (redact_urls and source_url) else source_url
    if safe_source_url:
        safe_source_url = yaml_escape_str(safe_source_url)
    parts.append("---")
    parts.append(f'title: "{safe_title}"')
    if safe_source_url:
        parts.append(f'source: "{safe_source_url}"')
    parts.append(f'date: "{date_str}"')
    parts.append(f'pages: {len([r for r in results if r.success])}')
    parts.append("---")
    parts.append("")
    
    # ä¸»æ ‡é¢˜
    parts.append(f"# {title}")
    parts.append("")
    
    # æ¥æºä¿¡æ¯æ±‡æ€»ï¼ˆPhase 4ï¼‰
    if show_source_summary:
        success_results = [r for r in results if r.success]
        if success_results:
            parts.append("## æ–‡æ¡£ä¿¡æ¯")
            parts.append("")
            parts.append(f"- **å¯¼å‡ºæ—¶é—´**ï¼š{date_str}")
            parts.append(f"- **é¡µé¢æ•°é‡**ï¼š{len(success_results)} é¡µ")
            if safe_source_url:
                parts.append(f"- **æ¥æºç«™ç‚¹**ï¼š{safe_source_url}")
            else:
                # ä»ç¬¬ä¸€ä¸ª URL æå–åŸŸå
                first_url = success_results[0].url
                parsed = urlparse(first_url)
                parts.append(f"- **æ¥æºç«™ç‚¹**ï¼š{parsed.scheme}://{parsed.netloc}")
            parts.append("")
            parts.append("---")
            parts.append("")
    
    # ç”Ÿæˆç›®å½•ï¼ˆä½¿ç”¨é¢„å…ˆæ³¨å†Œçš„å»é‡é”šç‚¹ï¼Œè½¬ä¹‰ Markdown ç‰¹æ®Šå­—ç¬¦ï¼‰
    if include_toc:
        parts.append("## ç›®å½•")
        parts.append("")
        for i, (result, anchor) in enumerate(result_anchors, 1):
            safe_link_title = escape_markdown_link_text(result.title)
            if result.success:
                parts.append(f"{i}. [{safe_link_title}](#{anchor})")
            else:
                parts.append(f"{i}. ~~{safe_link_title}~~ (è·å–å¤±è´¥)")
        parts.append("")
        parts.append("---")
        parts.append("")
    
    # æ·»åŠ å„é¡µé¢å†…å®¹ï¼ˆä½¿ç”¨é¢„å…ˆæ³¨å†Œçš„å»é‡é”šç‚¹ï¼‰
    for result, anchor in result_anchors:
        if not result.success:
            parts.append(f"## {result.title}")
            parts.append("")
            parts.append(f"> âš ï¸ è·å–å¤±è´¥ï¼š{result.error}")
            parts.append("")
            fail_url = redact_url(result.url) if redact_urls else result.url
            parts.append(f"- åŸå§‹é“¾æ¥ï¼š{fail_url}")
            parts.append("")
            parts.append("---")
            parts.append("")
            continue
        
        # é¡µé¢æ ‡é¢˜ï¼ˆä½¿ç”¨ HTML h2 æ ‡ç­¾å¸¦ id å±æ€§ï¼Œç¡®ä¿é”šç‚¹è·³è½¬å…¼å®¹æ€§ï¼‰
        # è½¬ä¹‰æ ‡é¢˜ä¸­çš„ HTML ç‰¹æ®Šå­—ç¬¦ï¼Œé¿å…æ³¨å…¥é£é™©å’Œæ¸²æŸ“é”™è¯¯
        safe_html_title = htmllib.escape(result.title)
        parts.append(f'<h2 id="{anchor}">{safe_html_title}</h2>')
        parts.append("")
        page_url = redact_url(result.url) if redact_urls else result.url
        parts.append(f"- æ¥æºï¼š{page_url}")
        parts.append("")
        
        # é¡µé¢å†…å®¹ï¼ˆè°ƒæ•´æ ‡é¢˜çº§åˆ«ï¼š# -> ###, ## -> ####, etc.ï¼‰
        content = result.md_content
        # å°†åŸå†…å®¹ä¸­çš„æ ‡é¢˜çº§åˆ«ä¸‹è°ƒä¸¤çº§
        content = re.sub(r"^(#{1,4})\s+", lambda m: "#" * (len(m.group(1)) + 2) + " ", content, flags=re.MULTILINE)
        
        # ç«™å†…é“¾æ¥æ”¹å†™ï¼ˆPhase 3ï¼‰
        if rewrite_links and url_to_anchor:
            content, count = rewrite_internal_links(content, url_to_anchor)
            total_rewrite_count += count

        if redact_urls:
            content = redact_urls_in_markdown(content)
        
        parts.append(content)
        parts.append("")
        parts.append("---")
        parts.append("")
    
    # å¦‚æœå¯ç”¨äº†é“¾æ¥æ”¹å†™ï¼Œåœ¨æ–‡æ¡£æœ«å°¾æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
    if rewrite_links and total_rewrite_count > 0:
        parts.append("")
        parts.append(f"<!-- ç«™å†…é“¾æ¥æ”¹å†™ï¼šå…± {total_rewrite_count} å¤„ -->")
    
    # è·å–é”šç‚¹å†²çªç»Ÿè®¡
    anchor_stats = anchor_manager.get_stats()
    
    return "\n".join(parts), anchor_stats


def generate_index_markdown(
    results: List[BatchPageResult],
    output_dir: str,
    main_title: Optional[str] = None,
    source_url: Optional[str] = None,
    saved_files: Optional[List[str]] = None,
    redact_urls: bool = True,
) -> str:
    """
    ç”Ÿæˆç´¢å¼•æ–‡ä»¶å†…å®¹ï¼ˆPhase 3-B2 å¢å¼ºç‰ˆï¼‰
    
    Args:
        results: å¤„ç†ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        main_title: ä¸»æ ‡é¢˜
        source_url: æ¥æºç«™ç‚¹ URL
        saved_files: å·²ä¿å­˜çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç”¨äºå‡†ç¡®é“¾æ¥ï¼‰
        redact_urls: æ˜¯å¦è„±æ• URL
    """
    parts: List[str] = []
    
    title = main_title or "æ‰¹é‡å¯¼å‡ºç´¢å¼•"
    date_str = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    # YAML Frontmatterï¼ˆä½¿ç”¨ç»Ÿä¸€çš„ YAML è½¬ä¹‰ï¼‰
    safe_title = yaml_escape_str(title)
    safe_source_url = redact_url(source_url) if (redact_urls and source_url) else source_url
    if safe_source_url:
        safe_source_url = yaml_escape_str(safe_source_url)
    
    parts.append("---")
    parts.append(f'title: "{safe_title}"')
    if safe_source_url:
        parts.append(f'source: "{safe_source_url}"')
    parts.append(f'date: "{date_str}"')
    success_count = len([r for r in results if r.success])
    parts.append(f'pages: {success_count}')
    parts.append("---")
    parts.append("")
    
    # ä¸»æ ‡é¢˜
    parts.append(f"# {title}")
    parts.append("")
    
    # æ–‡æ¡£ä¿¡æ¯
    parts.append("## æ–‡æ¡£ä¿¡æ¯")
    parts.append("")
    parts.append(f"- **å¯¼å‡ºæ—¶é—´**ï¼š{date_str}")
    parts.append(f"- **é¡µé¢æ•°é‡**ï¼š{success_count} é¡µ")
    if safe_source_url:
        parts.append(f"- **æ¥æºç«™ç‚¹**ï¼š{safe_source_url}")
    elif results and results[0].url:
        # ä»ç¬¬ä¸€ä¸ª URL æå–åŸŸå
        parsed = urlparse(results[0].url)
        parts.append(f"- **æ¥æºç«™ç‚¹**ï¼š{parsed.scheme}://{parsed.netloc}")
    parts.append("")
    parts.append("---")
    parts.append("")
    
    # é¡µé¢åˆ—è¡¨
    parts.append("## é¡µé¢åˆ—è¡¨")
    parts.append("")
    
    # æ„å»ºæ–‡ä»¶åæ˜ å°„ï¼ˆå¦‚æœæä¾›äº† saved_filesï¼‰
    # saved_files æŒ‰é¡ºåºå¯¹åº” results ä¸­æˆåŠŸçš„é¡¹ç›®
    filename_map: Dict[int, str] = {}
    if saved_files:
        saved_idx = 0
        for i, r in enumerate(results):
            if r.success and saved_idx < len(saved_files):
                filename_map[i] = os.path.basename(saved_files[saved_idx])
                saved_idx += 1
    
    for i, result in enumerate(results, 1):
        # è½¬ä¹‰æ ‡é¢˜ä¸­çš„ Markdown ç‰¹æ®Šå­—ç¬¦
        safe_link_title = escape_markdown_link_text(result.title)
        if result.success:
            # ä¼˜å…ˆä½¿ç”¨å®é™…ç”Ÿæˆçš„æ–‡ä»¶å
            if (i - 1) in filename_map:
                filename = filename_map[i - 1]
            else:
                filename = _sanitize_filename_part(result.title)[:50] + ".md"
            parts.append(f"{i}. [{safe_link_title}](./{filename})")
        else:
            parts.append(f"{i}. ~~{safe_link_title}~~ (è·å–å¤±è´¥: {result.error})")
    
    parts.append("")
    return "\n".join(parts)


def batch_save_individual(
    results: List[BatchPageResult],
    output_dir: str,
    include_frontmatter: bool = True,
    redact_urls: bool = True,
    shared_assets_dir: Optional[str] = None,
) -> List[str]:
    """
    å°†ç»“æœä¿å­˜ä¸ºç‹¬ç«‹çš„ MD æ–‡ä»¶ï¼ˆPhase 3-B2 å¢å¼ºç‰ˆï¼‰
    
    Args:
        results: å¤„ç†ç»“æœåˆ—è¡¨
        output_dir: è¾“å‡ºç›®å½•
        include_frontmatter: æ˜¯å¦åŒ…å« frontmatter
        redact_urls: æ˜¯å¦è„±æ• URL
        shared_assets_dir: å…±äº« assets ç›®å½•ï¼ˆç”¨äºåŒç‰ˆæœ¬è¾“å‡ºæ—¶è°ƒæ•´å›¾ç‰‡è·¯å¾„ï¼‰
    
    Returns:
        ç”Ÿæˆçš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    """
    os.makedirs(output_dir, exist_ok=True)
    saved_files: List[str] = []
    
    for result in results:
        if not result.success:
            continue
        
        # ç”Ÿæˆæ–‡ä»¶å
        filename = _sanitize_filename_part(result.title)[:50]
        filename = _safe_path_length(output_dir, filename + ".md")
        filepath = os.path.join(output_dir, filename)
        
        # é¿å…é‡å
        base, ext = os.path.splitext(filepath)
        counter = 1
        while os.path.exists(filepath):
            filepath = f"{base}_{counter}{ext}"
            counter += 1
        
        # å¤„ç†å†…å®¹ä¸­çš„å›¾ç‰‡è·¯å¾„
        content = result.md_content
        if shared_assets_dir:
            # è®¡ç®—ä» output_dir åˆ° shared_assets_dir çš„ç›¸å¯¹è·¯å¾„
            try:
                rel_assets_path = os.path.relpath(shared_assets_dir, output_dir)
                # ç»Ÿä¸€ä½¿ç”¨æ­£æ–œæ ï¼ˆWindows ä¸Š relpath å¯èƒ½è¿”å›åæ–œæ ï¼‰
                rel_assets_path = rel_assets_path.replace("\\", "/")
                # æ›¿æ¢å›¾ç‰‡è·¯å¾„ï¼šå°† xxx.assets/ æ›¿æ¢ä¸ºç›¸å¯¹è·¯å¾„
                # åŒ¹é… ![...](xxx.assets/...) æˆ– <img src="xxx.assets/..."
                content = re.sub(
                    r'(\!\[[^\]]*\]\()([^/)]+\.assets/)([^)]+\))',
                    lambda m: m.group(1) + rel_assets_path + '/' + m.group(3),
                    content
                )
                content = re.sub(
                    r'(<img[^>]+src=["\'])([^"\'/]+\.assets/)([^"\']+)',
                    lambda m: m.group(1) + rel_assets_path + '/' + m.group(3),
                    content
                )
            except ValueError:
                # å¦‚æœæ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼ˆè·¨é©±åŠ¨å™¨ç­‰ï¼‰ï¼Œä¿æŒåŸæ ·
                pass

        if redact_urls:
            content = redact_urls_in_markdown(content)
        
        # å†™å…¥æ–‡ä»¶
        with open(filepath, "w", encoding="utf-8") as f:
            page_url = redact_url(result.url) if redact_urls else result.url
            if include_frontmatter:
                f.write(generate_frontmatter(result.title, page_url))
            f.write(f"# {result.title}\n\n")
            f.write(f"- Source: {page_url}\n\n")
            f.write(content)
        
        saved_files.append(filepath)
    
    return saved_files


def fetch_html(
    session: requests.Session,
    url: str,
    timeout_s: int,
    retries: int,
    *,
    max_html_bytes: int = _DEFAULT_MAX_HTML_BYTES,
) -> str:
    last_err: Optional[Exception] = None
    max_bytes: Optional[int] = max_html_bytes if (max_html_bytes and max_html_bytes > 0) else None
    for attempt in range(1, retries + 1):
        r: Optional[requests.Response] = None
        try:
            r = session.get(
                url,
                timeout=timeout_s,
                stream=True,
                headers={
                    "Connection": "close",
                    "Accept-Encoding": "identity",
                },
            )
            r.raise_for_status()

            if max_bytes is not None:
                cl = r.headers.get("Content-Length")
                if cl:
                    try:
                        if int(cl) > max_bytes:
                            raise RuntimeError(f"HTML å“åº”è¿‡å¤§ï¼ˆContent-Length={cl} > {max_bytes} bytesï¼‰ï¼š{url}")
                    except ValueError:
                        pass

            buf = bytearray()
            for chunk in r.iter_content(chunk_size=1024 * 128):
                if not chunk:
                    continue
                buf.extend(chunk)
                if max_bytes is not None and len(buf) > max_bytes:
                    raise RuntimeError(f"HTML å“åº”è¿‡å¤§ï¼ˆ>{max_bytes} bytesï¼‰ï¼š{url}")

            encoding = r.encoding or "utf-8"
            return bytes(buf).decode(encoding, errors="replace")
        except Exception as e:  # noqa: BLE001 - CLI tool wants retries on network errors
            last_err = e
            if attempt >= retries:
                raise
            time.sleep(min(3.0, 0.6 * attempt))
        finally:
            if r is not None:
                try:
                    r.close()
                except Exception:
                    pass
    raise last_err or RuntimeError("fetch failed")


def _parse_cookies_file(filepath: str) -> Dict[str, str]:
    """è§£æ Netscape æ ¼å¼çš„ cookies.txt æ–‡ä»¶ã€‚"""
    cookies: Dict[str, str] = {}
    with open(filepath, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line or line.startswith("#"):
                continue
            parts = line.split("\t")
            if len(parts) >= 7:
                # Netscape æ ¼å¼: domain, flag, path, secure, expiry, name, value
                name, value = parts[5], parts[6]
                cookies[name] = value
    return cookies


def _parse_cookie_string(cookie_str: str) -> Dict[str, str]:
    """è§£æ Cookie å­—ç¬¦ä¸²ï¼Œå¦‚ 'session=abc; token=xyz'ã€‚"""
    cookies: Dict[str, str] = {}
    for part in cookie_str.split(";"):
        part = part.strip()
        if "=" in part:
            name, value = part.split("=", 1)
            cookies[name.strip()] = value.strip()
    return cookies


def _apply_header_lines(headers: Dict[str, str], header_lines: Sequence[str]) -> None:
    for h in header_lines:
        if not h:
            continue
        if ":" not in h:
            raise ValueError(f"--header æ ¼å¼åº”ä¸º 'Key: Value'ï¼Œæ”¶åˆ°ï¼š{h!r}")
        k, v = h.split(":", 1)
        k = k.strip()
        v = v.strip()
        if not k:
            raise ValueError(f"--header Key ä¸èƒ½ä¸ºç©ºï¼š{h!r}")
        headers[k] = v


def _create_session(args: argparse.Namespace, referer_url: Optional[str] = None) -> requests.Session:
    """åˆ›å»ºå¹¶é…ç½® requests.Session"""
    session = requests.Session()
    session.headers.update(
        {
            "User-Agent": _resolve_user_agent(args.user_agent, args.ua_preset),
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9,zh-CN;q=0.8,zh;q=0.7",
        }
    )
    if referer_url:
        session.headers.setdefault("Referer", referer_url)

    # å¤„ç† Cookie
    if args.cookies_file:
        try:
            cookies = _parse_cookies_file(args.cookies_file)
            session.cookies.update(cookies)
            print(f"å·²åŠ è½½ Cookie æ–‡ä»¶ï¼š{args.cookies_file}ï¼ˆ{len(cookies)} ä¸ª cookieï¼‰")
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•è§£æ cookies æ–‡ä»¶ï¼š{e}", file=sys.stderr)
    if args.cookie:
        cookies = _parse_cookie_string(args.cookie)
        session.cookies.update(cookies)
        print(f"å·²åŠ è½½ Cookie å­—ç¬¦ä¸²ï¼ˆ{len(cookies)} ä¸ª cookieï¼‰")

    # å¤„ç†è‡ªå®šä¹‰ Header
    if args.headers:
        try:
            custom_headers = json.loads(args.headers)
            if not isinstance(custom_headers, dict):
                raise ValueError("headers JSON å¿…é¡»æ˜¯å¯¹è±¡ï¼ˆå¦‚ {\"Authorization\": \"Bearer xxx\"}ï¼‰")
            sanitized: Dict[str, str] = {}
            for k, v in custom_headers.items():
                kk = str(k).strip()
                if not kk:
                    continue
                sanitized[kk] = str(v)
            session.headers.update(sanitized)
            print(f"å·²åŠ è½½è‡ªå®šä¹‰ Headerï¼ˆ{len(sanitized)} ä¸ªï¼‰")
        except Exception as e:  # noqa: BLE001
            print(f"è­¦å‘Šï¼šæ— æ³•è§£æ/åº”ç”¨ headers JSONï¼š{e}", file=sys.stderr)

    if args.header:
        try:
            _apply_header_lines(session.headers, args.header)
            print(f"å·²åŠ è½½è¿½åŠ  Headerï¼ˆ{len(args.header)} ä¸ªï¼‰")
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•è§£æ --headerï¼š{e}", file=sys.stderr)

    return session


def _batch_main(args: argparse.Namespace) -> int:
    """æ‰¹é‡å¤„ç†æ¨¡å¼çš„ä¸»å‡½æ•°"""
    
    # åˆ›å»º Session
    session = _create_session(args, referer_url=args.url)
    
    # æ”¶é›†è¦å¤„ç†çš„ URL åˆ—è¡¨
    urls: List[Tuple[str, Optional[str]]] = []
    source_url: Optional[str] = None
    
    if args.urls_file:
        # ä»æ–‡ä»¶è¯»å– URL
        if not os.path.isfile(args.urls_file):
            print(f"é”™è¯¯ï¼šURL åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼š{args.urls_file}", file=sys.stderr)
            return EXIT_ERROR
        urls = read_urls_file(args.urls_file)
        print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(urls)} ä¸ª URL")
    
    if args.crawl:
        # ä»ç´¢å¼•é¡µçˆ¬å–é“¾æ¥
        if not args.url:
            print("é”™è¯¯ï¼šçˆ¬å–æ¨¡å¼éœ€è¦æä¾›ç´¢å¼•é¡µ URL", file=sys.stderr)
            return EXIT_ERROR
        
        source_url = args.url
        print(f"æ­£åœ¨ä»ç´¢å¼•é¡µæå–é“¾æ¥ï¼š{args.url}")
        
        try:
            index_html = fetch_html(
                session=session,
                url=args.url,
                timeout_s=args.timeout,
                retries=args.retries,
                max_html_bytes=args.max_html_bytes,
            )
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•è·å–ç´¢å¼•é¡µï¼š{e}", file=sys.stderr)
            return EXIT_ERROR
        
        # æå–é“¾æ¥
        links = extract_links_from_html(
            html=index_html,
            base_url=args.url,
            pattern=args.crawl_pattern,
            same_domain=args.same_domain,
        )
        
        # æ·»åŠ åˆ° URL åˆ—è¡¨ï¼ˆé¿å…é‡å¤ï¼‰
        existing_urls = {u for u, _ in urls}
        for link_url, link_text in links:
            if link_url not in existing_urls:
                urls.append((link_url, link_text))
                existing_urls.add(link_url)
        
        print(f"ä»ç´¢å¼•é¡µæå–äº† {len(links)} ä¸ªé“¾æ¥ï¼Œæ€»è®¡ {len(urls)} ä¸ª URL")
    
    if not urls:
        print("é”™è¯¯ï¼šæ²¡æœ‰è¦å¤„ç†çš„ URL", file=sys.stderr)
        return EXIT_ERROR
    
    # æ˜¾ç¤º URL åˆ—è¡¨é¢„è§ˆ
    print("\nå³å°†å¤„ç†çš„ URL åˆ—è¡¨ï¼š")
    for i, (url, title) in enumerate(urls[:10], 1):
        display = f"  {i}. {title or url}"
        if len(display) > 80:
            display = display[:77] + "..."
        print(display)
    if len(urls) > 10:
        print(f"  ... å…± {len(urls)} ä¸ª")
    print()
    
    # é…ç½®æ‰¹é‡å¤„ç†
    config = BatchConfig(
        max_workers=args.max_workers,
        delay=args.delay,
        skip_errors=args.skip_errors,
        timeout=args.timeout,
        retries=args.retries,
        max_html_bytes=args.max_html_bytes,
        best_effort_images=args.best_effort_images,  # Bug fix: ä½¿ç”¨ç”¨æˆ·å‚æ•°è€Œéç¡¬ç¼–ç 
        keep_html=args.keep_html,
        target_id=args.target_id,
        target_class=args.target_class,
        clean_wiki_noise=args.clean_wiki_noise,
        download_images=args.download_images,
        wechat=args.wechat,
        # Phase 1: å¯¼èˆªå‰¥ç¦»å‚æ•°
        strip_nav=args.strip_nav,
        strip_page_toc=args.strip_page_toc,
        exclude_selectors=args.exclude_selectors,
        anchor_list_threshold=args.anchor_list_threshold,
        # Phase 2: æ™ºèƒ½æ­£æ–‡å®šä½å‚æ•°
        docs_preset=args.docs_preset,
        auto_detect=args.auto_detect,
    )
    
    # Phase 2: åº”ç”¨æ–‡æ¡£æ¡†æ¶é¢„è®¾
    if args.docs_preset:
        preset = DOCS_PRESETS.get(args.docs_preset)
        if preset:
            print(f"\nğŸ“¦ ä½¿ç”¨æ–‡æ¡£æ¡†æ¶é¢„è®¾ï¼š{preset.name} ({preset.description})")
            # åº”ç”¨é¢„è®¾çš„ target é…ç½®
            if not config.target_id and preset.target_ids:
                config.target_id = ",".join(preset.target_ids)
            if not config.target_class and preset.target_classes:
                config.target_class = ",".join(preset.target_classes)
            # åˆå¹¶é¢„è®¾çš„ exclude_selectors
            preset_excludes = ",".join(preset.exclude_selectors)
            if config.exclude_selectors:
                config.exclude_selectors = f"{config.exclude_selectors},{preset_excludes}"
            else:
                config.exclude_selectors = preset_excludes
            # è‡ªåŠ¨å¯ç”¨å¯¼èˆªå‰¥ç¦»
            config.strip_nav = True
            config.strip_page_toc = True
            # é¢„è®¾æ¨¡å¼ä¸‹ï¼Œå¦‚æœç”¨æˆ·æœªæ˜¾å¼è®¾ç½® anchor_list_thresholdï¼Œåˆ™è‡ªåŠ¨å¯ç”¨ï¼ˆé»˜è®¤ 10ï¼‰
            if args.anchor_list_threshold == 0:
                config.anchor_list_threshold = 10
            print(f"  â€¢ æ­£æ–‡å®¹å™¨ IDï¼š{config.target_id or '(æœªè®¾ç½®)'}")
            print(f"  â€¢ æ­£æ–‡å®¹å™¨ classï¼š{config.target_class or '(æœªè®¾ç½®)'}")
            print(f"  â€¢ æ’é™¤é€‰æ‹©å™¨ï¼š{len(preset.exclude_selectors)} ä¸ª")
            if config.anchor_list_threshold > 0:
                print(f"  â€¢ é”šç‚¹åˆ—è¡¨é˜ˆå€¼ï¼š{config.anchor_list_threshold} è¡Œ")
    
    # Phase 1: æ‰“å°å¯¼èˆªå‰¥ç¦»é…ç½®
    if args.strip_nav or args.strip_page_toc or args.exclude_selectors:
        selectors = get_strip_selectors(args.strip_nav, args.strip_page_toc, args.exclude_selectors)
        print(f"å¯ç”¨å¯¼èˆªå‰¥ç¦»ï¼š{len(selectors)} ä¸ªé€‰æ‹©å™¨")
        if args.anchor_list_threshold > 0:
            print(f"é”šç‚¹åˆ—è¡¨ç§»é™¤é˜ˆå€¼ï¼š{args.anchor_list_threshold} è¡Œ")
    
    # è¿›åº¦å›è°ƒ
    def progress_callback(current: int, total: int, url: str) -> None:
        short_url = url if len(url) <= 50 else url[:47] + "..."
        print(f"[{current}/{total}] å¤„ç†ä¸­ï¼š{short_url}")
    
    # æ‰§è¡Œæ‰¹é‡å¤„ç†
    print(f"å¼€å§‹æ‰¹é‡å¤„ç†ï¼ˆå¹¶å‘æ•°ï¼š{config.max_workers}ï¼Œé—´éš”ï¼š{config.delay}sï¼‰...\n")
    
    try:
        results = batch_process_urls(
            session=session,
            urls=urls,
            config=config,
            progress_callback=progress_callback,
        )
    except RuntimeError as e:
        print(f"\né”™è¯¯ï¼š{e}", file=sys.stderr)
        return EXIT_ERROR
    
    # ç»Ÿè®¡ç»“æœ
    success_count = len([r for r in results if r.success])
    fail_count = len(results) - success_count
    print(f"\nå¤„ç†å®Œæˆï¼šæˆåŠŸ {success_count}ï¼Œå¤±è´¥ {fail_count}")
    
    # Phase 1: å¯¼èˆªå‰¥ç¦»ç»Ÿè®¡ï¼ˆT1.5 å¯è§‚æµ‹æ€§ï¼‰
    if args.strip_nav or args.strip_page_toc or args.exclude_selectors:
        selectors = get_strip_selectors(args.strip_nav, args.strip_page_toc, args.exclude_selectors)
        print(f"\nğŸ“Š å¯¼èˆªå‰¥ç¦»å·²ç”Ÿæ•ˆï¼š")
        print(f"  â€¢ åº”ç”¨é€‰æ‹©å™¨ï¼š{len(selectors)} ä¸ª")
        if args.strip_nav:
            print(f"  â€¢ --strip-nav: ç§»é™¤å¯¼èˆªå…ƒç´ ï¼ˆnav/aside/.sidebar ç­‰ï¼‰")
        if args.strip_page_toc:
            print(f"  â€¢ --strip-page-toc: ç§»é™¤é¡µå†…ç›®å½•ï¼ˆ.toc/.on-this-page ç­‰ï¼‰")
        if args.exclude_selectors:
            print(f"  â€¢ --exclude-selectors: {args.exclude_selectors}")
        if args.anchor_list_threshold > 0:
            print(f"  â€¢ é”šç‚¹åˆ—è¡¨é˜ˆå€¼ï¼š>{args.anchor_list_threshold} è¡Œè‡ªåŠ¨ç§»é™¤")
    
    # ä¸‹è½½å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    url_to_local: Dict[str, str] = {}
    if args.download_images:
        # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
        total_images = sum(len(r.image_urls) for r in results if r.success)
        unique_images = len(set(url for r in results if r.success for url in r.image_urls))
        
        if unique_images > 0:
            # ç¡®å®š assets ç›®å½•
            if args.merge:
                output_file = args.merge_output or "merged.md"
                # è‡ªåŠ¨åˆ›å»ºåŒåä¸Šçº§ç›®å½•ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šç›®å½•ï¼‰
                output_file = auto_wrap_output_dir(output_file)
                assets_dir = os.path.splitext(output_file)[0] + ".assets"
                md_dir = os.path.dirname(output_file) or "."
            else:
                assets_dir = os.path.join(args.output_dir, "assets")
                md_dir = args.output_dir
            
            print(f"\nå‘ç° {unique_images} å¼ å›¾ç‰‡ï¼ˆå»é‡åï¼‰ï¼Œå¼€å§‹ä¸‹è½½åˆ°ï¼š{assets_dir}")
            
            def img_progress(current: int, total: int, url: str) -> None:
                short_url = url if len(url) <= 50 else url[:47] + "..."
                print(f"  [{current}/{total}] ä¸‹è½½ï¼š{short_url}")
            
            url_to_local = batch_download_images(
                session=session,
                results=results,
                assets_dir=assets_dir,
                md_dir=md_dir,
                timeout_s=args.timeout,
                retries=args.retries,
                best_effort=bool(args.best_effort_images),
                progress_callback=img_progress,
                redact_urls=args.redact_url,
                max_image_bytes=args.max_image_bytes,
            )
            
            print(f"  å›¾ç‰‡ä¸‹è½½å®Œæˆï¼š{len(url_to_local)} å¼ æˆåŠŸ")
            
            # æ›´æ–°ç»“æœä¸­çš„ Markdown å†…å®¹ï¼Œæ›¿æ¢å›¾ç‰‡ URL
            for result in results:
                if result.success and result.md_content:
                    result.md_content = replace_image_urls_in_markdown(
                        result.md_content, url_to_local
                    )
        else:
            print("\næœªå‘ç°éœ€è¦ä¸‹è½½çš„å›¾ç‰‡")
    
    # è¾“å‡ºç»“æœ
    if args.merge:
        # åˆå¹¶è¾“å‡ºæ¨¡å¼
        output_file = args.merge_output or "merged.md"
        # è‡ªåŠ¨åˆ›å»ºåŒåä¸Šçº§ç›®å½•ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šç›®å½•ï¼‰
        output_file = auto_wrap_output_dir(output_file)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_file) and not args.overwrite:
            print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼š{output_file}ï¼ˆå¦‚éœ€è¦†ç›–è¯·åŠ  --overwriteï¼‰", file=sys.stderr)
            return EXIT_FILE_EXISTS
        
        # æ¥æº URL ä¼˜å…ˆçº§ï¼š--source-url > çˆ¬å–æ¨¡å¼çš„ç´¢å¼•é¡µ > Noneï¼ˆæå–åŸŸåï¼‰
        final_source_url = args.source_url or source_url
        
        merged_content, anchor_stats = generate_merged_markdown(
            results=results,
            include_toc=args.toc,
            main_title=args.merge_title or args.title,
            source_url=final_source_url,
            rewrite_links=args.rewrite_links,
            show_source_summary=not args.no_source_summary,
            redact_urls=args.redact_url,
        )
        
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(merged_content)
        
        print(f"\nå·²ç”Ÿæˆåˆå¹¶æ–‡æ¡£ï¼š{output_file}")
        print(f"æ–‡æ¡£å¤§å°ï¼š{len(merged_content):,} å­—ç¬¦")
        
        # Phase 3-A: è¾“å‡ºé”šç‚¹å†²çªç»Ÿè®¡
        if anchor_stats.has_collisions:
            if hasattr(args, 'warn_anchor_collisions') and args.warn_anchor_collisions:
                anchor_stats.print_summary()
            else:
                print(f"ğŸ“Œ é”šç‚¹å†²çªï¼š{anchor_stats.collision_count} ä¸ªå·²è‡ªåŠ¨ä¿®å¤ï¼ˆä½¿ç”¨ --warn-anchor-collisions æŸ¥çœ‹è¯¦æƒ…ï¼‰")
        if url_to_local:
            assets_dir = os.path.splitext(output_file)[0] + ".assets"
            # ç»Ÿè®¡å›¾ç‰‡å¼•ç”¨æƒ…å†µï¼ˆéç ´åæ€§ï¼šåªæŠ¥å‘Šä¸åˆ é™¤ï¼‰
            if os.path.isdir(assets_dir):
                # ç»Ÿè®¡å®é™…æ–‡ä»¶æ•°
                all_files = [f for f in os.listdir(assets_dir) if os.path.isfile(os.path.join(assets_dir, f))]
                actual_count = len(all_files)
                
                # ç»Ÿè®¡è¢«å¼•ç”¨çš„æ–‡ä»¶ï¼ˆä¿å®ˆæ£€æµ‹ï¼šä½¿ç”¨æ–‡ä»¶ååŒ¹é…ï¼‰
                unused_files = []
                for filename in all_files:
                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åœ¨æœ€ç»ˆå†…å®¹ä¸­å‡ºç°
                    if filename not in merged_content:
                        unused_files.append(filename)
                
                unused_count = len(unused_files)
                if unused_count > 0:
                    print(f"å›¾ç‰‡ç›®å½•ï¼š{assets_dir}ï¼ˆ{actual_count} å¼ å›¾ç‰‡ï¼Œ{unused_count} å¼ å¯èƒ½æœªå¼•ç”¨ï¼‰")
                    print(f"  âš ï¸ æœªè‡ªåŠ¨æ¸…ç†æœªå¼•ç”¨å›¾ç‰‡ï¼ˆå¯èƒ½å­˜åœ¨è¯¯åˆ¤ï¼‰ï¼Œå¦‚éœ€æ¸…ç†è¯·æ‰‹åŠ¨æ£€æŸ¥")
                else:
                    print(f"å›¾ç‰‡ç›®å½•ï¼š{assets_dir}ï¼ˆ{actual_count} å¼ å›¾ç‰‡ï¼‰")
            else:
                print(f"å›¾ç‰‡ç›®å½•ï¼š{assets_dir}ï¼ˆ{len(url_to_local)} å¼ å›¾ç‰‡ï¼‰")
        
        # Phase 3-B1: åŒç‰ˆæœ¬è¾“å‡ºï¼ˆåŒæ—¶ç”Ÿæˆåˆ†æ–‡ä»¶ç‰ˆæœ¬ï¼‰
        if hasattr(args, 'split_output') and args.split_output:
            split_dir = args.split_output
            os.makedirs(split_dir, exist_ok=True)
            
            # ç¡®å®šå…±äº«çš„ assets ç›®å½•ï¼ˆä½¿ç”¨åˆå¹¶ç‰ˆæœ¬çš„ assetsï¼‰
            shared_assets = os.path.splitext(output_file)[0] + ".assets" if url_to_local else None
            
            # ç”Ÿæˆåˆ†æ–‡ä»¶
            saved_files = batch_save_individual(
                results=results,
                output_dir=split_dir,
                include_frontmatter=args.frontmatter,
                redact_urls=args.redact_url,
                shared_assets_dir=shared_assets,
            )
            
            # ç”Ÿæˆç´¢å¼•æ–‡ä»¶
            index_content = generate_index_markdown(
                results=results,
                output_dir=split_dir,
                main_title=args.merge_title or args.title,
                source_url=final_source_url,
                saved_files=saved_files,
                redact_urls=args.redact_url,
            )
            index_path = os.path.join(split_dir, "INDEX.md")
            with open(index_path, "w", encoding="utf-8") as f:
                f.write(index_content)
            
            print(f"\nğŸ“‚ å·²åŒæ—¶ç”Ÿæˆåˆ†æ–‡ä»¶ç‰ˆæœ¬ï¼š")
            print(f"  â€¢ ç›®å½•ï¼š{split_dir}")
            print(f"  â€¢ æ–‡ä»¶æ•°ï¼š{len(saved_files)} ä¸ª")
            print(f"  â€¢ ç´¢å¼•ï¼š{index_path}")
            if shared_assets:
                rel_assets = os.path.relpath(shared_assets, split_dir)
                print(f"  â€¢ å…±äº« assetsï¼š{rel_assets}")
        
    else:
        # ç‹¬ç«‹æ–‡ä»¶è¾“å‡ºæ¨¡å¼
        os.makedirs(args.output_dir, exist_ok=True)
        
        saved_files = batch_save_individual(
            results=results,
            output_dir=args.output_dir,
            include_frontmatter=args.frontmatter,
            redact_urls=args.redact_url,
            shared_assets_dir=None,
        )
        
        # æ¥æº URL ä¼˜å…ˆçº§ï¼š--source-url > çˆ¬å–æ¨¡å¼çš„ç´¢å¼•é¡µ > Noneï¼ˆæå–åŸŸåï¼‰
        final_source_url = args.source_url or source_url
        
        # ç”Ÿæˆç´¢å¼•æ–‡ä»¶ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆï¼‰
        index_content = generate_index_markdown(
            results=results,
            output_dir=args.output_dir,
            main_title=args.merge_title or args.title,
            source_url=final_source_url,
            saved_files=saved_files,
            redact_urls=args.redact_url,
        )
        index_path = os.path.join(args.output_dir, "INDEX.md")
        with open(index_path, "w", encoding="utf-8") as f:
            f.write(index_content)
        
        print(f"\nå·²ç”Ÿæˆ {len(saved_files)} ä¸ªæ–‡ä»¶åˆ°ï¼š{args.output_dir}")
        print(f"ç´¢å¼•æ–‡ä»¶ï¼š{index_path}")
    
    # æ˜¾ç¤ºå¤±è´¥åˆ—è¡¨
    if fail_count > 0:
        print("\nå¤±è´¥çš„ URLï¼š")
        for result in results:
            if not result.success:
                print(f"  - {result.url}")
                print(f"    é”™è¯¯ï¼š{result.error}")
    
    return EXIT_SUCCESS


def main(argv: Optional[Sequence[str]] = None) -> int:
    ap = argparse.ArgumentParser(
        description="æŠ“å–ç½‘é¡µæ­£æ–‡ä¸å›¾ç‰‡ï¼Œä¿å­˜ä¸º Markdown + assetsã€‚æ”¯æŒå•é¡µå’Œæ‰¹é‡æ¨¡å¼ã€‚",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ‰¹é‡å¤„ç†ç¤ºä¾‹ï¼š
  # ä»æ–‡ä»¶è¯»å– URL åˆ—è¡¨ï¼Œåˆå¹¶ä¸ºå•ä¸ªæ–‡æ¡£
  python grab_web_to_md.py --urls-file urls.txt --merge --merge-output output.md

  # ä»ç´¢å¼•é¡µçˆ¬å–é“¾æ¥å¹¶æ‰¹é‡å¯¼å‡º
  python grab_web_to_md.py https://example.com/index --crawl --merge --toc

  # æ‰¹é‡å¯¼å‡ºä¸ºç‹¬ç«‹æ–‡ä»¶
  python grab_web_to_md.py --urls-file urls.txt --output-dir ./docs

urls.txt æ–‡ä»¶æ ¼å¼ï¼š
  # è¿™æ˜¯æ³¨é‡Š
  https://example.com/page1
  https://example.com/page2 | è‡ªå®šä¹‰æ ‡é¢˜
""",
    )
    ap.add_argument("url", nargs="?", help="è¦æŠ“å–çš„ç½‘é¡µ URLï¼ˆå•é¡µæ¨¡å¼å¿…éœ€ï¼Œæ‰¹é‡æ¨¡å¼å¯é€‰ï¼‰")
    ap.add_argument("--out", help="è¾“å‡º md æ–‡ä»¶åï¼ˆé»˜è®¤æ ¹æ® URL è‡ªåŠ¨ç”Ÿæˆï¼‰")
    ap.add_argument("--assets-dir", help="å›¾ç‰‡ç›®å½•åï¼ˆé»˜è®¤ <out>.assetsï¼‰")
    ap.add_argument("--title", help="Markdown é¡¶éƒ¨æ ‡é¢˜ï¼ˆé»˜è®¤ä» <title> æå–ï¼‰")
    ap.add_argument("--with-pdf", action="store_true", help="åŒæ—¶ç”ŸæˆåŒå PDFï¼ˆéœ€è¦æœ¬æœº Edge/Chromeï¼‰")
    ap.add_argument("--timeout", type=int, default=60, help="è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 60")
    ap.add_argument("--retries", type=int, default=3, help="ç½‘ç»œé‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ 3")
    ap.add_argument(
        "--max-html-bytes",
        type=int,
        default=_DEFAULT_MAX_HTML_BYTES,
        help="å•é¡µ HTML æœ€å¤§å…è®¸å­—èŠ‚æ•°ï¼ˆé»˜è®¤ 10MBï¼›è®¾ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰",
    )
    ap.add_argument("--best-effort-images", action="store_true", help="å›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ä»…è­¦å‘Šå¹¶è·³è¿‡ï¼ˆé»˜è®¤å¤±è´¥å³é€€å‡ºï¼‰")
    ap.add_argument("--overwrite", action="store_true", help="å…è®¸è¦†ç›–å·²å­˜åœ¨çš„ md æ–‡ä»¶")
    ap.add_argument("--validate", action="store_true", help="ç”Ÿæˆåæ‰§è¡Œæ ¡éªŒå¹¶è¾“å‡ºç»“æœ")
    # JS åçˆ¬å¤„ç†
    ap.add_argument("--local-html", metavar="FILE", help="ä»æœ¬åœ° HTML æ–‡ä»¶è¯»å–å†…å®¹ï¼ˆè·³è¿‡ç½‘ç»œè¯·æ±‚ï¼Œç”¨äºå¤„ç†æµè§ˆå™¨ä¿å­˜çš„é¡µé¢ï¼‰")
    ap.add_argument("--base-url", help="é…åˆ --local-html ä½¿ç”¨ï¼ŒæŒ‡å®šå›¾ç‰‡ä¸‹è½½çš„åŸºå‡† URL")
    ap.add_argument("--force", action="store_true", help="æ£€æµ‹åˆ° JS åçˆ¬æ—¶ä»å¼ºåˆ¶ç»§ç»­å¤„ç†ï¼ˆå†…å®¹å¯èƒ½ä¸ºç©ºæˆ–ä¸å®Œæ•´ï¼‰")
    ap.add_argument(
        "--max-image-bytes",
        type=int,
        default=_DEFAULT_MAX_IMAGE_BYTES,
        help="å•å¼ å›¾ç‰‡æœ€å¤§å…è®¸å­—èŠ‚æ•°ï¼ˆé»˜è®¤ 25MBï¼›è®¾ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰",
    )
    ap.add_argument(
        "--redact-url",
        dest="redact_url",
        action="store_true",
        default=True,
        help="è¾“å‡ºæ–‡ä»¶ä¸­å¯¹ URL è„±æ•ï¼ˆé»˜è®¤å¯ç”¨ï¼‰ï¼šä»…ä¿ç•™ scheme://host/pathï¼Œç§»é™¤ query/fragment",
    )
    ap.add_argument(
        "--no-redact-url",
        dest="redact_url",
        action="store_false",
        help="å…³é—­ URL è„±æ•ï¼ˆä¿ç•™å®Œæ•´ URLï¼ŒåŒ…æ‹¬ query/fragmentï¼‰",
    )
    ap.add_argument(
        "--no-map-json",
        action="store_true",
        help="ä¸ç”Ÿæˆ *.assets.json URLâ†’æœ¬åœ°æ˜ å°„æ–‡ä»¶ï¼ˆé¿å…æ³„éœ²å›¾ç‰‡ URLï¼‰",
    )
    ap.add_argument(
        "--pdf-allow-file-access",
        action="store_true",
        help="ç”Ÿæˆ PDF æ—¶å…è®¸ file:// è®¿é—®å…¶ä»–æœ¬åœ°æ–‡ä»¶ï¼ˆå¯èƒ½æœ‰å®‰å…¨é£é™©ï¼›é»˜è®¤å…³é—­ï¼‰",
    )
    # Frontmatter æ”¯æŒ
    ap.add_argument("--frontmatter", action="store_true", default=True,
                    help="ç”Ÿæˆ YAML Frontmatter å…ƒæ•°æ®å¤´ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    ap.add_argument("--no-frontmatter", action="store_false", dest="frontmatter",
                    help="ç¦ç”¨ YAML Frontmatter")
    ap.add_argument("--tags", help="Frontmatter ä¸­çš„æ ‡ç­¾ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚ 'tech,ai,tutorial'")
    # Cookie/Header æ”¯æŒ
    ap.add_argument("--cookie", help="Cookie å­—ç¬¦ä¸²ï¼Œå¦‚ 'session=abc; token=xyz'")
    ap.add_argument("--cookies-file", help="Netscape æ ¼å¼çš„ cookies.txt æ–‡ä»¶è·¯å¾„")
    ap.add_argument("--headers", help="è‡ªå®šä¹‰è¯·æ±‚å¤´ï¼ŒJSON æ ¼å¼ï¼Œå¦‚ '{\"Authorization\": \"Bearer xxx\"}'")
    ap.add_argument("--header", action="append", default=[], help="è¿½åŠ è¯·æ±‚å¤´ï¼ˆå¯é‡å¤ï¼‰ï¼Œå¦‚ 'Authorization: Bearer xxx'")
    # UA å¯é…ç½®
    ap.add_argument("--ua-preset", choices=sorted(UA_PRESETS.keys()), default="chrome-win", help="User-Agent é¢„è®¾ï¼ˆé»˜è®¤ chrome-winï¼‰")
    ap.add_argument("--user-agent", "--ua", dest="user_agent", help="è‡ªå®šä¹‰ User-Agentï¼ˆä¼˜å…ˆäº --ua-presetï¼‰")
    # å¤æ‚è¡¨æ ¼ä¿ç•™ HTML
    ap.add_argument("--keep-html", action="store_true",
                    help="å¯¹å¤æ‚è¡¨æ ¼ï¼ˆå« colspan/rowspanï¼‰ä¿ç•™åŸå§‹ HTML è€Œéå¼ºè½¬ Markdown")
    # æ‰‹åŠ¨æŒ‡å®šæ­£æ–‡åŒºåŸŸ
    ap.add_argument("--target-id", help="æ‰‹åŠ¨æŒ‡å®šæ­£æ–‡å®¹å™¨ idï¼ˆå¦‚ content / post-contentï¼‰ï¼Œä¼˜å…ˆçº§é«˜äºè‡ªåŠ¨æŠ½å–")
    ap.add_argument("--target-class", help="æ‰‹åŠ¨æŒ‡å®šæ­£æ–‡å®¹å™¨ classï¼ˆå¦‚ post-bodyï¼‰ï¼Œä¼˜å…ˆçº§é«˜äºè‡ªåŠ¨æŠ½å–")
    # SPA é¡µé¢æç¤º
    ap.add_argument("--spa-warn-len", type=int, default=500, help="æ­£æ–‡æ–‡æœ¬é•¿åº¦ä½äºè¯¥å€¼æ—¶æç¤ºå¯èƒ½ä¸º SPA åŠ¨æ€æ¸²æŸ“ï¼Œé»˜è®¤ 500ï¼›è®¾ä¸º 0 å¯å…³é—­")
    # Wiki å™ªéŸ³æ¸…ç†
    ap.add_argument("--clean-wiki-noise", action="store_true",
                    help="æ¸…ç† Wiki ç³»ç»Ÿå™ªéŸ³ï¼ˆç¼–è¾‘æŒ‰é’®ã€å¯¼èˆªé“¾æ¥ã€è¿”å›é¡¶éƒ¨ç­‰ï¼‰ï¼Œé€‚ç”¨äº PukiWiki/MediaWiki ç­‰ç«™ç‚¹")
    # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ”¯æŒ
    ap.add_argument("--wechat", action="store_true",
                    help="å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ¨¡å¼ï¼šè‡ªåŠ¨æå– rich_media_content æ­£æ–‡å¹¶æ¸…ç†äº¤äº’æŒ‰é’®å™ªéŸ³ã€‚"
                         "å¦‚ä¸æŒ‡å®šï¼Œè„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹ mp.weixin.qq.com é“¾æ¥å¹¶å¯ç”¨æ­¤æ¨¡å¼")
    
    # ========== å¯¼èˆª/ç›®å½•å‰¥ç¦»å‚æ•°ï¼ˆPhase 1ï¼‰==========
    nav_group = ap.add_argument_group("å¯¼èˆªå‰¥ç¦»å‚æ•°ï¼ˆDocs/Wiki ç«™ç‚¹ä¼˜åŒ–ï¼‰")
    nav_group.add_argument("--strip-nav", action="store_true",
                           help="ç§»é™¤å¯¼èˆªå…ƒç´ ï¼ˆnav/aside/.sidebar ç­‰ï¼‰ï¼Œé€‚ç”¨äº docs ç«™ç‚¹æ‰¹é‡å¯¼å‡º")
    nav_group.add_argument("--strip-page-toc", action="store_true",
                           help="ç§»é™¤é¡µå†…ç›®å½•ï¼ˆ.toc/.on-this-page ç­‰ï¼‰")
    nav_group.add_argument("--exclude-selectors",
                           help="è‡ªå®šä¹‰ç§»é™¤çš„å…ƒç´ é€‰æ‹©å™¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œæ”¯æŒï¼štag/.class/#id/[attr=val]")
    nav_group.add_argument("--anchor-list-threshold", type=int, default=0,
                           help="è¿ç»­é”šç‚¹åˆ—è¡¨ç§»é™¤é˜ˆå€¼ï¼ˆé»˜è®¤ 0 å…³é—­ï¼‰ï¼Œå»ºè®®ä¸ --strip-nav é…åˆä½¿ç”¨ï¼Œæ¨èå€¼ 10-20")
    
    # ========== æ™ºèƒ½æ­£æ–‡å®šä½å‚æ•°ï¼ˆPhase 2ï¼‰==========
    smart_group = ap.add_argument_group("æ™ºèƒ½æ­£æ–‡å®šä½å‚æ•°ï¼ˆPhase 2ï¼‰")
    smart_group.add_argument("--docs-preset", choices=get_available_presets(),
                             help="ä½¿ç”¨æ–‡æ¡£æ¡†æ¶é¢„è®¾ï¼ˆè‡ªåŠ¨é…ç½® target å’Œ excludeï¼‰ï¼š" + 
                                  ", ".join(get_available_presets()))
    smart_group.add_argument("--auto-detect", action="store_true",
                             help="è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ¡†æ¶å¹¶åº”ç”¨é¢„è®¾ï¼ˆé«˜ç½®ä¿¡åº¦æ—¶ï¼‰")
    smart_group.add_argument("--list-presets", action="store_true",
                             help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£æ¡†æ¶é¢„è®¾")
    
    # ========== æ‰¹é‡å¤„ç†å‚æ•° ==========
    batch_group = ap.add_argument_group("æ‰¹é‡å¤„ç†å‚æ•°")
    batch_group.add_argument("--urls-file", help="ä»æ–‡ä»¶è¯»å– URL åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œæ”¯æŒ # æ³¨é‡Šå’Œ URL|æ ‡é¢˜ æ ¼å¼ï¼‰")
    batch_group.add_argument("--output-dir", default="./batch_output", help="æ‰¹é‡è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ ./batch_outputï¼‰")
    batch_group.add_argument("--max-workers", type=int, default=3, help="å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 3ï¼Œå»ºè®®ä¸è¶…è¿‡ 5ï¼‰")
    batch_group.add_argument("--delay", type=float, default=1.0, help="è¯·æ±‚é—´éš”ç§’æ•°ï¼ˆé»˜è®¤ 1.0ï¼Œé¿å…è¢«å°ï¼‰")
    batch_group.add_argument("--skip-errors", action="store_true", help="è·³è¿‡å¤±è´¥çš„ URL ç»§ç»­å¤„ç†")
    batch_group.add_argument("--download-images", action="store_true", 
                             help="ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ° assets ç›®å½•ï¼ˆé»˜è®¤ä¸ä¸‹è½½ï¼Œä¿ç•™åŸå§‹ URLï¼‰")
    
    # åˆå¹¶è¾“å‡ºå‚æ•°
    merge_group = ap.add_argument_group("åˆå¹¶è¾“å‡ºå‚æ•°")
    merge_group.add_argument("--merge", action="store_true", help="åˆå¹¶æ‰€æœ‰é¡µé¢ä¸ºå•ä¸ª MD æ–‡ä»¶")
    merge_group.add_argument("--merge-output", help="åˆå¹¶è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤ merged.mdï¼‰")
    merge_group.add_argument("--toc", action="store_true", help="åœ¨åˆå¹¶æ–‡ä»¶å¼€å¤´ç”Ÿæˆç›®å½•")
    merge_group.add_argument("--merge-title", help="åˆå¹¶æ–‡æ¡£çš„ä¸»æ ‡é¢˜")
    merge_group.add_argument("--source-url", help="æ¥æºç«™ç‚¹ URLï¼ˆæ˜¾ç¤ºåœ¨æ–‡æ¡£ä¿¡æ¯ä¸­ï¼‰")
    merge_group.add_argument("--rewrite-links", action="store_true",
                             help="å°†ç«™å†…é“¾æ¥æ”¹å†™ä¸ºæ–‡æ¡£å†…é”šç‚¹ï¼ˆä»…åˆå¹¶æ¨¡å¼æœ‰æ•ˆï¼‰")
    merge_group.add_argument("--no-source-summary", action="store_true",
                             help="ä¸åœ¨æ–‡æ¡£å¼€å¤´æ˜¾ç¤ºæ¥æºä¿¡æ¯æ±‡æ€»")
    merge_group.add_argument("--warn-anchor-collisions", action="store_true",
                             help="æ˜¾ç¤ºé”šç‚¹å†²çªè¯¦æƒ…ï¼ˆåŒåæ ‡é¢˜è‡ªåŠ¨æ·»åŠ åç¼€ -2, -3...ï¼‰")
    merge_group.add_argument("--split-output", metavar="DIR",
                             help="åŒæ—¶è¾“å‡ºåˆ†æ–‡ä»¶ç‰ˆæœ¬åˆ°æŒ‡å®šç›®å½•ï¼ˆä¸ --merge é…åˆä½¿ç”¨ï¼Œç”ŸæˆåŒç‰ˆæœ¬ï¼‰")
    
    # çˆ¬å–æ¨¡å¼å‚æ•°
    crawl_group = ap.add_argument_group("çˆ¬å–æ¨¡å¼å‚æ•°")
    crawl_group.add_argument("--crawl", action="store_true", help="ä»ç´¢å¼•é¡µæå–é“¾æ¥å¹¶æ‰¹é‡æŠ“å–")
    crawl_group.add_argument("--crawl-pattern", help="é“¾æ¥åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¦‚ 'index\\.php\\?MMR'ï¼‰")
    crawl_group.add_argument("--same-domain", action="store_true", default=True, help="ä»…æŠ“å–åŒåŸŸåé“¾æ¥ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    crawl_group.add_argument("--no-same-domain", action="store_false", dest="same_domain", help="å…è®¸æŠ“å–è·¨åŸŸé“¾æ¥")
    
    args = ap.parse_args(argv)
    
    # ========== åˆ—å‡ºé¢„è®¾ ==========
    if args.list_presets:
        print("\nğŸ“¦ å¯ç”¨çš„æ–‡æ¡£æ¡†æ¶é¢„è®¾ï¼š\n")
        for name, preset in DOCS_PRESETS.items():
            print(f"  {name:15} - {preset.description}")
            print(f"                   æ­£æ–‡ ID: {', '.join(preset.target_ids) or '(æ— )'}")
            print(f"                   æ­£æ–‡ class: {', '.join(preset.target_classes[:3]) or '(æ— )'}{'...' if len(preset.target_classes) > 3 else ''}")
            print(f"                   æ’é™¤é€‰æ‹©å™¨: {len(preset.exclude_selectors)} ä¸ª")
            print()
        print("ä½¿ç”¨ç¤ºä¾‹ï¼špython grab_web_to_md.py URL --docs-preset mintlify")
        return EXIT_SUCCESS
    
    # ========== æ‰¹é‡å¤„ç†æ¨¡å¼ ==========
    is_batch_mode = bool(args.urls_file or args.crawl)
    
    if is_batch_mode:
        return _batch_main(args)
    
    # ========== å•é¡µå¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰ ==========
    
    # æ”¯æŒ --local-html æ¨¡å¼ï¼ˆä»æœ¬åœ°æ–‡ä»¶è¯»å–ï¼Œè·³è¿‡ç½‘ç»œè¯·æ±‚ï¼‰
    if args.local_html:
        if not os.path.isfile(args.local_html):
            print(f"é”™è¯¯ï¼šæœ¬åœ° HTML æ–‡ä»¶ä¸å­˜åœ¨ï¼š{args.local_html}", file=sys.stderr)
            return EXIT_ERROR

        # æœ¬åœ°æ–‡ä»¶åŒæ ·åšä½“ç§¯ä¿æŠ¤ï¼ˆä¸ fetch_html çš„ --max-html-bytes è¡Œä¸ºä¿æŒä¸€è‡´ï¼‰
        try:
            size = os.path.getsize(args.local_html)
            if args.max_html_bytes and args.max_html_bytes > 0 and size > args.max_html_bytes:
                print(
                    f"é”™è¯¯ï¼šæœ¬åœ° HTML æ–‡ä»¶è¿‡å¤§ï¼ˆ{size} > {args.max_html_bytes} bytesï¼‰ï¼š{args.local_html}",
                    file=sys.stderr,
                )
                return EXIT_ERROR
        except OSError:
            pass
        
        # --local-html æ¨¡å¼ä¸‹ï¼Œurl å‚æ•°å¯é€‰ï¼Œç”¨äºå›¾ç‰‡ä¸‹è½½ï¼›ä¼˜å…ˆä½¿ç”¨ --base-url
        url = args.base_url or args.url or ""
        if not url:
            print("è­¦å‘Šï¼šæœªæŒ‡å®š --base-url æˆ– urlï¼Œå›¾ç‰‡å°†æ— æ³•ä¸‹è½½ï¼ˆä»…ä¿ç•™åŸå§‹å¼•ç”¨ï¼‰", file=sys.stderr)
        
        with open(args.local_html, "r", encoding="utf-8", errors="replace") as f:
            page_html = f.read()
        print(f"ä»æœ¬åœ°æ–‡ä»¶è¯»å–ï¼š{args.local_html}")
        
        # è¾“å‡ºæ–‡ä»¶å
        if args.out:
            base = args.out
        else:
            base = os.path.splitext(os.path.basename(args.local_html))[0] + ".md"
    else:
        # ç½‘ç»œæ¨¡å¼ï¼šå¿…é¡»æä¾› URL
        if not args.url:
            ap.error("å•é¡µæ¨¡å¼å¿…é¡»æä¾› URL å‚æ•°ï¼Œæˆ–ä½¿ç”¨ --urls-file / --crawl è¿›å…¥æ‰¹é‡æ¨¡å¼ï¼Œæˆ–ä½¿ç”¨ --local-html è¯»å–æœ¬åœ°æ–‡ä»¶")
        
        url = args.url
        base = args.out or (_default_basename(url) + ".md")
    
    out_md = base
    # è‡ªåŠ¨åˆ›å»ºåŒåä¸Šçº§ç›®å½•ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šç›®å½•ï¼‰
    out_md = auto_wrap_output_dir(out_md)
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶è·¯å¾„é•¿åº¦
    md_dir = os.path.dirname(out_md) or "."
    out_md_name = os.path.basename(out_md)
    out_md_name = _safe_path_length(md_dir, out_md_name)
    out_md = os.path.join(md_dir, out_md_name) if md_dir != "." else out_md_name
    assets_dir = args.assets_dir or (os.path.splitext(out_md)[0] + ".assets")
    map_json = out_md + ".assets.json"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if md_dir != ".":
        os.makedirs(md_dir, exist_ok=True)

    if os.path.exists(out_md) and not args.overwrite:
        print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼š{out_md}ï¼ˆå¦‚éœ€è¦†ç›–è¯·åŠ  --overwriteï¼‰", file=sys.stderr)
        return EXIT_FILE_EXISTS

    session = _create_session(args, referer_url=url)

    # ç½‘ç»œæ¨¡å¼ä¸‹ä¸‹è½½é¡µé¢
    if not args.local_html:
        print(f"ä¸‹è½½é¡µé¢ï¼š{url}")
        page_html = fetch_html(
            session=session,
            url=url,
            timeout_s=args.timeout,
            retries=args.retries,
            max_html_bytes=args.max_html_bytes,
        )
        
        # ====== JS åçˆ¬æ£€æµ‹ ======
        js_detection = detect_js_challenge(page_html)
        if js_detection.is_challenge:
            print_js_challenge_warning(js_detection, url)
            if not args.force:
                return EXIT_JS_CHALLENGE
            print("å·²æ·»åŠ  --force å‚æ•°ï¼Œå¼ºåˆ¶ç»§ç»­å¤„ç†...", file=sys.stderr)

    # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« è‡ªåŠ¨æ£€æµ‹
    is_wechat = args.wechat
    if url and not is_wechat and is_wechat_article_url(url):
        is_wechat = True
        print("æ£€æµ‹åˆ°å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œè‡ªåŠ¨å¯ç”¨å¾®ä¿¡æ¨¡å¼")
    elif not is_wechat and is_wechat_article_html(page_html):
        is_wechat = True
        print("æ£€æµ‹åˆ°å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ç‰¹å¾ï¼Œè‡ªåŠ¨å¯ç”¨å¾®ä¿¡æ¨¡å¼")

    # ç¡®å®šæ­£æ–‡æå–ç­–ç•¥
    target_id = args.target_id
    target_class = args.target_class
    exclude_selectors = args.exclude_selectors
    strip_nav = args.strip_nav
    strip_page_toc = args.strip_page_toc
    anchor_list_threshold = args.anchor_list_threshold
    
    # å•é¡µæ¨¡å¼ï¼šåº”ç”¨ docs-presetï¼ˆPhase 2ï¼‰
    if hasattr(args, 'docs_preset') and args.docs_preset:
        preset = DOCS_PRESETS.get(args.docs_preset)
        if preset:
            print(f"ğŸ“¦ ä½¿ç”¨æ–‡æ¡£æ¡†æ¶é¢„è®¾ï¼š{preset.name} ({preset.description})")
            # åº”ç”¨é¢„è®¾çš„ target é…ç½®ï¼ˆä»…å½“ç”¨æˆ·æœªæŒ‡å®šæ—¶ï¼‰
            if not target_id and preset.target_ids:
                target_id = ",".join(preset.target_ids)
            if not target_class and preset.target_classes:
                target_class = ",".join(preset.target_classes)
            # åˆå¹¶é¢„è®¾çš„ exclude_selectors
            preset_excludes = ",".join(preset.exclude_selectors)
            if exclude_selectors:
                exclude_selectors = f"{exclude_selectors},{preset_excludes}"
            else:
                exclude_selectors = preset_excludes
            # è‡ªåŠ¨å¯ç”¨å¯¼èˆªå‰¥ç¦»
            strip_nav = True
            strip_page_toc = True
            # é¢„è®¾æ¨¡å¼ä¸‹è‡ªåŠ¨å¯ç”¨é”šç‚¹åˆ—è¡¨å‰¥ç¦»
            if anchor_list_threshold == 0:
                anchor_list_threshold = 10
            print(f"  â€¢ æ­£æ–‡å®¹å™¨ IDï¼š{target_id or '(æœªè®¾ç½®)'}")
            print(f"  â€¢ æ­£æ–‡å®¹å™¨ classï¼š{target_class or '(æœªè®¾ç½®)'}")
    
    # å•é¡µæ¨¡å¼ï¼šè‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ¡†æ¶ï¼ˆPhase 2ï¼‰
    elif hasattr(args, 'auto_detect') and args.auto_detect:
        framework, confidence, signals = detect_docs_framework(page_html)
        if framework and confidence >= 0.6:
            preset = DOCS_PRESETS.get(framework)
            if preset:
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æ–‡æ¡£æ¡†æ¶ï¼š{preset.name}ï¼ˆç½®ä¿¡åº¦ï¼š{confidence:.0%}ï¼‰")
                # åº”ç”¨é¢„è®¾é…ç½®
                if not target_id and preset.target_ids:
                    target_id = ",".join(preset.target_ids)
                if not target_class and preset.target_classes:
                    target_class = ",".join(preset.target_classes)
                preset_excludes = ",".join(preset.exclude_selectors)
                if exclude_selectors:
                    exclude_selectors = f"{exclude_selectors},{preset_excludes}"
                else:
                    exclude_selectors = preset_excludes
                strip_nav = True
                strip_page_toc = True
                if anchor_list_threshold == 0:
                    anchor_list_threshold = 10
        elif framework:
            print(f"ğŸ” æ£€æµ‹åˆ°å¯èƒ½çš„æ–‡æ¡£æ¡†æ¶ï¼š{framework}ï¼ˆç½®ä¿¡åº¦ï¼š{confidence:.0%}ï¼Œæœªè‡ªåŠ¨åº”ç”¨ï¼‰")
    
    # å¾®ä¿¡æ¨¡å¼ä¸‹ï¼Œå¦‚æœæœªæŒ‡å®š targetï¼Œè‡ªåŠ¨ä½¿ç”¨ rich_media_content
    if is_wechat and not target_id and not target_class:
        target_class = "rich_media_content"
        print("ä½¿ç”¨å¾®ä¿¡æ­£æ–‡åŒºåŸŸï¼šrich_media_content")

    # ä½¿ç”¨å¤šå€¼ target æå–ï¼ˆPhase 2 æ”¯æŒé€—å·åˆ†éš”ï¼‰
    if target_id or target_class:
        article_html, matched_selector = extract_target_html_multi(
            page_html, target_ids=target_id, target_classes=target_class
        )
        if not article_html:
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ°æŒ‡å®šçš„ç›®æ ‡åŒºåŸŸï¼Œå°†å›é€€åˆ°è‡ªåŠ¨æŠ½å–ã€‚", file=sys.stderr)
            article_html = extract_main_html(page_html)
        elif matched_selector:
            print(f"ä½¿ç”¨æ­£æ–‡å®¹å™¨ï¼š{matched_selector}")
    else:
        article_html = extract_main_html(page_html)

    # å•é¡µæ¨¡å¼ï¼šåº”ç”¨å¯¼èˆªå‰¥ç¦»ï¼ˆPhase 1ï¼‰
    strip_selectors = get_strip_selectors(
        strip_nav=strip_nav,
        strip_page_toc=strip_page_toc,
        exclude_selectors=exclude_selectors,
    )
    if strip_selectors:
        article_html, strip_stats = strip_html_elements(article_html, strip_selectors)
        if strip_stats.elements_removed > 0:
            print(f"å·²ç§»é™¤ {strip_stats.elements_removed} ä¸ªå¯¼èˆªå…ƒç´ ")

    if args.spa_warn_len and html_text_len(article_html) < args.spa_warn_len:
        print(
            f"è­¦å‘Šï¼šæŠ½å–åˆ°çš„æ­£æ–‡å†…å®¹è¾ƒçŸ­ï¼ˆ<{args.spa_warn_len} å­—ç¬¦ï¼‰ï¼Œè¯¥é¡µé¢å¯èƒ½ä¸º SPA åŠ¨æ€æ¸²æŸ“ï¼›"
            "å¦‚å†…å®¹ä¸ºç©º/ä¸å®Œæ•´ï¼Œå¯å°è¯•ï¼š1) ä½¿ç”¨ --target-id/--target-class æŒ‡å®šæ­£æ–‡åŒºåŸŸï¼›"
            "2) ç­‰å¾…é¡µé¢å®Œæ•´åŠ è½½åä¿å­˜ HTML å†å¤„ç†ï¼›3) ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·è·å–æ¸²æŸ“åçš„ HTMLã€‚",
            file=sys.stderr,
        )

    collector = ImageURLCollector(base_url=url)
    collector.feed(article_html)
    image_urls = uniq_preserve_order(collector.image_urls)

    print(f"å‘ç°å›¾ç‰‡ï¼š{len(image_urls)} å¼ ï¼Œå¼€å§‹ä¸‹è½½åˆ°ï¼š{assets_dir}")
    url_to_local = download_images(
        session=session,
        image_urls=image_urls,
        assets_dir=assets_dir,
        md_dir=md_dir,
        timeout_s=args.timeout,
        retries=args.retries,
        best_effort=bool(args.best_effort_images),
        page_url=url,
        redact_urls=args.redact_url,
        max_image_bytes=args.max_image_bytes,
    )

    # æå–æ ‡é¢˜ï¼ˆå¾®ä¿¡æ¨¡å¼ä¸‹ä¼˜å…ˆä½¿ç”¨ä¸“ç”¨æå–å‡½æ•°ï¼‰
    if args.title:
        title = args.title
    elif is_wechat:
        title = extract_wechat_title(page_html) or extract_h1(article_html) or extract_title(page_html) or "Untitled"
    else:
        title = extract_h1(article_html) or extract_title(page_html) or "Untitled"
    md_body = html_to_markdown(
        article_html=article_html,
        base_url=url,
        url_to_local=url_to_local,
        keep_html=args.keep_html,
    )
    md_body = strip_duplicate_h1(md_body, title)

    # æ¸…ç†å™ªéŸ³å†…å®¹
    if is_wechat:
        md_body = clean_wechat_noise(md_body)
        print("å·²æ¸…ç†å¾®ä¿¡å…¬ä¼—å· UI å™ªéŸ³")

    # å•é¡µæ¨¡å¼ï¼šé”šç‚¹åˆ—è¡¨å‰¥ç¦»ï¼ˆPhase 1ï¼‰
    # ä½¿ç”¨å±€éƒ¨å˜é‡ anchor_list_thresholdï¼ˆå¯èƒ½è¢«é¢„è®¾ä¿®æ”¹ï¼‰
    if anchor_list_threshold > 0:
        md_body, anchor_stats = strip_anchor_lists(md_body, anchor_list_threshold)
        if anchor_stats.anchor_lists_removed > 0:
            print(f"å·²ç§»é™¤ {anchor_stats.anchor_lists_removed} ä¸ªé”šç‚¹åˆ—è¡¨å—ï¼ˆå…± {anchor_stats.anchor_lines_removed} è¡Œï¼‰")

    # è§£æ tags å‚æ•°
    tags: Optional[List[str]] = None
    if args.tags:
        tags = [t.strip() for t in args.tags.split(",") if t.strip()]

    display_url = redact_url(url) if args.redact_url else url
    if args.redact_url:
        md_body = redact_urls_in_markdown(md_body)

    with open(out_md, "w", encoding="utf-8") as f:
        if args.frontmatter:
            f.write(generate_frontmatter(title, display_url, tags))
        # ä¿æŒæ­£æ–‡å¯è¯»æ€§ï¼šæ— è®ºæ˜¯å¦å¯ç”¨ frontmatterï¼Œéƒ½å†™å…¥å¯è§æ ‡é¢˜ä¸æ¥æºè¡Œã€‚
        f.write(f"# {title}\n\n")
        f.write(f"- Source: {display_url}\n\n")
        f.write(md_body)

    wrote_map_json = False
    if not args.no_map_json:
        with open(map_json, "w", encoding="utf-8") as f:
            map_payload = _redact_url_to_local_map(url_to_local) if args.redact_url else url_to_local
            json.dump(map_payload, f, ensure_ascii=False, indent=2)
        wrote_map_json = True
    else:
        # Bug fix: --no-map-json æ—¶åˆ é™¤æ—§çš„æ˜ å°„æ–‡ä»¶ï¼Œé¿å…é—ç•™æœªè„±æ•çš„å†å² URL
        if os.path.exists(map_json):
            try:
                os.remove(map_json)
                print(f"å·²åˆ é™¤æ—§æ˜ å°„æ–‡ä»¶ï¼š{map_json}")
            except OSError as e:
                print(f"è­¦å‘Šï¼šæ— æ³•åˆ é™¤æ—§æ˜ å°„æ–‡ä»¶ {map_json}: {e}", file=sys.stderr)

    print(f"å·²ç”Ÿæˆï¼š{out_md}")
    print(f"å›¾ç‰‡ç›®å½•ï¼š{assets_dir}")
    if wrote_map_json:
        print(f"æ˜ å°„æ–‡ä»¶ï¼š{map_json}")

    if args.with_pdf:
        out_pdf = os.path.splitext(out_md)[0] + ".pdf"
        if os.path.exists(out_pdf) and (not args.overwrite):
            print(f"PDF å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼š{out_pdf}ï¼ˆå¦‚éœ€è¦†ç›–è¯·åŠ  --overwriteï¼‰", file=sys.stderr)
        else:
            print(f"ç”Ÿæˆ PDFï¼š{out_pdf}")
            if args.frontmatter:
                # md æ–‡ä»¶ä¿ç•™ frontmatterï¼›ä½† PDF æ¸²æŸ“æ—¶å‰¥ç¦»å…ƒæ•°æ®å—ï¼Œå¹¶è¡¥ä¸€ä¸ªå¯è§æ ‡é¢˜/æ¥æºè¡Œã€‚
                pdf_md = f"# {title}\n\n- Source: {display_url}\n\n{md_body}"
                md_dir_abs = os.path.dirname(os.path.abspath(out_md)) or "."
                tmp = None
                try:
                    with tempfile.NamedTemporaryFile(
                        "w",
                        encoding="utf-8",
                        suffix=".no_frontmatter.md",
                        dir=md_dir_abs,
                        delete=False,
                    ) as tf:
                        tf.write(strip_yaml_frontmatter(pdf_md))
                        tmp = tf.name
                    generate_pdf_from_markdown(md_path=tmp, pdf_path=out_pdf, allow_file_access=args.pdf_allow_file_access)
                finally:
                    if tmp and os.path.isfile(tmp):
                        try:
                            os.remove(tmp)
                        except OSError:
                            pass
            else:
                generate_pdf_from_markdown(md_path=out_md, pdf_path=out_pdf, allow_file_access=args.pdf_allow_file_access)

    if args.validate:
        result = validate_markdown(out_md, assets_dir)
        print("\næ ¡éªŒç»“æœï¼š")
        print(f"- å›¾ç‰‡å¼•ç”¨æ•°ï¼ˆæ€»ï¼‰ï¼š{result.image_refs}")
        print(f"- å›¾ç‰‡å¼•ç”¨æ•°ï¼ˆæœ¬åœ°ï¼‰ï¼š{result.local_image_refs}")
        print(f"- assets æ–‡ä»¶æ•°ï¼š{result.asset_files}")
        if result.missing_files:
            print("- ç¼ºå¤±æ–‡ä»¶ï¼š")
            for m in result.missing_files:
                print(f"  - {m}")
            return EXIT_VALIDATION_FAILED
        else:
            print("- ç¼ºå¤±æ–‡ä»¶ï¼š0")

    return EXIT_SUCCESS


if __name__ == "__main__":
    raise SystemExit(main())
