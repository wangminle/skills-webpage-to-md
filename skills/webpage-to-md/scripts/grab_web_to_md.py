#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
æŠ“å–ç½‘é¡µæ­£æ–‡ä¸å›¾ç‰‡ï¼Œä¿å­˜ä¸º Markdown + æœ¬åœ° assets ç›®å½•ã€‚

ä¾èµ–è¯´æ˜ï¼š
- å¿…éœ€ä¾èµ–ï¼šrequestsï¼ˆHTTP è¯·æ±‚ï¼‰
- å¯é€‰ä¾èµ–ï¼šmarkdownï¼ˆç”¨äº PDF æ¸²æŸ“æ—¶çš„ Markdownâ†’HTML è½¬æ¢ï¼Œæ— åˆ™ä½¿ç”¨å†…ç½®ç®€æ˜“è½¬æ¢ï¼‰
- PDF ç”Ÿæˆï¼šä½¿ç”¨ç³»ç»Ÿå·²å®‰è£…çš„ Edge/Chrome æµè§ˆå™¨ headless æ¨¡å¼ï¼Œæ— éœ€é¢å¤–å®‰è£…å·¥å…·
- ä¸ä¾èµ–ï¼špandocã€playwrightã€seleniumã€bs4ã€lxml

è®¾è®¡ç›®æ ‡ï¼ˆæ¥è‡ªä¹‹å‰å››ä¸ªç«™ç‚¹çš„å®è·µï¼‰ï¼š
- ä¼˜å…ˆæå– <article>ï¼ˆå…¶æ¬¡ <main>/<body>ï¼‰ï¼Œå‡å°‘å¯¼èˆª/é¡µè„šå™ªéŸ³
- ä»…ç”¨æ ‡å‡†åº“ HTMLParserï¼ˆä¸ä¾èµ– bs4/lxmlï¼‰ï¼Œé€‚é…ç¦»çº¿/å—é™ç¯å¢ƒ
- å›¾ç‰‡ä¸‹è½½æ”¯æŒï¼šsrc/data-src/srcset/picture/sourceï¼›ç›¸å¯¹ URLï¼›content-type ç¼ºå¤±æ—¶å—…æ¢æ ¼å¼
- Ghost/Anthropic ç­‰ç«™ç‚¹ä¼šæŠŠè§†é¢‘æ’­æ”¾å™¨/å›¾æ ‡æ··è¿›æ­£æ–‡ï¼šè·³è¿‡å¸¸è§ UI æ ‡ç­¾/ç±»
- å¤„ç† <tag/> è‡ªé—­åˆå¯¼è‡´çš„ skip æ ˆä¸å‡ºæ ˆï¼šå®ç° handle_startendtag
- ç®€å•è¡¨æ ¼è½¬æ¢ä¸º Markdown tableï¼›å¹¶æä¾›æ ¡éªŒï¼ˆå¼•ç”¨æ•°=æ–‡ä»¶æ•°/æ–‡ä»¶å­˜åœ¨ï¼‰
"""

from __future__ import annotations

import argparse
import json
import os
import sys
import tempfile
import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, List, Optional, Sequence, Tuple, Callable

import requests

# æ”¯æŒé€šè¿‡ importlib ç›´æ¥åŠ è½½æœ¬è„šæœ¬æ—¶å¯¼å…¥åŒçº§ package
_SCRIPT_DIR = os.path.dirname(os.path.abspath(__file__))
if _SCRIPT_DIR not in sys.path:
    sys.path.insert(0, _SCRIPT_DIR)

from webpage_to_md.models import BatchConfig, BatchPageResult, JSChallengeResult, ValidationResult
from webpage_to_md.http_client import (
    UA_PRESETS,
    _DEFAULT_MAX_HTML_BYTES,
    _create_session,
    fetch_html,
)
from webpage_to_md.extractors import (
    DOCS_PRESETS,
    ImageURLCollector,
    detect_docs_framework,
    extract_h1,
    extract_links_from_html,
    extract_main_html,
    extract_target_html,
    extract_target_html_multi,
    extract_title,
    extract_wechat_title,
    get_available_presets,
    get_strip_selectors,
    html_text_len,
    is_wechat_article_html,
    is_wechat_article_url,
    read_urls_file,
    strip_anchor_lists,
    strip_html_elements,
    uniq_preserve_order,
)
from webpage_to_md.images import (
    _DEFAULT_MAX_IMAGE_BYTES,
    batch_download_images,
    download_images,
    replace_image_urls_in_markdown,
)
from webpage_to_md.markdown_conv import (
    clean_wechat_noise,
    clean_wiki_noise,
    html_to_markdown,
    rewrite_internal_links,
    strip_duplicate_h1,
)
from webpage_to_md.output import (
    _default_basename,
    _safe_path_length,
    _sanitize_filename_part,
    auto_wrap_output_dir,
    batch_save_individual,
    generate_frontmatter,
    generate_index_markdown,
    generate_merged_markdown,
)
from webpage_to_md.pdf_utils import (
    generate_pdf_from_markdown,
    strip_yaml_frontmatter,
)
from webpage_to_md.security import (
    _redact_url_to_local_map,
    detect_js_challenge,
    print_js_challenge_warning,
    redact_url,
    redact_urls_in_markdown,
    validate_markdown,
)


# ============================================================================
# é€€å‡ºç å®šä¹‰
# ============================================================================
EXIT_SUCCESS = 0
EXIT_ERROR = 1
EXIT_FILE_EXISTS = 2
EXIT_VALIDATION_FAILED = 3
EXIT_JS_CHALLENGE = 4  # æ£€æµ‹åˆ° JS åçˆ¬ä¿æŠ¤ï¼Œæ— æ³•è·å–å†…å®¹



def process_single_url(
    session: requests.Session,
    url: str,
    config: BatchConfig,
    custom_title: Optional[str] = None,
    order: int = 0,
) -> BatchPageResult:
    """å¤„ç†å•ä¸ª URLï¼Œè¿”å›ç»“æœ"""
    try:
        # è·å–é¡µé¢
        page_html = fetch_html(
            session=session,
            url=url,
            timeout_s=config.timeout,
            retries=config.retries,
            max_html_bytes=config.max_html_bytes,
        )

        # æ‰¹é‡æ¨¡å¼åŒæ ·æ£€æµ‹ JS åçˆ¬æŒ‘æˆ˜é¡µï¼Œé¿å…æŠŠéªŒè¯é¡µè¯¯å½“æ­£æ–‡å¯¼å‡º
        js_detection = detect_js_challenge(page_html)
        if js_detection.is_challenge and not config.force:
            suggestions = js_detection.get_suggestions(url)
            suggestion_text = "\n".join(f"  {s}" for s in suggestions)
            raise RuntimeError(
                "æ£€æµ‹åˆ° JavaScript åçˆ¬ä¿æŠ¤ï¼Œå½“å‰é¡µé¢æ— æ³•é€šè¿‡çº¯ HTTP è¯·æ±‚è·å–å®Œæ•´å†…å®¹ã€‚\n"
                f"ç½®ä¿¡åº¦ï¼š{js_detection.confidence}\n"
                "å»ºè®®æ“ä½œï¼š\n"
                f"{suggestion_text}\n"
                "å¦‚éœ€è·³è¿‡è¯¥æ£€æŸ¥å¹¶å¼ºåˆ¶ç»§ç»­ï¼Œè¯·æ·»åŠ  --forceã€‚"
            )
        
        # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« è‡ªåŠ¨æ£€æµ‹
        is_wechat = config.wechat
        if not is_wechat and is_wechat_article_url(url):
            is_wechat = True
        elif not is_wechat and is_wechat_article_html(page_html):
            is_wechat = True
        
        # ç¡®å®šæ­£æ–‡æå–ç­–ç•¥
        target_id = config.target_id
        target_class = config.target_class
        exclude_selectors = config.exclude_selectors
        strip_nav = config.strip_nav
        strip_page_toc = config.strip_page_toc
        anchor_list_threshold = config.anchor_list_threshold
        
        # å¾®ä¿¡æ¨¡å¼ä¸‹ï¼Œå¦‚æœæœªæŒ‡å®š targetï¼Œè‡ªåŠ¨ä½¿ç”¨ rich_media_content
        if is_wechat and not target_id and not target_class:
            target_class = "rich_media_content"
        
        # Phase 2: è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ¡†æ¶
        detected_preset: Optional[str] = None
        if config.auto_detect and not config.docs_preset:
            detected_preset, confidence, signals = detect_docs_framework(page_html)
            if detected_preset and confidence >= 0.5:
                preset = DOCS_PRESETS.get(detected_preset)
                if preset:
                    # é«˜ç½®ä¿¡åº¦æ—¶åº”ç”¨é¢„è®¾
                    if not target_id and preset.target_ids:
                        target_id = ",".join(preset.target_ids)
                    if not target_class and preset.target_classes:
                        target_class = ",".join(preset.target_classes)
                    # æ‰¹é‡æ¨¡å¼ä¸‹ auto-detect ä¹Ÿåº”å°½é‡å¤ç”¨é¢„è®¾çš„â€œå»å¯¼èˆªâ€èƒ½åŠ›ï¼Œä¿æŒä¸å•é¡µæ¨¡å¼ä¸€è‡´
                    preset_excludes = ",".join(preset.exclude_selectors) if preset.exclude_selectors else ""
                    if preset_excludes:
                        if exclude_selectors:
                            exclude_selectors = f"{exclude_selectors},{preset_excludes}"
                        else:
                            exclude_selectors = preset_excludes
                    strip_nav = True
                    strip_page_toc = True
                    if anchor_list_threshold == 0:
                        anchor_list_threshold = 10
        
        # æå–æ­£æ–‡ï¼ˆæ”¯æŒå¤šå€¼ targetï¼ŒT2.1ï¼‰
        if target_id or target_class:
            # ä½¿ç”¨å¤šå€¼æå–
            article_html, matched = extract_target_html_multi(
                page_html, 
                target_ids=target_id, 
                target_classes=target_class
            )
            if not article_html:
                # å›é€€åˆ°å•å€¼æå–ï¼ˆå…¼å®¹æ—§é€»è¾‘ï¼‰
                article_html = extract_target_html(
                    page_html,
                    target_id=target_id.split(",")[0] if target_id else None,
                    target_class=target_class.split(",")[0] if target_class else None,
                ) or ""
            if not article_html:
                article_html = extract_main_html(page_html)
        else:
            article_html = extract_main_html(page_html)
        
        # Phase 1: HTML å¯¼èˆªå…ƒç´ å‰¥ç¦»ï¼ˆåœ¨æå–æ­£æ–‡åã€è½¬æ¢ Markdown å‰ï¼‰
        strip_selectors = get_strip_selectors(
            strip_nav=strip_nav,
            strip_page_toc=strip_page_toc,
            exclude_selectors=exclude_selectors,
        )
        if strip_selectors:
            article_html, _ = strip_html_elements(article_html, strip_selectors)
        
        # æå–æ ‡é¢˜ï¼ˆå¾®ä¿¡æ¨¡å¼ä¸‹ä¼˜å…ˆä½¿ç”¨ä¸“ç”¨æå–å‡½æ•°ï¼‰
        if custom_title:
            title = custom_title
        elif is_wechat:
            title = extract_wechat_title(page_html) or extract_h1(article_html) or extract_title(page_html) or "Untitled"
        else:
            title = extract_h1(article_html) or extract_title(page_html) or "Untitled"
        
        # æ”¶é›†å›¾ç‰‡ URLï¼ˆå¦‚æœéœ€è¦ä¸‹è½½å›¾ç‰‡ï¼‰
        image_urls: List[str] = []
        if config.download_images:
            collector = ImageURLCollector(base_url=url)
            collector.feed(article_html)
            image_urls = uniq_preserve_order(collector.image_urls)
        
        # è½¬æ¢ä¸º Markdownï¼ˆæ‰¹é‡æ¨¡å¼å…ˆä¸æ›¿æ¢å›¾ç‰‡è·¯å¾„ï¼Œåç»­ç»Ÿä¸€å¤„ç†ï¼‰
        md_body = html_to_markdown(
            article_html=article_html,
            base_url=url,
            url_to_local={},  # å…ˆä¸æ›¿æ¢å›¾ç‰‡è·¯å¾„
            keep_html=config.keep_html,
        )
        md_body = strip_duplicate_h1(md_body, title)
        
        # æ¸…ç†å™ªéŸ³å†…å®¹
        if is_wechat:
            md_body = clean_wechat_noise(md_body)
        if config.clean_wiki_noise:
            md_body = clean_wiki_noise(md_body)
        
        # Phase 1: Markdown é”šç‚¹åˆ—è¡¨å‰¥ç¦»
        if anchor_list_threshold > 0:
            md_body, _ = strip_anchor_lists(md_body, anchor_list_threshold)
        
        return BatchPageResult(
            url=url,
            title=title,
            md_content=md_body,
            success=True,
            order=order,
            image_urls=image_urls,
        )
    
    except Exception as e:
        return BatchPageResult(
            url=url,
            title=custom_title or url,
            md_content="",
            success=False,
            error=str(e),
            order=order,
        )


def batch_process_urls(
    session: requests.Session,
    urls: List[Tuple[str, Optional[str]]],
    config: BatchConfig,
    progress_callback: Optional[Callable[[int, int, str], None]] = None,
) -> List[BatchPageResult]:
    """
    æ‰¹é‡å¤„ç† URL åˆ—è¡¨
    
    Args:
        session: requests.Session
        urls: [(url, custom_title), ...]
        config: æ‰¹é‡å¤„ç†é…ç½®
        progress_callback: è¿›åº¦å›è°ƒå‡½æ•° (current, total, url)
    
    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨
    """
    results: List[BatchPageResult] = []
    total = len(urls)
    lock = threading.Lock()
    last_request_time = [0.0]  # ä½¿ç”¨åˆ—è¡¨ä»¥ä¾¿åœ¨é—­åŒ…ä¸­ä¿®æ”¹
    
    def process_with_delay(args: Tuple[int, str, Optional[str]]) -> BatchPageResult:
        idx, url, custom_title = args
        
        # æ§åˆ¶è¯·æ±‚é—´éš”
        with lock:
            now = time.time()
            elapsed = now - last_request_time[0]
            if elapsed < config.delay:
                time.sleep(config.delay - elapsed)
            last_request_time[0] = time.time()
        
        if progress_callback:
            progress_callback(idx + 1, total, url)
        
        return process_single_url(
            session=session,
            url=url,
            config=config,
            custom_title=custom_title,
            order=idx,
        )
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘å¤„ç†
    with ThreadPoolExecutor(max_workers=config.max_workers) as executor:
        args_list = [(i, url, title) for i, (url, title) in enumerate(urls)]
        futures = {executor.submit(process_with_delay, args): args for args in args_list}
        
        for future in as_completed(futures):
            result = future.result()
            results.append(result)
            
            if not result.success and not config.skip_errors:
                # å–æ¶ˆå‰©ä½™ä»»åŠ¡
                for f in futures:
                    f.cancel()
                raise RuntimeError(f"å¤„ç†å¤±è´¥ï¼š{result.url}\né”™è¯¯ï¼š{result.error}")
    
    # æŒ‰åŸå§‹é¡ºåºæ’åº
    results.sort(key=lambda r: r.order)
    return results


def _batch_main(args: argparse.Namespace) -> int:
    """æ‰¹é‡å¤„ç†æ¨¡å¼çš„ä¸»å‡½æ•°"""
    
    # åˆ›å»º Session
    session = _create_session(args, referer_url=args.url)
    
    # æ”¶é›†è¦å¤„ç†çš„ URL åˆ—è¡¨
    urls: List[Tuple[str, Optional[str]]] = []
    source_url: Optional[str] = None
    
    if args.urls_file:
        # ä»æ–‡ä»¶è¯»å– URL
        if not os.path.isfile(args.urls_file):
            print(f"é”™è¯¯ï¼šURL åˆ—è¡¨æ–‡ä»¶ä¸å­˜åœ¨ï¼š{args.urls_file}", file=sys.stderr)
            return EXIT_ERROR
        urls = read_urls_file(args.urls_file)
        print(f"ä»æ–‡ä»¶åŠ è½½äº† {len(urls)} ä¸ª URL")
    
    if args.crawl:
        # ä»ç´¢å¼•é¡µçˆ¬å–é“¾æ¥
        if not args.url:
            print("é”™è¯¯ï¼šçˆ¬å–æ¨¡å¼éœ€è¦æä¾›ç´¢å¼•é¡µ URL", file=sys.stderr)
            return EXIT_ERROR
        
        source_url = args.url
        print(f"æ­£åœ¨ä»ç´¢å¼•é¡µæå–é“¾æ¥ï¼š{args.url}")
        
        try:
            index_html = fetch_html(
                session=session,
                url=args.url,
                timeout_s=args.timeout,
                retries=args.retries,
                max_html_bytes=args.max_html_bytes,
            )
        except Exception as e:
            print(f"é”™è¯¯ï¼šæ— æ³•è·å–ç´¢å¼•é¡µï¼š{e}", file=sys.stderr)
            return EXIT_ERROR

        # ç´¢å¼•é¡µä¹Ÿå¯èƒ½å‘½ä¸­ JS åçˆ¬ï¼šæå‰ç»™å‡ºè§£å†³æ–¹æ¡ˆï¼Œé¿å…åç»­â€œæå– 0 é“¾æ¥â€çš„è¯¯å¯¼
        js_detection = detect_js_challenge(index_html)
        if js_detection.is_challenge:
            print_js_challenge_warning(js_detection, args.url)
            if not args.force:
                return EXIT_JS_CHALLENGE
            print("å·²æ·»åŠ  --force å‚æ•°ï¼Œå¼ºåˆ¶ç»§ç»­å¤„ç†ç´¢å¼•é¡µ...", file=sys.stderr)
        
        # æå–é“¾æ¥
        links = extract_links_from_html(
            html=index_html,
            base_url=args.url,
            pattern=args.crawl_pattern,
            same_domain=args.same_domain,
        )
        
        # æ·»åŠ åˆ° URL åˆ—è¡¨ï¼ˆé¿å…é‡å¤ï¼‰
        existing_urls = {u for u, _ in urls}
        for link_url, link_text in links:
            if link_url not in existing_urls:
                urls.append((link_url, link_text))
                existing_urls.add(link_url)
        
        print(f"ä»ç´¢å¼•é¡µæå–äº† {len(links)} ä¸ªé“¾æ¥ï¼Œæ€»è®¡ {len(urls)} ä¸ª URL")
    
    if not urls:
        print("é”™è¯¯ï¼šæ²¡æœ‰è¦å¤„ç†çš„ URL", file=sys.stderr)
        return EXIT_ERROR
    
    # æ˜¾ç¤º URL åˆ—è¡¨é¢„è§ˆ
    print("\nå³å°†å¤„ç†çš„ URL åˆ—è¡¨ï¼š")
    for i, (url, title) in enumerate(urls[:10], 1):
        display = f"  {i}. {title or url}"
        if len(display) > 80:
            display = display[:77] + "..."
        print(display)
    if len(urls) > 10:
        print(f"  ... å…± {len(urls)} ä¸ª")
    print()
    
    # é…ç½®æ‰¹é‡å¤„ç†
    config = BatchConfig(
        max_workers=args.max_workers,
        delay=args.delay,
        skip_errors=args.skip_errors,
        timeout=args.timeout,
        retries=args.retries,
        max_html_bytes=args.max_html_bytes,
        best_effort_images=args.best_effort_images,  # Bug fix: ä½¿ç”¨ç”¨æˆ·å‚æ•°è€Œéç¡¬ç¼–ç 
        keep_html=args.keep_html,
        target_id=args.target_id,
        target_class=args.target_class,
        clean_wiki_noise=args.clean_wiki_noise,
        download_images=args.download_images,
        wechat=args.wechat,
        # Phase 1: å¯¼èˆªå‰¥ç¦»å‚æ•°
        strip_nav=args.strip_nav,
        strip_page_toc=args.strip_page_toc,
        exclude_selectors=args.exclude_selectors,
        anchor_list_threshold=args.anchor_list_threshold,
        # Phase 2: æ™ºèƒ½æ­£æ–‡å®šä½å‚æ•°
        docs_preset=args.docs_preset,
        auto_detect=args.auto_detect,
        force=args.force,
    )
    
    # Phase 2: åº”ç”¨æ–‡æ¡£æ¡†æ¶é¢„è®¾
    if args.docs_preset:
        preset = DOCS_PRESETS.get(args.docs_preset)
        if preset:
            print(f"\nğŸ“¦ ä½¿ç”¨æ–‡æ¡£æ¡†æ¶é¢„è®¾ï¼š{preset.name} ({preset.description})")
            # åº”ç”¨é¢„è®¾çš„ target é…ç½®
            if not config.target_id and preset.target_ids:
                config.target_id = ",".join(preset.target_ids)
            if not config.target_class and preset.target_classes:
                config.target_class = ",".join(preset.target_classes)
            # åˆå¹¶é¢„è®¾çš„ exclude_selectors
            preset_excludes = ",".join(preset.exclude_selectors)
            if config.exclude_selectors:
                config.exclude_selectors = f"{config.exclude_selectors},{preset_excludes}"
            else:
                config.exclude_selectors = preset_excludes
            # è‡ªåŠ¨å¯ç”¨å¯¼èˆªå‰¥ç¦»
            config.strip_nav = True
            config.strip_page_toc = True
            # é¢„è®¾æ¨¡å¼ä¸‹ï¼Œå¦‚æœç”¨æˆ·æœªæ˜¾å¼è®¾ç½® anchor_list_thresholdï¼Œåˆ™è‡ªåŠ¨å¯ç”¨ï¼ˆé»˜è®¤ 10ï¼‰
            if args.anchor_list_threshold == 0:
                config.anchor_list_threshold = 10
            print(f"  â€¢ æ­£æ–‡å®¹å™¨ IDï¼š{config.target_id or '(æœªè®¾ç½®)'}")
            print(f"  â€¢ æ­£æ–‡å®¹å™¨ classï¼š{config.target_class or '(æœªè®¾ç½®)'}")
            print(f"  â€¢ æ’é™¤é€‰æ‹©å™¨ï¼š{len(preset.exclude_selectors)} ä¸ª")
            if config.anchor_list_threshold > 0:
                print(f"  â€¢ é”šç‚¹åˆ—è¡¨é˜ˆå€¼ï¼š{config.anchor_list_threshold} è¡Œ")
    
    # Phase 1: æ‰“å°å¯¼èˆªå‰¥ç¦»é…ç½®
    if args.strip_nav or args.strip_page_toc or args.exclude_selectors:
        selectors = get_strip_selectors(args.strip_nav, args.strip_page_toc, args.exclude_selectors)
        print(f"å¯ç”¨å¯¼èˆªå‰¥ç¦»ï¼š{len(selectors)} ä¸ªé€‰æ‹©å™¨")
        if args.anchor_list_threshold > 0:
            print(f"é”šç‚¹åˆ—è¡¨ç§»é™¤é˜ˆå€¼ï¼š{args.anchor_list_threshold} è¡Œ")
    
    # è¿›åº¦å›è°ƒ
    def progress_callback(current: int, total: int, url: str) -> None:
        short_url = url if len(url) <= 50 else url[:47] + "..."
        print(f"[{current}/{total}] å¤„ç†ä¸­ï¼š{short_url}")
    
    # æ‰§è¡Œæ‰¹é‡å¤„ç†
    print(f"å¼€å§‹æ‰¹é‡å¤„ç†ï¼ˆå¹¶å‘æ•°ï¼š{config.max_workers}ï¼Œé—´éš”ï¼š{config.delay}sï¼‰...\n")
    
    try:
        results = batch_process_urls(
            session=session,
            urls=urls,
            config=config,
            progress_callback=progress_callback,
        )
    except RuntimeError as e:
        print(f"\né”™è¯¯ï¼š{e}", file=sys.stderr)
        return EXIT_ERROR
    
    # ç»Ÿè®¡ç»“æœ
    success_count = len([r for r in results if r.success])
    fail_count = len(results) - success_count
    print(f"\nå¤„ç†å®Œæˆï¼šæˆåŠŸ {success_count}ï¼Œå¤±è´¥ {fail_count}")
    
    # Phase 1: å¯¼èˆªå‰¥ç¦»ç»Ÿè®¡ï¼ˆT1.5 å¯è§‚æµ‹æ€§ï¼‰
    if args.strip_nav or args.strip_page_toc or args.exclude_selectors:
        selectors = get_strip_selectors(args.strip_nav, args.strip_page_toc, args.exclude_selectors)
        print(f"\nğŸ“Š å¯¼èˆªå‰¥ç¦»å·²ç”Ÿæ•ˆï¼š")
        print(f"  â€¢ åº”ç”¨é€‰æ‹©å™¨ï¼š{len(selectors)} ä¸ª")
        if args.strip_nav:
            print(f"  â€¢ --strip-nav: ç§»é™¤å¯¼èˆªå…ƒç´ ï¼ˆnav/aside/.sidebar ç­‰ï¼‰")
        if args.strip_page_toc:
            print(f"  â€¢ --strip-page-toc: ç§»é™¤é¡µå†…ç›®å½•ï¼ˆ.toc/.on-this-page ç­‰ï¼‰")
        if args.exclude_selectors:
            print(f"  â€¢ --exclude-selectors: {args.exclude_selectors}")
        if args.anchor_list_threshold > 0:
            print(f"  â€¢ é”šç‚¹åˆ—è¡¨é˜ˆå€¼ï¼š>{args.anchor_list_threshold} è¡Œè‡ªåŠ¨ç§»é™¤")
    
    # ä¸‹è½½å›¾ç‰‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    url_to_local: Dict[str, str] = {}
    if args.download_images:
        # ç»Ÿè®¡å›¾ç‰‡æ•°é‡
        total_images = sum(len(r.image_urls) for r in results if r.success)
        unique_images = len(set(url for r in results if r.success for url in r.image_urls))
        
        if unique_images > 0:
            # ç¡®å®š assets ç›®å½•
            if args.merge:
                output_file = args.merge_output or "merged.md"
                # è‡ªåŠ¨åˆ›å»ºåŒåä¸Šçº§ç›®å½•ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šç›®å½•ï¼‰
                output_file = auto_wrap_output_dir(output_file)
                assets_dir = os.path.splitext(output_file)[0] + ".assets"
                md_dir = os.path.dirname(output_file) or "."
            else:
                assets_dir = os.path.join(args.output_dir, "assets")
                md_dir = args.output_dir
            
            print(f"\nå‘ç° {unique_images} å¼ å›¾ç‰‡ï¼ˆå»é‡åï¼‰ï¼Œå¼€å§‹ä¸‹è½½åˆ°ï¼š{assets_dir}")
            
            def img_progress(current: int, total: int, url: str) -> None:
                short_url = url if len(url) <= 50 else url[:47] + "..."
                print(f"  [{current}/{total}] ä¸‹è½½ï¼š{short_url}")
            
            url_to_local = batch_download_images(
                session=session,
                results=results,
                assets_dir=assets_dir,
                md_dir=md_dir,
                timeout_s=args.timeout,
                retries=args.retries,
                best_effort=bool(args.best_effort_images),
                progress_callback=img_progress,
                redact_urls=args.redact_url,
                max_image_bytes=args.max_image_bytes,
            )
            
            print(f"  å›¾ç‰‡ä¸‹è½½å®Œæˆï¼š{len(url_to_local)} å¼ æˆåŠŸ")
            
            # æ›´æ–°ç»“æœä¸­çš„ Markdown å†…å®¹ï¼Œæ›¿æ¢å›¾ç‰‡ URL
            for result in results:
                if result.success and result.md_content:
                    result.md_content = replace_image_urls_in_markdown(
                        result.md_content, url_to_local
                    )
        else:
            print("\næœªå‘ç°éœ€è¦ä¸‹è½½çš„å›¾ç‰‡")
    
    # è¾“å‡ºç»“æœ
    if args.merge:
        # åˆå¹¶è¾“å‡ºæ¨¡å¼
        output_file = args.merge_output or "merged.md"
        # è‡ªåŠ¨åˆ›å»ºåŒåä¸Šçº§ç›®å½•ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šç›®å½•ï¼‰
        output_file = auto_wrap_output_dir(output_file)
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_dir = os.path.dirname(output_file)
        if output_dir:
            os.makedirs(output_dir, exist_ok=True)
        
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        if os.path.exists(output_file) and not args.overwrite:
            print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼š{output_file}ï¼ˆå¦‚éœ€è¦†ç›–è¯·åŠ  --overwriteï¼‰", file=sys.stderr)
            return EXIT_FILE_EXISTS
        
        # æ¥æº URL ä¼˜å…ˆçº§ï¼š--source-url > çˆ¬å–æ¨¡å¼çš„ç´¢å¼•é¡µ > Noneï¼ˆæå–åŸŸåï¼‰
        final_source_url = args.source_url or source_url
        
        merged_content, anchor_stats = generate_merged_markdown(
            results=results,
            include_toc=args.toc,
            main_title=args.merge_title or args.title,
            source_url=final_source_url,
            rewrite_links=args.rewrite_links,
            show_source_summary=not args.no_source_summary,
            redact_urls=args.redact_url,
        )
        
        with open(output_file, "w", encoding="utf-8") as f:
            f.write(merged_content)
        
        print(f"\nå·²ç”Ÿæˆåˆå¹¶æ–‡æ¡£ï¼š{output_file}")
        print(f"æ–‡æ¡£å¤§å°ï¼š{len(merged_content):,} å­—ç¬¦")
        
        # Phase 3-A: è¾“å‡ºé”šç‚¹å†²çªç»Ÿè®¡
        if anchor_stats.has_collisions:
            if hasattr(args, 'warn_anchor_collisions') and args.warn_anchor_collisions:
                anchor_stats.print_summary()
            else:
                print(f"ğŸ“Œ é”šç‚¹å†²çªï¼š{anchor_stats.collision_count} ä¸ªå·²è‡ªåŠ¨ä¿®å¤ï¼ˆä½¿ç”¨ --warn-anchor-collisions æŸ¥çœ‹è¯¦æƒ…ï¼‰")
        if url_to_local:
            assets_dir = os.path.splitext(output_file)[0] + ".assets"
            # ç»Ÿè®¡å›¾ç‰‡å¼•ç”¨æƒ…å†µï¼ˆéç ´åæ€§ï¼šåªæŠ¥å‘Šä¸åˆ é™¤ï¼‰
            if os.path.isdir(assets_dir):
                # ç»Ÿè®¡å®é™…æ–‡ä»¶æ•°
                all_files = [f for f in os.listdir(assets_dir) if os.path.isfile(os.path.join(assets_dir, f))]
                actual_count = len(all_files)
                
                # ç»Ÿè®¡è¢«å¼•ç”¨çš„æ–‡ä»¶ï¼ˆä¿å®ˆæ£€æµ‹ï¼šä½¿ç”¨æ–‡ä»¶ååŒ¹é…ï¼‰
                unused_files = []
                for filename in all_files:
                    # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦åœ¨æœ€ç»ˆå†…å®¹ä¸­å‡ºç°
                    if filename not in merged_content:
                        unused_files.append(filename)
                
                unused_count = len(unused_files)
                if unused_count > 0:
                    print(f"å›¾ç‰‡ç›®å½•ï¼š{assets_dir}ï¼ˆ{actual_count} å¼ å›¾ç‰‡ï¼Œ{unused_count} å¼ å¯èƒ½æœªå¼•ç”¨ï¼‰")
                    print(f"  âš ï¸ æœªè‡ªåŠ¨æ¸…ç†æœªå¼•ç”¨å›¾ç‰‡ï¼ˆå¯èƒ½å­˜åœ¨è¯¯åˆ¤ï¼‰ï¼Œå¦‚éœ€æ¸…ç†è¯·æ‰‹åŠ¨æ£€æŸ¥")
                else:
                    print(f"å›¾ç‰‡ç›®å½•ï¼š{assets_dir}ï¼ˆ{actual_count} å¼ å›¾ç‰‡ï¼‰")
            else:
                print(f"å›¾ç‰‡ç›®å½•ï¼š{assets_dir}ï¼ˆ{len(url_to_local)} å¼ å›¾ç‰‡ï¼‰")
        
        # Phase 3-B1: åŒç‰ˆæœ¬è¾“å‡ºï¼ˆåŒæ—¶ç”Ÿæˆåˆ†æ–‡ä»¶ç‰ˆæœ¬ï¼‰
        if hasattr(args, 'split_output') and args.split_output:
            split_dir = args.split_output
            os.makedirs(split_dir, exist_ok=True)
            
            # ç¡®å®šå…±äº«çš„ assets ç›®å½•ï¼ˆä½¿ç”¨åˆå¹¶ç‰ˆæœ¬çš„ assetsï¼‰
            shared_assets = os.path.splitext(output_file)[0] + ".assets" if url_to_local else None
            
            # ç”Ÿæˆåˆ†æ–‡ä»¶
            saved_files = batch_save_individual(
                results=results,
                output_dir=split_dir,
                include_frontmatter=args.frontmatter,
                redact_urls=args.redact_url,
                shared_assets_dir=shared_assets,
            )
            
            # ç”Ÿæˆç´¢å¼•æ–‡ä»¶
            index_content = generate_index_markdown(
                results=results,
                output_dir=split_dir,
                main_title=args.merge_title or args.title,
                source_url=final_source_url,
                saved_files=saved_files,
                redact_urls=args.redact_url,
            )
            index_path = os.path.join(split_dir, "INDEX.md")
            with open(index_path, "w", encoding="utf-8") as f:
                f.write(index_content)
            
            print(f"\nğŸ“‚ å·²åŒæ—¶ç”Ÿæˆåˆ†æ–‡ä»¶ç‰ˆæœ¬ï¼š")
            print(f"  â€¢ ç›®å½•ï¼š{split_dir}")
            print(f"  â€¢ æ–‡ä»¶æ•°ï¼š{len(saved_files)} ä¸ª")
            print(f"  â€¢ ç´¢å¼•ï¼š{index_path}")
            if shared_assets:
                rel_assets = os.path.relpath(shared_assets, split_dir)
                print(f"  â€¢ å…±äº« assetsï¼š{rel_assets}")
        
    else:
        # ç‹¬ç«‹æ–‡ä»¶è¾“å‡ºæ¨¡å¼
        os.makedirs(args.output_dir, exist_ok=True)
        
        saved_files = batch_save_individual(
            results=results,
            output_dir=args.output_dir,
            include_frontmatter=args.frontmatter,
            redact_urls=args.redact_url,
            shared_assets_dir=None,
        )
        
        # æ¥æº URL ä¼˜å…ˆçº§ï¼š--source-url > çˆ¬å–æ¨¡å¼çš„ç´¢å¼•é¡µ > Noneï¼ˆæå–åŸŸåï¼‰
        final_source_url = args.source_url or source_url
        
        # ç”Ÿæˆç´¢å¼•æ–‡ä»¶ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆï¼‰
        index_content = generate_index_markdown(
            results=results,
            output_dir=args.output_dir,
            main_title=args.merge_title or args.title,
            source_url=final_source_url,
            saved_files=saved_files,
            redact_urls=args.redact_url,
        )
        index_path = os.path.join(args.output_dir, "INDEX.md")
        with open(index_path, "w", encoding="utf-8") as f:
            f.write(index_content)
        
        print(f"\nå·²ç”Ÿæˆ {len(saved_files)} ä¸ªæ–‡ä»¶åˆ°ï¼š{args.output_dir}")
        print(f"ç´¢å¼•æ–‡ä»¶ï¼š{index_path}")
    
    # æ˜¾ç¤ºå¤±è´¥åˆ—è¡¨
    if fail_count > 0:
        print("\nå¤±è´¥çš„ URLï¼š")
        for result in results:
            if not result.success:
                print(f"  - {result.url}")
                print(f"    é”™è¯¯ï¼š{result.error}")
    
    return EXIT_SUCCESS


def _fetch_page_html(
    session: requests.Session,
    url: str,
    args: argparse.Namespace,
) -> Tuple[Optional[str], Optional[int]]:
    """è·å–é¡µé¢ HTML å¹¶å¤„ç†é”™è¯¯å’Œ JS åçˆ¬æ£€æµ‹ã€‚

    Returns:
        (page_html, exit_code) â€” exit_code ä¸º None è¡¨ç¤ºæˆåŠŸ
    """
    print(f"ä¸‹è½½é¡µé¢ï¼š{url}")
    try:
        page_html = fetch_html(
            session=session,
            url=url,
            timeout_s=args.timeout,
            retries=args.retries,
            max_html_bytes=args.max_html_bytes,
        )
    except requests.exceptions.HTTPError as exc:
        status = exc.response.status_code if exc.response is not None else "unknown"
        safe_url = redact_url(url) if args.redact_url else url
        print(f"é”™è¯¯ï¼šè¯·æ±‚å¤±è´¥ï¼ˆHTTP {status}ï¼‰ï¼š{safe_url}", file=sys.stderr)
        if status in (403, 429):
            print("", file=sys.stderr)
            print("å¯èƒ½è§¦å‘äº†ç«™ç‚¹çš„åçˆ¬æˆ–è®¿é—®é¢‘æ§ã€‚å»ºè®®ï¼š", file=sys.stderr)
            print("  1. åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€è¯¥ URLï¼Œç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½", file=sys.stderr)
            print("  2. å³é”®é¡µé¢å¦å­˜ä¸º .html æ–‡ä»¶", file=sys.stderr)
            print("  3. ä½¿ç”¨ --local-html ä¸ --base-url è¿›è¡Œå¤„ç†ï¼Œä¾‹å¦‚ï¼š", file=sys.stderr)
            print(
                f'     python grab_web_to_md.py --local-html saved.html --base-url "{safe_url}" --out output.md',
                file=sys.stderr,
            )
        return None, EXIT_ERROR
    except requests.exceptions.RequestException as exc:
        safe_url = redact_url(url) if args.redact_url else url
        print(f"é”™è¯¯ï¼šä¸‹è½½å¤±è´¥ï¼š{safe_url}", file=sys.stderr)
        print(f"è¯¦æƒ…ï¼š{exc}", file=sys.stderr)
        print("å»ºè®®ï¼šå¯æ”¹ç”¨æµè§ˆå™¨ä¿å­˜ HTML åï¼Œé€šè¿‡ --local-html ç¦»çº¿å¤„ç†ã€‚", file=sys.stderr)
        return None, EXIT_ERROR

    # JS åçˆ¬æ£€æµ‹
    js_detection = detect_js_challenge(page_html)
    if js_detection.is_challenge:
        print_js_challenge_warning(js_detection, url)
        if not args.force:
            return None, EXIT_JS_CHALLENGE
        print("å·²æ·»åŠ  --force å‚æ•°ï¼Œå¼ºåˆ¶ç»§ç»­å¤„ç†...", file=sys.stderr)

    return page_html, None


def _extract_title_for_filename(page_html: str, url: str = "") -> str:
    """ä»é¡µé¢ HTML ä¸­æå–æ ‡é¢˜ï¼ˆç”¨äºè‡ªåŠ¨å‘½åæ–‡ä»¶ï¼‰ã€‚

    ä¼˜å…ˆçº§ï¼šå¾®ä¿¡æ ‡é¢˜ > H1 > <title> > "Untitled"
    """
    # æ³¨æ„ï¼šurl å¯èƒ½ä¸ºç©ºï¼ˆ--local-html æœªæŒ‡å®š --base-urlï¼‰ï¼Œ
    # æ­¤æ—¶ä»éœ€é€šè¿‡ HTML ç‰¹å¾æ£€æµ‹å¾®ä¿¡é¡µé¢ï¼Œå› æ­¤ä¸¤ä¸ªæ¡ä»¶ç”¨ or è¿æ¥ã€‚
    is_wechat = (bool(url) and is_wechat_article_url(url)) or is_wechat_article_html(page_html)
    if is_wechat:
        wechat_title = extract_wechat_title(page_html)
        if wechat_title:
            return wechat_title
    return extract_h1(page_html) or extract_title(page_html) or "Untitled"


def main(argv: Optional[Sequence[str]] = None) -> int:
    ap = argparse.ArgumentParser(
        description="æŠ“å–ç½‘é¡µæ­£æ–‡ä¸å›¾ç‰‡ï¼Œä¿å­˜ä¸º Markdown + assetsã€‚æ”¯æŒå•é¡µå’Œæ‰¹é‡æ¨¡å¼ã€‚",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
æ‰¹é‡å¤„ç†ç¤ºä¾‹ï¼š
  # ä»æ–‡ä»¶è¯»å– URL åˆ—è¡¨ï¼Œåˆå¹¶ä¸ºå•ä¸ªæ–‡æ¡£
  python grab_web_to_md.py --urls-file urls.txt --merge --merge-output output.md

  # ä»ç´¢å¼•é¡µçˆ¬å–é“¾æ¥å¹¶æ‰¹é‡å¯¼å‡º
  python grab_web_to_md.py https://example.com/index --crawl --merge --toc

  # æ‰¹é‡å¯¼å‡ºä¸ºç‹¬ç«‹æ–‡ä»¶
  python grab_web_to_md.py --urls-file urls.txt --output-dir ./docs

urls.txt æ–‡ä»¶æ ¼å¼ï¼š
  # è¿™æ˜¯æ³¨é‡Š
  https://example.com/page1
  https://example.com/page2 | è‡ªå®šä¹‰æ ‡é¢˜
""",
    )
    ap.add_argument("url", nargs="?", help="è¦æŠ“å–çš„ç½‘é¡µ URLï¼ˆå•é¡µæ¨¡å¼å¿…éœ€ï¼Œæ‰¹é‡æ¨¡å¼å¯é€‰ï¼‰")
    ap.add_argument("--out", help="è¾“å‡º md æ–‡ä»¶åï¼ˆé»˜è®¤æ ¹æ® URL è‡ªåŠ¨ç”Ÿæˆï¼‰")
    ap.add_argument("--auto-title", action="store_true",
                    help="ä»é¡µé¢æ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆè¾“å‡ºæ–‡ä»¶åï¼ˆä¼˜å…ˆçº§ä½äº --outï¼›æœªæŒ‡å®š --out æ—¶ç”Ÿæ•ˆï¼‰")
    ap.add_argument("--assets-dir", help="å›¾ç‰‡ç›®å½•åï¼ˆé»˜è®¤ <out>.assetsï¼‰")
    ap.add_argument("--title", help="Markdown é¡¶éƒ¨æ ‡é¢˜ï¼ˆé»˜è®¤ä» <title> æå–ï¼‰")
    ap.add_argument("--with-pdf", action="store_true", help="åŒæ—¶ç”ŸæˆåŒå PDFï¼ˆéœ€è¦æœ¬æœº Edge/Chromeï¼‰")
    ap.add_argument("--timeout", type=int, default=60, help="è¯·æ±‚è¶…æ—¶ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 60")
    ap.add_argument("--retries", type=int, default=3, help="ç½‘ç»œé‡è¯•æ¬¡æ•°ï¼Œé»˜è®¤ 3")
    ap.add_argument(
        "--max-html-bytes",
        type=int,
        default=_DEFAULT_MAX_HTML_BYTES,
        help="å•é¡µ HTML æœ€å¤§å…è®¸å­—èŠ‚æ•°ï¼ˆé»˜è®¤ 10MBï¼›è®¾ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰",
    )
    ap.add_argument("--best-effort-images", action="store_true", help="å›¾ç‰‡ä¸‹è½½å¤±è´¥æ—¶ä»…è­¦å‘Šå¹¶è·³è¿‡ï¼ˆé»˜è®¤å¤±è´¥å³é€€å‡ºï¼‰")
    ap.add_argument("--overwrite", action="store_true", help="å…è®¸è¦†ç›–å·²å­˜åœ¨çš„ md æ–‡ä»¶")
    ap.add_argument("--validate", action="store_true", help="ç”Ÿæˆåæ‰§è¡Œæ ¡éªŒå¹¶è¾“å‡ºç»“æœ")
    # JS åçˆ¬å¤„ç†
    ap.add_argument("--local-html", metavar="FILE", help="ä»æœ¬åœ° HTML æ–‡ä»¶è¯»å–å†…å®¹ï¼ˆè·³è¿‡ç½‘ç»œè¯·æ±‚ï¼Œç”¨äºå¤„ç†æµè§ˆå™¨ä¿å­˜çš„é¡µé¢ï¼‰")
    ap.add_argument("--base-url", help="é…åˆ --local-html ä½¿ç”¨ï¼ŒæŒ‡å®šå›¾ç‰‡ä¸‹è½½çš„åŸºå‡† URL")
    ap.add_argument("--force", action="store_true", help="æ£€æµ‹åˆ° JS åçˆ¬æ—¶ä»å¼ºåˆ¶ç»§ç»­å¤„ç†ï¼ˆå†…å®¹å¯èƒ½ä¸ºç©ºæˆ–ä¸å®Œæ•´ï¼‰")
    ap.add_argument(
        "--max-image-bytes",
        type=int,
        default=_DEFAULT_MAX_IMAGE_BYTES,
        help="å•å¼ å›¾ç‰‡æœ€å¤§å…è®¸å­—èŠ‚æ•°ï¼ˆé»˜è®¤ 25MBï¼›è®¾ä¸º 0 è¡¨ç¤ºä¸é™åˆ¶ï¼‰",
    )
    ap.add_argument(
        "--redact-url",
        dest="redact_url",
        action="store_true",
        default=True,
        help="è¾“å‡ºæ–‡ä»¶ä¸­å¯¹ URL è„±æ•ï¼ˆé»˜è®¤å¯ç”¨ï¼‰ï¼šä»…ä¿ç•™ scheme://host/pathï¼Œç§»é™¤ query/fragment",
    )
    ap.add_argument(
        "--no-redact-url",
        dest="redact_url",
        action="store_false",
        help="å…³é—­ URL è„±æ•ï¼ˆä¿ç•™å®Œæ•´ URLï¼ŒåŒ…æ‹¬ query/fragmentï¼‰",
    )
    ap.add_argument(
        "--no-map-json",
        action="store_true",
        help="ä¸ç”Ÿæˆ *.assets.json URLâ†’æœ¬åœ°æ˜ å°„æ–‡ä»¶ï¼ˆé¿å…æ³„éœ²å›¾ç‰‡ URLï¼‰",
    )
    ap.add_argument(
        "--pdf-allow-file-access",
        action="store_true",
        help="ç”Ÿæˆ PDF æ—¶å…è®¸ file:// è®¿é—®å…¶ä»–æœ¬åœ°æ–‡ä»¶ï¼ˆå¯èƒ½æœ‰å®‰å…¨é£é™©ï¼›é»˜è®¤å…³é—­ï¼‰",
    )
    # Frontmatter æ”¯æŒ
    ap.add_argument("--frontmatter", action="store_true", default=True,
                    help="ç”Ÿæˆ YAML Frontmatter å…ƒæ•°æ®å¤´ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    ap.add_argument("--no-frontmatter", action="store_false", dest="frontmatter",
                    help="ç¦ç”¨ YAML Frontmatter")
    ap.add_argument("--tags", help="Frontmatter ä¸­çš„æ ‡ç­¾ï¼Œé€—å·åˆ†éš”ï¼Œå¦‚ 'tech,ai,tutorial'")
    # Cookie/Header æ”¯æŒ
    ap.add_argument("--cookie", help="Cookie å­—ç¬¦ä¸²ï¼Œå¦‚ 'session=abc; token=xyz'")
    ap.add_argument("--cookies-file", help="Netscape æ ¼å¼çš„ cookies.txt æ–‡ä»¶è·¯å¾„")
    ap.add_argument("--headers", help="è‡ªå®šä¹‰è¯·æ±‚å¤´ï¼ŒJSON æ ¼å¼ï¼Œå¦‚ '{\"Authorization\": \"Bearer xxx\"}'")
    ap.add_argument("--header", action="append", default=[], help="è¿½åŠ è¯·æ±‚å¤´ï¼ˆå¯é‡å¤ï¼‰ï¼Œå¦‚ 'Authorization: Bearer xxx'")
    # UA å¯é…ç½®
    ap.add_argument("--ua-preset", choices=sorted(UA_PRESETS.keys()), default="chrome-win", help="User-Agent é¢„è®¾ï¼ˆé»˜è®¤ chrome-winï¼‰")
    ap.add_argument("--user-agent", "--ua", dest="user_agent", help="è‡ªå®šä¹‰ User-Agentï¼ˆä¼˜å…ˆäº --ua-presetï¼‰")
    # å¤æ‚è¡¨æ ¼ä¿ç•™ HTML
    ap.add_argument("--keep-html", action="store_true",
                    help="å¯¹å¤æ‚è¡¨æ ¼ï¼ˆå« colspan/rowspanï¼‰ä¿ç•™åŸå§‹ HTML è€Œéå¼ºè½¬ Markdown")
    # æ‰‹åŠ¨æŒ‡å®šæ­£æ–‡åŒºåŸŸ
    ap.add_argument("--target-id", help="æ‰‹åŠ¨æŒ‡å®šæ­£æ–‡å®¹å™¨ idï¼ˆå¦‚ content / post-contentï¼‰ï¼Œä¼˜å…ˆçº§é«˜äºè‡ªåŠ¨æŠ½å–")
    ap.add_argument("--target-class", help="æ‰‹åŠ¨æŒ‡å®šæ­£æ–‡å®¹å™¨ classï¼ˆå¦‚ post-bodyï¼‰ï¼Œä¼˜å…ˆçº§é«˜äºè‡ªåŠ¨æŠ½å–")
    # SPA é¡µé¢æç¤º
    ap.add_argument("--spa-warn-len", type=int, default=500, help="æ­£æ–‡æ–‡æœ¬é•¿åº¦ä½äºè¯¥å€¼æ—¶æç¤ºå¯èƒ½ä¸º SPA åŠ¨æ€æ¸²æŸ“ï¼Œé»˜è®¤ 500ï¼›è®¾ä¸º 0 å¯å…³é—­")
    # Wiki å™ªéŸ³æ¸…ç†
    ap.add_argument("--clean-wiki-noise", action="store_true",
                    help="æ¸…ç† Wiki ç³»ç»Ÿå™ªéŸ³ï¼ˆç¼–è¾‘æŒ‰é’®ã€å¯¼èˆªé“¾æ¥ã€è¿”å›é¡¶éƒ¨ç­‰ï¼‰ï¼Œé€‚ç”¨äº PukiWiki/MediaWiki ç­‰ç«™ç‚¹")
    # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ”¯æŒ
    ap.add_argument("--wechat", action="store_true",
                    help="å¾®ä¿¡å…¬ä¼—å·æ–‡ç« æ¨¡å¼ï¼šè‡ªåŠ¨æå– rich_media_content æ­£æ–‡å¹¶æ¸…ç†äº¤äº’æŒ‰é’®å™ªéŸ³ã€‚"
                         "å¦‚ä¸æŒ‡å®šï¼Œè„šæœ¬ä¼šè‡ªåŠ¨æ£€æµ‹ mp.weixin.qq.com é“¾æ¥å¹¶å¯ç”¨æ­¤æ¨¡å¼")
    
    # ========== å¯¼èˆª/ç›®å½•å‰¥ç¦»å‚æ•°ï¼ˆPhase 1ï¼‰==========
    nav_group = ap.add_argument_group("å¯¼èˆªå‰¥ç¦»å‚æ•°ï¼ˆDocs/Wiki ç«™ç‚¹ä¼˜åŒ–ï¼‰")
    nav_group.add_argument("--strip-nav", action="store_true",
                           help="ç§»é™¤å¯¼èˆªå…ƒç´ ï¼ˆnav/aside/.sidebar ç­‰ï¼‰ï¼Œé€‚ç”¨äº docs ç«™ç‚¹æ‰¹é‡å¯¼å‡º")
    nav_group.add_argument("--strip-page-toc", action="store_true",
                           help="ç§»é™¤é¡µå†…ç›®å½•ï¼ˆ.toc/.on-this-page ç­‰ï¼‰")
    nav_group.add_argument("--exclude-selectors",
                           help="è‡ªå®šä¹‰ç§»é™¤çš„å…ƒç´ é€‰æ‹©å™¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œæ”¯æŒï¼štag/.class/#id/[attr=val]")
    nav_group.add_argument("--anchor-list-threshold", type=int, default=0,
                           help="è¿ç»­é”šç‚¹åˆ—è¡¨ç§»é™¤é˜ˆå€¼ï¼ˆé»˜è®¤ 0 å…³é—­ï¼‰ï¼Œå»ºè®®ä¸ --strip-nav é…åˆä½¿ç”¨ï¼Œæ¨èå€¼ 10-20")
    
    # ========== æ™ºèƒ½æ­£æ–‡å®šä½å‚æ•°ï¼ˆPhase 2ï¼‰==========
    smart_group = ap.add_argument_group("æ™ºèƒ½æ­£æ–‡å®šä½å‚æ•°ï¼ˆPhase 2ï¼‰")
    smart_group.add_argument("--docs-preset", choices=get_available_presets(),
                             help="ä½¿ç”¨æ–‡æ¡£æ¡†æ¶é¢„è®¾ï¼ˆè‡ªåŠ¨é…ç½® target å’Œ excludeï¼‰ï¼š" + 
                                  ", ".join(get_available_presets()))
    smart_group.add_argument("--auto-detect", action="store_true",
                             help="è‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ¡†æ¶å¹¶åº”ç”¨é¢„è®¾ï¼ˆé«˜ç½®ä¿¡åº¦æ—¶ï¼‰")
    smart_group.add_argument("--list-presets", action="store_true",
                             help="åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ–‡æ¡£æ¡†æ¶é¢„è®¾")
    
    # ========== æ‰¹é‡å¤„ç†å‚æ•° ==========
    batch_group = ap.add_argument_group("æ‰¹é‡å¤„ç†å‚æ•°")
    batch_group.add_argument("--urls-file", help="ä»æ–‡ä»¶è¯»å– URL åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ªï¼Œæ”¯æŒ # æ³¨é‡Šå’Œ URL|æ ‡é¢˜ æ ¼å¼ï¼‰")
    batch_group.add_argument("--output-dir", default="./batch_output", help="æ‰¹é‡è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ ./batch_outputï¼‰")
    batch_group.add_argument("--max-workers", type=int, default=3, help="å¹¶å‘çº¿ç¨‹æ•°ï¼ˆé»˜è®¤ 3ï¼Œå»ºè®®ä¸è¶…è¿‡ 5ï¼‰")
    batch_group.add_argument("--delay", type=float, default=1.0, help="è¯·æ±‚é—´éš”ç§’æ•°ï¼ˆé»˜è®¤ 1.0ï¼Œé¿å…è¢«å°ï¼‰")
    batch_group.add_argument("--skip-errors", action="store_true", help="è·³è¿‡å¤±è´¥çš„ URL ç»§ç»­å¤„ç†")
    batch_group.add_argument("--download-images", action="store_true", 
                             help="ä¸‹è½½å›¾ç‰‡åˆ°æœ¬åœ° assets ç›®å½•ï¼ˆé»˜è®¤ä¸ä¸‹è½½ï¼Œä¿ç•™åŸå§‹ URLï¼‰")
    
    # åˆå¹¶è¾“å‡ºå‚æ•°
    merge_group = ap.add_argument_group("åˆå¹¶è¾“å‡ºå‚æ•°")
    merge_group.add_argument("--merge", action="store_true", help="åˆå¹¶æ‰€æœ‰é¡µé¢ä¸ºå•ä¸ª MD æ–‡ä»¶")
    merge_group.add_argument("--merge-output", help="åˆå¹¶è¾“å‡ºæ–‡ä»¶åï¼ˆé»˜è®¤ merged.mdï¼‰")
    merge_group.add_argument("--toc", action="store_true", help="åœ¨åˆå¹¶æ–‡ä»¶å¼€å¤´ç”Ÿæˆç›®å½•")
    merge_group.add_argument("--merge-title", help="åˆå¹¶æ–‡æ¡£çš„ä¸»æ ‡é¢˜")
    merge_group.add_argument("--source-url", help="æ¥æºç«™ç‚¹ URLï¼ˆæ˜¾ç¤ºåœ¨æ–‡æ¡£ä¿¡æ¯ä¸­ï¼‰")
    merge_group.add_argument("--rewrite-links", action="store_true",
                             help="å°†ç«™å†…é“¾æ¥æ”¹å†™ä¸ºæ–‡æ¡£å†…é”šç‚¹ï¼ˆä»…åˆå¹¶æ¨¡å¼æœ‰æ•ˆï¼‰")
    merge_group.add_argument("--no-source-summary", action="store_true",
                             help="ä¸åœ¨æ–‡æ¡£å¼€å¤´æ˜¾ç¤ºæ¥æºä¿¡æ¯æ±‡æ€»")
    merge_group.add_argument("--warn-anchor-collisions", action="store_true",
                             help="æ˜¾ç¤ºé”šç‚¹å†²çªè¯¦æƒ…ï¼ˆåŒåæ ‡é¢˜è‡ªåŠ¨æ·»åŠ åç¼€ -2, -3...ï¼‰")
    merge_group.add_argument("--split-output", metavar="DIR",
                             help="åŒæ—¶è¾“å‡ºåˆ†æ–‡ä»¶ç‰ˆæœ¬åˆ°æŒ‡å®šç›®å½•ï¼ˆä¸ --merge é…åˆä½¿ç”¨ï¼Œç”ŸæˆåŒç‰ˆæœ¬ï¼‰")
    
    # çˆ¬å–æ¨¡å¼å‚æ•°
    crawl_group = ap.add_argument_group("çˆ¬å–æ¨¡å¼å‚æ•°")
    crawl_group.add_argument("--crawl", action="store_true", help="ä»ç´¢å¼•é¡µæå–é“¾æ¥å¹¶æ‰¹é‡æŠ“å–")
    crawl_group.add_argument("--crawl-pattern", help="é“¾æ¥åŒ¹é…æ­£åˆ™è¡¨è¾¾å¼ï¼ˆå¦‚ 'index\\.php\\?MMR'ï¼‰")
    crawl_group.add_argument("--same-domain", action="store_true", default=True, help="ä»…æŠ“å–åŒåŸŸåé“¾æ¥ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    crawl_group.add_argument("--no-same-domain", action="store_false", dest="same_domain", help="å…è®¸æŠ“å–è·¨åŸŸé“¾æ¥")
    
    args = ap.parse_args(argv)
    
    # ========== åˆ—å‡ºé¢„è®¾ ==========
    if args.list_presets:
        print("\nğŸ“¦ å¯ç”¨çš„æ–‡æ¡£æ¡†æ¶é¢„è®¾ï¼š\n")
        for name, preset in DOCS_PRESETS.items():
            print(f"  {name:15} - {preset.description}")
            print(f"                   æ­£æ–‡ ID: {', '.join(preset.target_ids) or '(æ— )'}")
            print(f"                   æ­£æ–‡ class: {', '.join(preset.target_classes[:3]) or '(æ— )'}{'...' if len(preset.target_classes) > 3 else ''}")
            print(f"                   æ’é™¤é€‰æ‹©å™¨: {len(preset.exclude_selectors)} ä¸ª")
            print()
        print("ä½¿ç”¨ç¤ºä¾‹ï¼špython grab_web_to_md.py URL --docs-preset mintlify")
        return EXIT_SUCCESS
    
    # ========== æ‰¹é‡å¤„ç†æ¨¡å¼ ==========
    is_batch_mode = bool(args.urls_file or args.crawl)
    
    if is_batch_mode:
        return _batch_main(args)
    
    # ========== å•é¡µå¤„ç†æ¨¡å¼ï¼ˆåŸæœ‰é€»è¾‘ï¼‰ ==========
    
    page_html: Optional[str] = None  # å¯èƒ½åœ¨ auto-title æˆ– local-html æ¨¡å¼ä¸‹æå‰è·å–
    session: Optional[requests.Session] = None  # å¯èƒ½åœ¨ auto-title æ¨¡å¼ä¸‹æå‰åˆ›å»º

    # --auto-title ä¸ --out åŒæ—¶æŒ‡å®šæ—¶ï¼Œ--out ä¼˜å…ˆï¼ˆauto-title è¢«å¿½ç•¥ï¼‰
    use_auto_title = bool(args.auto_title and not args.out)

    # æ”¯æŒ --local-html æ¨¡å¼ï¼ˆä»æœ¬åœ°æ–‡ä»¶è¯»å–ï¼Œè·³è¿‡ç½‘ç»œè¯·æ±‚ï¼‰
    if args.local_html:
        if not os.path.isfile(args.local_html):
            print(f"é”™è¯¯ï¼šæœ¬åœ° HTML æ–‡ä»¶ä¸å­˜åœ¨ï¼š{args.local_html}", file=sys.stderr)
            return EXIT_ERROR

        # æœ¬åœ°æ–‡ä»¶åŒæ ·åšä½“ç§¯ä¿æŠ¤ï¼ˆä¸ fetch_html çš„ --max-html-bytes è¡Œä¸ºä¿æŒä¸€è‡´ï¼‰
        try:
            size = os.path.getsize(args.local_html)
            if args.max_html_bytes and args.max_html_bytes > 0 and size > args.max_html_bytes:
                print(
                    f"é”™è¯¯ï¼šæœ¬åœ° HTML æ–‡ä»¶è¿‡å¤§ï¼ˆ{size} > {args.max_html_bytes} bytesï¼‰ï¼š{args.local_html}",
                    file=sys.stderr,
                )
                return EXIT_ERROR
        except OSError:
            pass
        
        # --local-html æ¨¡å¼ä¸‹ï¼Œurl å‚æ•°å¯é€‰ï¼Œç”¨äºå›¾ç‰‡ä¸‹è½½ï¼›ä¼˜å…ˆä½¿ç”¨ --base-url
        url = args.base_url or args.url or ""
        if not url:
            print("è­¦å‘Šï¼šæœªæŒ‡å®š --base-url æˆ– urlï¼Œå›¾ç‰‡å°†æ— æ³•ä¸‹è½½ï¼ˆä»…ä¿ç•™åŸå§‹å¼•ç”¨ï¼‰", file=sys.stderr)
        
        with open(args.local_html, "r", encoding="utf-8", errors="replace") as f:
            page_html = f.read()
        print(f"ä»æœ¬åœ°æ–‡ä»¶è¯»å–ï¼š{args.local_html}")
        
        # è¾“å‡ºæ–‡ä»¶å
        if args.out:
            base = args.out
        elif use_auto_title:
            _page_title = _extract_title_for_filename(page_html, url)
            _auto_name = _sanitize_filename_part(_page_title)
            if len(_auto_name) > 80:
                _auto_name = _auto_name[:80].rstrip("-")
            base = _auto_name + ".md"
            print(f"è‡ªåŠ¨æ ‡é¢˜å‘½åï¼š{_page_title} â†’ {base}")
        else:
            base = os.path.splitext(os.path.basename(args.local_html))[0] + ".md"
    else:
        # ç½‘ç»œæ¨¡å¼ï¼šå¿…é¡»æä¾› URL
        if not args.url:
            ap.error("å•é¡µæ¨¡å¼å¿…é¡»æä¾› URL å‚æ•°ï¼Œæˆ–ä½¿ç”¨ --urls-file / --crawl è¿›å…¥æ‰¹é‡æ¨¡å¼ï¼Œæˆ–ä½¿ç”¨ --local-html è¯»å–æœ¬åœ°æ–‡ä»¶")
        
        url = args.url

        if args.out:
            base = args.out
        elif use_auto_title:
            # --auto-title æ¨¡å¼ï¼šå…ˆè·å–é¡µé¢ï¼Œæå–æ ‡é¢˜åç”Ÿæˆæ–‡ä»¶å
            session = _create_session(args, referer_url=url)
            page_html, exit_code = _fetch_page_html(session, url, args)
            if exit_code is not None:
                return exit_code
            _page_title = _extract_title_for_filename(page_html, url)
            _auto_name = _sanitize_filename_part(_page_title)
            if len(_auto_name) > 80:
                _auto_name = _auto_name[:80].rstrip("-")
            base = _auto_name + ".md"
            print(f"è‡ªåŠ¨æ ‡é¢˜å‘½åï¼š{_page_title} â†’ {base}")
        else:
            base = _default_basename(url) + ".md"
    
    out_md = base
    # è‡ªåŠ¨åˆ›å»ºåŒåä¸Šçº§ç›®å½•ï¼ˆå¦‚æœç”¨æˆ·æœªæŒ‡å®šç›®å½•ï¼‰
    out_md = auto_wrap_output_dir(out_md)
    # æ£€æŸ¥è¾“å‡ºæ–‡ä»¶è·¯å¾„é•¿åº¦
    md_dir = os.path.dirname(out_md) or "."
    out_md_name = os.path.basename(out_md)
    out_md_name = _safe_path_length(md_dir, out_md_name)
    out_md = os.path.join(md_dir, out_md_name) if md_dir != "." else out_md_name
    assets_dir = args.assets_dir or (os.path.splitext(out_md)[0] + ".assets")
    map_json = out_md + ".assets.json"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    if md_dir != ".":
        os.makedirs(md_dir, exist_ok=True)

    if os.path.exists(out_md) and not args.overwrite:
        print(f"æ–‡ä»¶å·²å­˜åœ¨ï¼š{out_md}ï¼ˆå¦‚éœ€è¦†ç›–è¯·åŠ  --overwriteï¼‰", file=sys.stderr)
        return EXIT_FILE_EXISTS

    # åˆ›å»º Sessionï¼ˆå¦‚æœå°šæœªåœ¨ auto-title æµç¨‹ä¸­åˆ›å»ºï¼‰
    if session is None:
        session = _create_session(args, referer_url=url)

    # ç½‘ç»œæ¨¡å¼ä¸‹ä¸‹è½½é¡µé¢ï¼ˆå¦‚æœå°šæœªåœ¨ auto-title æµç¨‹ä¸­è·å–ï¼‰
    if not args.local_html and page_html is None:
        page_html, exit_code = _fetch_page_html(session, url, args)
        if exit_code is not None:
            return exit_code

    # å¾®ä¿¡å…¬ä¼—å·æ–‡ç« è‡ªåŠ¨æ£€æµ‹
    is_wechat = args.wechat
    if url and not is_wechat and is_wechat_article_url(url):
        is_wechat = True
        print("æ£€æµ‹åˆ°å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ï¼Œè‡ªåŠ¨å¯ç”¨å¾®ä¿¡æ¨¡å¼")
    elif not is_wechat and is_wechat_article_html(page_html):
        is_wechat = True
        print("æ£€æµ‹åˆ°å¾®ä¿¡å…¬ä¼—å·æ–‡ç« ç‰¹å¾ï¼Œè‡ªåŠ¨å¯ç”¨å¾®ä¿¡æ¨¡å¼")

    # ç¡®å®šæ­£æ–‡æå–ç­–ç•¥
    target_id = args.target_id
    target_class = args.target_class
    exclude_selectors = args.exclude_selectors
    strip_nav = args.strip_nav
    strip_page_toc = args.strip_page_toc
    anchor_list_threshold = args.anchor_list_threshold
    
    # å•é¡µæ¨¡å¼ï¼šåº”ç”¨ docs-presetï¼ˆPhase 2ï¼‰
    if hasattr(args, 'docs_preset') and args.docs_preset:
        preset = DOCS_PRESETS.get(args.docs_preset)
        if preset:
            print(f"ğŸ“¦ ä½¿ç”¨æ–‡æ¡£æ¡†æ¶é¢„è®¾ï¼š{preset.name} ({preset.description})")
            # åº”ç”¨é¢„è®¾çš„ target é…ç½®ï¼ˆä»…å½“ç”¨æˆ·æœªæŒ‡å®šæ—¶ï¼‰
            if not target_id and preset.target_ids:
                target_id = ",".join(preset.target_ids)
            if not target_class and preset.target_classes:
                target_class = ",".join(preset.target_classes)
            # åˆå¹¶é¢„è®¾çš„ exclude_selectors
            preset_excludes = ",".join(preset.exclude_selectors)
            if exclude_selectors:
                exclude_selectors = f"{exclude_selectors},{preset_excludes}"
            else:
                exclude_selectors = preset_excludes
            # è‡ªåŠ¨å¯ç”¨å¯¼èˆªå‰¥ç¦»
            strip_nav = True
            strip_page_toc = True
            # é¢„è®¾æ¨¡å¼ä¸‹è‡ªåŠ¨å¯ç”¨é”šç‚¹åˆ—è¡¨å‰¥ç¦»
            if anchor_list_threshold == 0:
                anchor_list_threshold = 10
            print(f"  â€¢ æ­£æ–‡å®¹å™¨ IDï¼š{target_id or '(æœªè®¾ç½®)'}")
            print(f"  â€¢ æ­£æ–‡å®¹å™¨ classï¼š{target_class or '(æœªè®¾ç½®)'}")
    
    # å•é¡µæ¨¡å¼ï¼šè‡ªåŠ¨æ£€æµ‹æ–‡æ¡£æ¡†æ¶ï¼ˆPhase 2ï¼‰
    elif hasattr(args, 'auto_detect') and args.auto_detect:
        framework, confidence, signals = detect_docs_framework(page_html)
        if framework and confidence >= 0.6:
            preset = DOCS_PRESETS.get(framework)
            if preset:
                print(f"ğŸ” è‡ªåŠ¨æ£€æµ‹åˆ°æ–‡æ¡£æ¡†æ¶ï¼š{preset.name}ï¼ˆç½®ä¿¡åº¦ï¼š{confidence:.0%}ï¼‰")
                # åº”ç”¨é¢„è®¾é…ç½®
                if not target_id and preset.target_ids:
                    target_id = ",".join(preset.target_ids)
                if not target_class and preset.target_classes:
                    target_class = ",".join(preset.target_classes)
                preset_excludes = ",".join(preset.exclude_selectors)
                if exclude_selectors:
                    exclude_selectors = f"{exclude_selectors},{preset_excludes}"
                else:
                    exclude_selectors = preset_excludes
                strip_nav = True
                strip_page_toc = True
                if anchor_list_threshold == 0:
                    anchor_list_threshold = 10
        elif framework:
            print(f"ğŸ” æ£€æµ‹åˆ°å¯èƒ½çš„æ–‡æ¡£æ¡†æ¶ï¼š{framework}ï¼ˆç½®ä¿¡åº¦ï¼š{confidence:.0%}ï¼Œæœªè‡ªåŠ¨åº”ç”¨ï¼‰")
    
    # å¾®ä¿¡æ¨¡å¼ä¸‹ï¼Œå¦‚æœæœªæŒ‡å®š targetï¼Œè‡ªåŠ¨ä½¿ç”¨ rich_media_content
    if is_wechat and not target_id and not target_class:
        target_class = "rich_media_content"
        print("ä½¿ç”¨å¾®ä¿¡æ­£æ–‡åŒºåŸŸï¼šrich_media_content")

    # ä½¿ç”¨å¤šå€¼ target æå–ï¼ˆPhase 2 æ”¯æŒé€—å·åˆ†éš”ï¼‰
    if target_id or target_class:
        article_html, matched_selector = extract_target_html_multi(
            page_html, target_ids=target_id, target_classes=target_class
        )
        if not article_html:
            print("è­¦å‘Šï¼šæœªæ‰¾åˆ°æŒ‡å®šçš„ç›®æ ‡åŒºåŸŸï¼Œå°†å›é€€åˆ°è‡ªåŠ¨æŠ½å–ã€‚", file=sys.stderr)
            article_html = extract_main_html(page_html)
        elif matched_selector:
            print(f"ä½¿ç”¨æ­£æ–‡å®¹å™¨ï¼š{matched_selector}")
    else:
        article_html = extract_main_html(page_html)

    # å•é¡µæ¨¡å¼ï¼šåº”ç”¨å¯¼èˆªå‰¥ç¦»ï¼ˆPhase 1ï¼‰
    strip_selectors = get_strip_selectors(
        strip_nav=strip_nav,
        strip_page_toc=strip_page_toc,
        exclude_selectors=exclude_selectors,
    )
    if strip_selectors:
        article_html, strip_stats = strip_html_elements(article_html, strip_selectors)
        if strip_stats.elements_removed > 0:
            print(f"å·²ç§»é™¤ {strip_stats.elements_removed} ä¸ªå¯¼èˆªå…ƒç´ ")

    if args.spa_warn_len and html_text_len(article_html) < args.spa_warn_len:
        print(
            f"è­¦å‘Šï¼šæŠ½å–åˆ°çš„æ­£æ–‡å†…å®¹è¾ƒçŸ­ï¼ˆ<{args.spa_warn_len} å­—ç¬¦ï¼‰ï¼Œè¯¥é¡µé¢å¯èƒ½ä¸º SPA åŠ¨æ€æ¸²æŸ“ï¼›"
            "å¦‚å†…å®¹ä¸ºç©º/ä¸å®Œæ•´ï¼Œå¯å°è¯•ï¼š1) ä½¿ç”¨ --target-id/--target-class æŒ‡å®šæ­£æ–‡åŒºåŸŸï¼›"
            "2) ç­‰å¾…é¡µé¢å®Œæ•´åŠ è½½åä¿å­˜ HTML å†å¤„ç†ï¼›3) ä½¿ç”¨æµè§ˆå™¨å¼€å‘è€…å·¥å…·è·å–æ¸²æŸ“åçš„ HTMLã€‚",
            file=sys.stderr,
        )

    collector = ImageURLCollector(base_url=url)
    collector.feed(article_html)
    image_urls = uniq_preserve_order(collector.image_urls)

    print(f"å‘ç°å›¾ç‰‡ï¼š{len(image_urls)} å¼ ï¼Œå¼€å§‹ä¸‹è½½åˆ°ï¼š{assets_dir}")
    url_to_local = download_images(
        session=session,
        image_urls=image_urls,
        assets_dir=assets_dir,
        md_dir=md_dir,
        timeout_s=args.timeout,
        retries=args.retries,
        best_effort=bool(args.best_effort_images),
        page_url=url,
        redact_urls=args.redact_url,
        max_image_bytes=args.max_image_bytes,
    )

    # æå–æ ‡é¢˜ï¼ˆå¾®ä¿¡æ¨¡å¼ä¸‹ä¼˜å…ˆä½¿ç”¨ä¸“ç”¨æå–å‡½æ•°ï¼‰
    if args.title:
        title = args.title
    elif is_wechat:
        title = extract_wechat_title(page_html) or extract_h1(article_html) or extract_title(page_html) or "Untitled"
    else:
        title = extract_h1(article_html) or extract_title(page_html) or "Untitled"
    md_body = html_to_markdown(
        article_html=article_html,
        base_url=url,
        url_to_local=url_to_local,
        keep_html=args.keep_html,
    )
    md_body = strip_duplicate_h1(md_body, title)

    # æ¸…ç†å™ªéŸ³å†…å®¹
    if is_wechat:
        md_body = clean_wechat_noise(md_body)
        print("å·²æ¸…ç†å¾®ä¿¡å…¬ä¼—å· UI å™ªéŸ³")

    # å•é¡µæ¨¡å¼ï¼šé”šç‚¹åˆ—è¡¨å‰¥ç¦»ï¼ˆPhase 1ï¼‰
    # ä½¿ç”¨å±€éƒ¨å˜é‡ anchor_list_thresholdï¼ˆå¯èƒ½è¢«é¢„è®¾ä¿®æ”¹ï¼‰
    if anchor_list_threshold > 0:
        md_body, anchor_stats = strip_anchor_lists(md_body, anchor_list_threshold)
        if anchor_stats.anchor_lists_removed > 0:
            print(f"å·²ç§»é™¤ {anchor_stats.anchor_lists_removed} ä¸ªé”šç‚¹åˆ—è¡¨å—ï¼ˆå…± {anchor_stats.anchor_lines_removed} è¡Œï¼‰")

    # è§£æ tags å‚æ•°
    tags: Optional[List[str]] = None
    if args.tags:
        tags = [t.strip() for t in args.tags.split(",") if t.strip()]

    display_url = redact_url(url) if args.redact_url else url
    if args.redact_url:
        md_body = redact_urls_in_markdown(md_body)

    with open(out_md, "w", encoding="utf-8") as f:
        if args.frontmatter:
            f.write(generate_frontmatter(title, display_url, tags))
        # ä¿æŒæ­£æ–‡å¯è¯»æ€§ï¼šæ— è®ºæ˜¯å¦å¯ç”¨ frontmatterï¼Œéƒ½å†™å…¥å¯è§æ ‡é¢˜ä¸æ¥æºè¡Œã€‚
        f.write(f"# {title}\n\n")
        f.write(f"- Source: {display_url}\n\n")
        f.write(md_body)

    wrote_map_json = False
    if not args.no_map_json:
        with open(map_json, "w", encoding="utf-8") as f:
            map_payload = _redact_url_to_local_map(url_to_local) if args.redact_url else url_to_local
            json.dump(map_payload, f, ensure_ascii=False, indent=2)
        wrote_map_json = True
    else:
        # Bug fix: --no-map-json æ—¶åˆ é™¤æ—§çš„æ˜ å°„æ–‡ä»¶ï¼Œé¿å…é—ç•™æœªè„±æ•çš„å†å² URL
        if os.path.exists(map_json):
            try:
                os.remove(map_json)
                print(f"å·²åˆ é™¤æ—§æ˜ å°„æ–‡ä»¶ï¼š{map_json}")
            except OSError as e:
                print(f"è­¦å‘Šï¼šæ— æ³•åˆ é™¤æ—§æ˜ å°„æ–‡ä»¶ {map_json}: {e}", file=sys.stderr)

    print(f"å·²ç”Ÿæˆï¼š{out_md}")
    print(f"å›¾ç‰‡ç›®å½•ï¼š{assets_dir}")
    if wrote_map_json:
        print(f"æ˜ å°„æ–‡ä»¶ï¼š{map_json}")

    if args.with_pdf:
        out_pdf = os.path.splitext(out_md)[0] + ".pdf"
        if os.path.exists(out_pdf) and (not args.overwrite):
            print(f"PDF å·²å­˜åœ¨ï¼Œè·³è¿‡ï¼š{out_pdf}ï¼ˆå¦‚éœ€è¦†ç›–è¯·åŠ  --overwriteï¼‰", file=sys.stderr)
        else:
            print(f"ç”Ÿæˆ PDFï¼š{out_pdf}")
            if args.frontmatter:
                # md æ–‡ä»¶ä¿ç•™ frontmatterï¼›ä½† PDF æ¸²æŸ“æ—¶å‰¥ç¦»å…ƒæ•°æ®å—ï¼Œå¹¶è¡¥ä¸€ä¸ªå¯è§æ ‡é¢˜/æ¥æºè¡Œã€‚
                pdf_md = f"# {title}\n\n- Source: {display_url}\n\n{md_body}"
                md_dir_abs = os.path.dirname(os.path.abspath(out_md)) or "."
                tmp = None
                try:
                    with tempfile.NamedTemporaryFile(
                        "w",
                        encoding="utf-8",
                        suffix=".no_frontmatter.md",
                        dir=md_dir_abs,
                        delete=False,
                    ) as tf:
                        tf.write(strip_yaml_frontmatter(pdf_md))
                        tmp = tf.name
                    generate_pdf_from_markdown(md_path=tmp, pdf_path=out_pdf, allow_file_access=args.pdf_allow_file_access)
                finally:
                    if tmp and os.path.isfile(tmp):
                        try:
                            os.remove(tmp)
                        except OSError:
                            pass
            else:
                generate_pdf_from_markdown(md_path=out_md, pdf_path=out_pdf, allow_file_access=args.pdf_allow_file_access)

    if args.validate:
        result = validate_markdown(out_md, assets_dir)
        print("\næ ¡éªŒç»“æœï¼š")
        print(f"- å›¾ç‰‡å¼•ç”¨æ•°ï¼ˆæ€»ï¼‰ï¼š{result.image_refs}")
        print(f"- å›¾ç‰‡å¼•ç”¨æ•°ï¼ˆæœ¬åœ°ï¼‰ï¼š{result.local_image_refs}")
        print(f"- assets æ–‡ä»¶æ•°ï¼š{result.asset_files}")
        if result.missing_files:
            print("- ç¼ºå¤±æ–‡ä»¶ï¼š")
            for m in result.missing_files:
                print(f"  - {m}")
            return EXIT_VALIDATION_FAILED
        else:
            print("- ç¼ºå¤±æ–‡ä»¶ï¼š0")

    return EXIT_SUCCESS


if __name__ == "__main__":
    raise SystemExit(main())
